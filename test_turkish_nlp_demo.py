"""
Turkish NLP Demo - Quick Start
TEKNOFEST 2025 - TÃ¼rkÃ§e NLP ModÃ¼llerini Test Et
"""

import sys
import os
from pathlib import Path

# Set UTF-8 encoding for Windows
if sys.platform == 'win32':
    os.environ['PYTHONIOENCODING'] = 'utf-8'

# Add project root to path
sys.path.append(str(Path(__file__).parent))

from src.nlp.advanced_turkish_morphology import (
    TurkishMorphologyAnalyzer, 
    TurkishStemmer, 
    TurkishLemmatizer
)
from src.nlp.turkish_bpe_tokenizer import TurkishBPETokenizer, TokenizerConfig
from src.nlp.turkish_nlp_integration import TurkishNLPPipeline

def demo_morphology():
    """Morfoloji analizi demo"""
    print("\n" + "="*60)
    print("TURKCE MORFOLOJI ANALIZI")
    print("="*60)
    
    analyzer = TurkishMorphologyAnalyzer()
    stemmer = TurkishStemmer()
    lemmatizer = TurkishLemmatizer()
    
    test_words = [
        "evlerimizden",
        "kitaplarÄ±",
        "Ã¶ÄŸretmenlik",
        "gÃ¼zelleÅŸtirmek",
        "TÃ¼rkiye'nin"
    ]
    
    for word in test_words:
        print(f"\nKelime: {word}")
        print("-" * 40)
        
        # Morfolojik analiz
        analysis = analyzer.analyze(word)
        print(f"  Kok: {analysis.root}")
        print(f"  Sozcuk Turu: {analysis.pos_tag}")
        print(f"  Ekler: {[m.surface for m in analysis.morphemes]}")
        
        # ÃœnlÃ¼ uyumu
        harmony = analyzer.analyze_vowel_harmony(word)
        print(f"  Unlu Uyumu: {'Gecerli' if harmony['valid'] else 'Gecersiz'}")
        if harmony['valid']:
            print(f"    - Kalinlik-Incelik: {'OK' if harmony['front_back'] else 'X'}")
            print(f"    - Duzluk-Yuvarliklik: {'OK' if harmony['rounded_unrounded'] else 'X'}")
        
        # Heceleme
        syllables = analyzer.syllabify(word)
        print(f"  Heceler: {'-'.join(syllables)}")
        
        # KÃ¶k ve lemma
        stem = stemmer.stem(word)
        lemma = lemmatizer.lemmatize(word)
        print(f"  KÃ¶k (Stem): {stem}")
        print(f"  Lemma: {lemma}")

def demo_tokenizer():
    """BPE Tokenizer demo"""
    print("\n" + "="*60)
    print("ğŸ”¤ TÃœRKÃ‡E BPE TOKENIZER")
    print("="*60)
    
    # Tokenizer yapÄ±landÄ±rmasÄ±
    config = TokenizerConfig(
        vocab_size=1000,
        turkish_specific=True,
        min_frequency=1
    )
    
    tokenizer = TurkishBPETokenizer(config)
    
    # Mini corpus ile eÄŸit
    corpus = [
        "Merhaba dÃ¼nya! BugÃ¼n hava Ã§ok gÃ¼zel.",
        "TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r.",
        "Ä°stanbul BoÄŸazÄ±'nda vapurla gezinti yapmak Ã§ok keyifli.",
        "Ã–ÄŸrenciler sÄ±navlara hazÄ±rlanÄ±yor.",
        "Yapay zeka teknolojileri hÄ±zla geliÅŸiyor.",
        "TEKNOFEST 2025 yarÄ±ÅŸmasÄ±na hazÄ±rlanÄ±yoruz.",
        "TÃ¼rkÃ§e doÄŸal dil iÅŸleme Ã§alÄ±ÅŸmalarÄ± Ã¶nem kazanÄ±yor.",
        "EÄŸitim teknolojileri Ã¶ÄŸrenmeyi kolaylaÅŸtÄ±rÄ±yor.",
    ]
    
    print("\nğŸ“Š Tokenizer EÄŸitimi BaÅŸlÄ±yor...")
    tokenizer.train(corpus, vocab_size=500)
    
    # Test metinleri
    test_texts = [
        "TEKNOFEST 2025 iÃ§in hazÄ±rÄ±z!",
        "TÃ¼rkÃ§e NLP Ã§ok gÃ¼zel Ã§alÄ±ÅŸÄ±yor.",
        "15:30'da %50 indirim var.",
        "Dr. Ahmet Bey 100 TL Ã¶dedi."
    ]
    
    for text in test_texts:
        print(f"\nğŸ“ Metin: {text}")
        print("-" * 40)
        
        # Tokenize et
        tokens = tokenizer.tokenize(text)
        print(f"  Tokenlar: {tokens}")
        
        # Encode/Decode
        token_ids = tokenizer.encode(text, add_special_tokens=True)
        print(f"  Token ID'ler: {token_ids[:10]}{'...' if len(token_ids) > 10 else ''}")
        
        decoded = tokenizer.decode(token_ids, skip_special_tokens=True)
        print(f"  Decoded: {decoded}")

def demo_pipeline():
    """Entegre pipeline demo"""
    print("\n" + "="*60)
    print("ğŸš€ TÃœRKÃ‡E NLP PIPELINE - KOMPLE ANALÄ°Z")
    print("="*60)
    
    # Pipeline baÅŸlat (harici baÄŸÄ±mlÄ±lÄ±klar olmadan)
    pipeline = TurkishNLPPipeline(
        use_zemberek=False,      # Zemberek kullanma (Java gerektirir)
        use_transformers=False,  # Transformer modelleri kullanma (bÃ¼yÃ¼k indirme gerektirir)
        use_spacy=False,        # spaCy kullanma (ek kurulum gerektirir)
        cache_enabled=True
    )
    
    # Test metinleri
    test_texts = [
        "Ankara TÃ¼rkiye'nin baÅŸkentidir.",
        "Ã–ÄŸrenciler yarÄ±n sÄ±nava girecekler.",
        "Bu kitap Ã§ok gÃ¼zel ve faydalÄ±.",
        "Ä°stanbul BoÄŸazÄ±'nda vapurla gezinti yaptÄ±k.",
        "TEKNOFEST 2025 yarÄ±ÅŸmasÄ± iÃ§in hazÄ±rlanÄ±yoruz.",
        "Prof. Dr. Ahmet YÄ±lmaz Ankara Ãœniversitesi'nde Ã§alÄ±ÅŸÄ±yor."
    ]
    
    for text in test_texts:
        print(f"\nğŸ“ Metin: {text}")
        print("=" * 50)
        
        # Komple analiz
        results = pipeline.analyze_complete(text)
        
        print("\nğŸ”¤ Tokenlar:")
        print(f"  {results['tokens']}")
        
        print("\nğŸ“š Lemma'lar:")
        print(f"  {results['lemmas']}")
        
        print("\nğŸŒ± KÃ¶kler:")
        print(f"  {results['stems']}")
        
        print("\nğŸ‘¤ VarlÄ±klar (Named Entities):")
        if results['entities']:
            for entity in results['entities']:
                print(f"  - {entity.get('text', 'N/A')} [{entity.get('type', 'N/A')}]")
        else:
            print("  (VarlÄ±k bulunamadÄ±)")
        
        print("\nğŸ˜Š Duygu Analizi:")
        sentiment = results['sentiment']
        print(f"  Duygu: {sentiment['label']}")
        print(f"  GÃ¼ven: {sentiment['confidence']:.2%}")
        
        print("\nğŸ“Š Ä°statistikler:")
        stats = results['statistics']
        print(f"  Karakter: {stats['char_count']}")
        print(f"  Kelime: {stats['word_count']}")
        print(f"  CÃ¼mle: {stats['sentence_count']}")
        print(f"  Stopword: {stats['stopword_count']}")

def demo_special_features():
    """Ã–zel Ã¶zellikler demo"""
    print("\n" + "="*60)
    print("âœ¨ Ã–ZEL Ã–ZELLÄ°KLER")
    print("="*60)
    
    analyzer = TurkishMorphologyAnalyzer()
    pipeline = TurkishNLPPipeline(
        use_zemberek=False,
        use_transformers=False,
        use_spacy=False
    )
    
    print("\n1ï¸âƒ£ ÃœNSÃœZ DEÄÄ°ÅÄ°MÄ° (Consonant Mutation)")
    print("-" * 40)
    mutations = [
        ("kitap", "kitabÄ±"),
        ("aÄŸaÃ§", "aÄŸacÄ±"),
        ("renk", "rengi"),
        ("kÃ¶pek", "kÃ¶peÄŸi")
    ]
    
    for base, expected in mutations:
        mutated = analyzer.apply_consonant_mutation(base, "accusative")
        print(f"  {base} â†’ {mutated} (beklenen: {expected})")
    
    print("\n2ï¸âƒ£ ÃœNLÃœ DÃœÅMESÄ° (Vowel Drop)")
    print("-" * 40)
    drops = [
        "burun", "aÄŸÄ±z", "boyun", "oÄŸul"
    ]
    
    for word in drops:
        dropped, did_drop = analyzer.apply_vowel_drop(word)
        if did_drop:
            print(f"  {word} â†’ {dropped} âœ…")
        else:
            print(f"  {word} (deÄŸiÅŸmedi)")
    
    print("\n3ï¸âƒ£ STOPWORD TEMÄ°ZLEME")
    print("-" * 40)
    text = "Bu ve ÅŸu kitaplar Ã§ok gÃ¼zel ama pahalÄ±"
    filtered = pipeline.remove_stopwords(text)
    print(f"  Orijinal: {text}")
    print(f"  TemizlenmiÅŸ: {filtered}")
    
    print("\n4ï¸âƒ£ KISALTMA AÃ‡MA")
    print("-" * 40)
    text = "Dr. Ahmet Bey T.C. vatandaÅŸÄ±, Prof. AyÅŸe HanÄ±m'la Ã§alÄ±ÅŸÄ±yor."
    expanded = pipeline.expand_abbreviations(text)
    print(f"  Orijinal: {text}")
    print(f"  AÃ§Ä±lmÄ±ÅŸ: {expanded}")
    
    print("\n5ï¸âƒ£ BÄ°RLEÅÄ°K KELÄ°ME ANALÄ°ZÄ°")
    print("-" * 40)
    compounds = ["kahvehane", "baÅŸÃ¶ÄŸretmen", "hanÄ±meli", "bilgisayar"]
    
    for word in compounds:
        is_compound, parts = analyzer.analyze_compound(word)
        if is_compound:
            print(f"  {word} â†’ {' + '.join(parts)} âœ…")
        else:
            print(f"  {word} (birleÅŸik deÄŸil)")

def main():
    """Ana demo fonksiyonu"""
    print("\n" + "=" * 60)
    print("   TEKNOFEST 2025 - TÃœRKÃ‡E NLP DEMO   ")
    print("=" * 60)
    
    try:
        # Morfoloji demo
        demo_morphology()
        
        # Tokenizer demo
        demo_tokenizer()
        
        # Pipeline demo
        demo_pipeline()
        
        # Ã–zel Ã¶zellikler
        demo_special_features()
        
        print("\n" + "="*60)
        print("âœ… TÃœM TESTLER BAÅARIYLA TAMAMLANDI!")
        print("="*60)
        
        print("\nğŸ“Œ Sonraki adÄ±mlar:")
        print("  1. Zemberek-NLP kurulumu iÃ§in: pip install JPype1")
        print("  2. Transformer modelleri iÃ§in: pip install transformers torch")
        print("  3. Daha bÃ¼yÃ¼k corpus ile tokenizer eÄŸitimi")
        print("  4. Production iÃ§in API entegrasyonu")
        
    except Exception as e:
        print(f"\nâŒ Hata oluÅŸtu: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()