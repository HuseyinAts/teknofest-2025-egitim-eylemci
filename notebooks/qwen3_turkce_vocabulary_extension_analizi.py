# QWEN3 TÃœRKÃ‡E VOCABULARY EXTENSION YAKLAÅIMI
# Ultra DetaylÄ± Analiz ve KarÅŸÄ±laÅŸtÄ±rma

"""
Bu dosya, Qwen3-8B modeline TÃ¼rkÃ§e vocabulary extension yapmanÄ±n
avantaj, dezavantaj ve implementasyon detaylarÄ±nÄ± iÃ§erir.

PROBLEM: turkish_mixtral_v3_fixed (32K) vs Qwen3 (151K) â†’ Loss 5.2383
Ã‡Ã–ZÃ¼M SEÃ‡ENEKLERÄ°:
1. Hibrit YaklaÅŸÄ±m (1-3 hafta)
2. Ã–zel Tokenizer (6-12 hafta)  
3. Vocabulary Extension (2-6 hafta) â† Bu analiz
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import json
from collections import Counter

class Qwen3TurkceVocabularyExtension:
    """Qwen3 TÃ¼rkÃ§e vocabulary extension analizi ve implementasyonu"""
    
    def __init__(self):
        self.qwen3_vocab_size = 151936
        self.target_extension_size = 20000  # Ã–nerilen TÃ¼rkÃ§e ekleme
        self.final_vocab_size = 171936      # %13 artÄ±ÅŸ
        
    def yaklaÅŸim_analizi(self):
        """Vocabulary Extension yaklaÅŸÄ±mÄ±nÄ±n detaylÄ± analizi"""
        
        return {
            "TEMEL KONSEPT": {
                "tanÄ±m": "Qwen3'Ã¼n mevcut vocabulary'sine TÃ¼rkÃ§e-specific tokenlar ekleme",
                "strateji": "Qwen3 base knowledge koruma + TÃ¼rkÃ§e optimization",
                "hedef": "Best-of-both-worlds hibrit Ã§Ã¶zÃ¼m"
            },
            
            "TEKNÄ°K YAKLAÅIM": {
                "adÄ±m_1": "Qwen3'Ã¼n 151,936 tokenÄ±nÄ± tamamen koruma",
                "adÄ±m_2": "En sÄ±k kullanÄ±lan 20,000 TÃ¼rkÃ§e token ekleme",
                "adÄ±m_3": "Smart embedding initialization",
                "adÄ±m_4": "Gradual fine-tuning ile adaptation",
                "sonuÃ§": "171,936 token hybrid vocabulary"
            },
            
            "KORUMA STRATEJÄ°SÄ°": {
                "qwen3_tokens": "100% preserved - no modification",
                "embeddings": "Original embeddings frozen during extension",
                "knowledge": "Pre-trained knowledge fully protected",
                "compatibility": "Perfect backward compatibility"
            }
        }
    
    def avantajlar_detaylÄ±(self):
        """Vocabulary Extension'Ä±n avantajlarÄ±"""
        
        return {
            "ğŸš€ HIZLI IMPLEMENTATION": {
                "sÃ¼re": "2-6 hafta (vs 6-12 hafta Ã¶zel tokenizer)",
                "karmaÅŸÄ±klÄ±k": "Orta seviye (vs Ã‡ok yÃ¼ksek)",
                "risk": "DÃ¼ÅŸÃ¼k-orta (vs YÃ¼ksek)",
                "baÅŸarÄ±_oranÄ±": "75-85% (gÃ¼venilir)"
            },
            
            "ğŸ’° MALIYET ETKÄ°NLÄ°ÄÄ°": {
                "development_cost": "$15,000-30,000 (vs $50,000-150,000)",
                "time_to_market": "4x daha hÄ±zlÄ±",
                "resource_requirement": "Orta seviye compute",
                "roi": "YÃ¼ksek return on investment"
            },
            
            "ğŸ”’ RÄ°SK MÄ°TÄ°GATION": {
                "qwen3_knowledge": "100% korunur",
                "backward_compatibility": "Tam uyumluluk",
                "fallback_option": "Original Qwen3'e kolay dÃ¶nÃ¼ÅŸ",
                "incremental_approach": "AÅŸamalÄ±, kontrollÃ¼ geliÅŸtirme"
            },
            
            "âš¡ PERFORMANS Ä°YÄ°LEÅTÄ°RMELERÄ°": {
                "tÃ¼rkÃ§e_tokenization": "15-25% daha verimli",
                "oov_reduction": "Out-of-vocabulary %40-60 azalma",
                "inference_speed": "5-15% hÄ±zlanma (shorter sequences)",
                "memory_usage": "Slight increase (acceptable trade-off)"
            },
            
            "ğŸ¯ TÃœRKÃ‡E OPTÄ°MÄ°ZASYON": {
                "high_frequency_words": "iÃ§in, olan, gibi, Ã§ok â†’ dedicated tokens",
                "morphological_patterns": "SÄ±k ek yapÄ±larÄ± â†’ efficient encoding",
                "compound_words": "TÃ¼rkÃ§e birleÅŸik kelimeler â†’ better handling",
                "domain_specific": "EÄŸitim, teknoloji terimleri â†’ specialized tokens"
            },
            
            "ğŸ”§ KOLAY MAINTENANCE": {
                "debugging": "Basit architecture, kolay debug",
                "monitoring": "Clear performance metrics",
                "updates": "Incremental vocabulary updates possible",
                "deployment": "Standard deployment pipeline"
            }
        }
    
    def dezavantajlar_detaylÄ±(self):
        """Vocabulary Extension'Ä±n dezavantajlarÄ±"""
        
        return {
            "âš ï¸ SINIRLI OPTÄ°MÄ°ZASYON": {
                "kÄ±sÄ±t": "Qwen3 base architecture deÄŸiÅŸtirilemez",
                "sonuÃ§": "Perfect Turkish optimization impossible",
                "karÅŸÄ±laÅŸtÄ±rma": "Ã–zel tokenizer kadar optimize edilemez",
                "trade_off": "GÃ¼venlik vs Maximum optimization"
            },
            
            "ğŸ“ˆ MODEL SIZE ARTIÅI": {
                "vocabulary_increase": "+20,000 tokens (%13 artÄ±ÅŸ)",
                "embedding_size": "+320MB additional parameters",
                "memory_overhead": "Training ve inference'da artÄ±ÅŸ",
                "storage_cost": "Model storage requirement artÄ±ÅŸÄ±"
            },
            
            "ğŸ”€ HÄ°BRÄ°T COMPLEXITY": {
                "dual_tokenization": "Ä°ki farklÄ± token space management",
                "overlap_handling": "Qwen3-Turkish token overlap issues",
                "performance_tuning": "Optimal balance finding challenges",
                "debugging_complexity": "Mixed token space debugging"
            },
            
            "âš–ï¸ BALANCE CHALLENGES": {
                "qwen3_vs_turkish": "Original vs Turkish token usage balance",
                "frequency_distribution": "Token frequency redistribution",
                "semantic_consistency": "Consistent meaning across token spaces",
                "training_stability": "Stable convergence with mixed vocabulary"
            },
            
            "ğŸ›ï¸ FINE-TUNING COMPLEXITY": {
                "embedding_initialization": "New tokens require careful initialization",
                "learning_rate_balancing": "Different LR for old vs new embeddings",
                "training_duration": "Longer training for vocabulary adaptation",
                "hyperparameter_sensitivity": "More sensitive hyperparameter tuning"
            },
            
            "ğŸ“Š EVALUATION CHALLENGES": {
                "baseline_comparison": "Difficult to establish clear baselines",
                "performance_attribution": "Hard to attribute improvements",
                "quality_metrics": "Complex quality assessment",
                "regression_detection": "Potential performance regressions"
            }
        }
    
    def implementasyon_detaylÄ±(self):
        """Step-by-step implementation planÄ±"""
        
        return {
            "AÅAMA 1 - TÃœRKÃ‡E TOKEN ANALÄ°ZÄ° (1 hafta)": {
                "hedef": "En deÄŸerli 20,000 TÃ¼rkÃ§e token belirleme",
                "yÃ¶ntem": {
                    "corpus_analysis": "100GB+ TÃ¼rkÃ§e text analizi",
                    "frequency_counting": "Token frequency distribution",
                    "morphology_analysis": "TÃ¼rkÃ§e morfolojik pattern analizi",
                    "overlap_detection": "Qwen3 vocabulary overlap kontrolÃ¼"
                },
                "Ã§Ä±ktÄ±": "20,000 optimal TÃ¼rkÃ§e token listesi",
                "kod_Ã¶rneÄŸi": """
# Turkish corpus analysis
turkish_corpus = load_turkish_corpus("100gb_turkish_text")
token_frequencies = analyze_frequency(turkish_corpus)
morphological_patterns = extract_morphology(turkish_corpus)
qwen3_overlap = find_overlap(token_frequencies, qwen3_vocab)

# Select optimal 20K tokens
optimal_tokens = select_optimal_tokens(
    frequencies=token_frequencies,
    morphology=morphological_patterns,
    overlap=qwen3_overlap,
    target_size=20000
)
                """
            },
            
            "AÅAMA 2 - VOCABULARY MERGE (3-5 gÃ¼n)": {
                "hedef": "Qwen3 + TÃ¼rkÃ§e vocabulary birleÅŸtirme",
                "yÃ¶ntem": {
                    "vocabulary_extension": "151,936 + 20,000 = 171,936",
                    "token_id_mapping": "New token ID assignment",
                    "special_tokens": "Turkish-specific special tokens",
                    "tokenizer_update": "Tokenizer configuration update"
                },
                "kod_Ã¶rneÄŸi": """
# Load original Qwen3 tokenizer
original_tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")
original_vocab = original_tokenizer.get_vocab()

# Create extended vocabulary
extended_vocab = original_vocab.copy()
start_id = len(original_vocab)

for i, turkish_token in enumerate(optimal_turkish_tokens):
    extended_vocab[turkish_token] = start_id + i

# Create new tokenizer with extended vocabulary
extended_tokenizer = create_extended_tokenizer(
    base_tokenizer=original_tokenizer,
    new_vocab=extended_vocab
)
                """
            },
            
            "AÅAMA 3 - MODEL EXTENSION (2-3 gÃ¼n)": {
                "hedef": "Model architecture'Ä± extended vocabulary'ye adapt etme",
                "yÃ¶ntem": {
                    "embedding_resize": "Model embedding layer geniÅŸletme",
                    "initialization": "Yeni embeddings iÃ§in smart initialization",
                    "architecture_validation": "Model compatibility kontrolÃ¼"
                },
                "kod_Ã¶rneÄŸi": """
# Load original model
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-8B")

# Resize embedding layers
old_embeddings = model.get_input_embeddings().weight.data
model.resize_token_embeddings(len(extended_tokenizer))

# Smart initialization for new tokens
new_embeddings = model.get_input_embeddings().weight.data
with torch.no_grad():
    # Original embeddings unchanged
    new_embeddings[:len(original_vocab)] = old_embeddings
    
    # Initialize Turkish tokens
    for i, token in enumerate(optimal_turkish_tokens):
        token_id = len(original_vocab) + i
        new_embeddings[token_id] = initialize_turkish_embedding(
            token, original_embeddings, method="semantic_similarity"
        )
                """
            },
            
            "AÅAMA 4 - SMART INITIALIZATION (1-2 gÃ¼n)": {
                "hedef": "TÃ¼rkÃ§e tokenlar iÃ§in optimal embedding baÅŸlangÄ±Ã§ deÄŸerleri",
                "yÃ¶ntemler": {
                    "semantic_similarity": "Anlamsal benzer tokenlardan average",
                    "morphological_composition": "Morfolojik bileÅŸenlerden oluÅŸturma",
                    "random_with_constraints": "KontrollÃ¼ rastgele initialization",
                    "transfer_learning": "BaÅŸka Turkish model'den transfer"
                },
                "kod_Ã¶rneÄŸi": """
def initialize_turkish_embedding(token, original_embeddings, method="semantic"):
    if method == "semantic_similarity":
        # Find semantically similar tokens
        similar_tokens = find_similar_tokens(token, original_vocab)
        similar_embeddings = original_embeddings[[
            original_vocab[t] for t in similar_tokens
        ]]
        return similar_embeddings.mean(dim=0) + torch.randn_like(similar_embeddings[0]) * 0.01
    
    elif method == "morphological":
        # Compose from morphological parts
        root, suffixes = analyze_morphology(token)
        root_embedding = get_embedding_if_exists(root, original_embeddings)
        suffix_embeddings = [get_embedding_if_exists(s, original_embeddings) for s in suffixes]
        return compose_morphological_embedding(root_embedding, suffix_embeddings)
    
    else:  # random with constraints
        return torch.randn(original_embeddings.size(1)) * 0.02
                """
            },
            
            "AÅAMA 5 - GRADUAL TRAINING (2-4 hafta)": {
                "hedef": "Model'i extended vocabulary'e adapt etme",
                "strateji": {
                    "phase_1": "Freeze original embeddings, train new only",
                    "phase_2": "Gradual unfreezing with low learning rates",
                    "phase_3": "Joint training with balanced learning rates"
                },
                "training_config": """
# Phase 1: New embeddings only (1 hafta)
training_config_phase1 = {
    "freeze_original_embeddings": True,
    "learning_rate_new": 1e-3,
    "learning_rate_original": 0.0,
    "epochs": 3,
    "batch_size": 8
}

# Phase 2: Gradual unfreezing (1 hafta)
training_config_phase2 = {
    "freeze_original_embeddings": False,
    "learning_rate_new": 5e-4,
    "learning_rate_original": 1e-5,
    "epochs": 2,
    "batch_size": 8
}

# Phase 3: Joint training (1-2 hafta)
training_config_phase3 = {
    "learning_rate_new": 2e-4,
    "learning_rate_original": 1e-4,
    "epochs": 3,
    "batch_size": 8
}
                """
            }
        }
    
    def karÅŸÄ±laÅŸtÄ±rma_matrisi(self):
        """TÃ¼m yaklaÅŸÄ±mlarÄ±n detaylÄ± karÅŸÄ±laÅŸtÄ±rmasÄ±"""
        
        return {
            "YAKLAÅIM_KARÅILAÅTIRMA": {
                "kriterler": [
                    "GeliÅŸtirme SÃ¼resi", "Maliyet", "BaÅŸarÄ± OranÄ±", "Risk",
                    "TÃ¼rkÃ§e Optimizasyon", "Qwen3 Knowledge", "Maintenance"
                ],
                
                "hibrit_yaklaÅŸÄ±m": {
                    "sÃ¼re": "1-3 hafta",
                    "maliyet": "$5,000-15,000",
                    "baÅŸarÄ±": "80-90%",
                    "risk": "DÃ¼ÅŸÃ¼k",
                    "tÃ¼rkÃ§e_opt": "Orta",
                    "qwen3_knowledge": "Korunur",
                    "maintenance": "KarmaÅŸÄ±k"
                },
                
                "vocabulary_extension": {
                    "sÃ¼re": "2-6 hafta",
                    "maliyet": "$15,000-30,000",
                    "baÅŸarÄ±": "75-85%",
                    "risk": "DÃ¼ÅŸÃ¼k-Orta",
                    "tÃ¼rkÃ§e_opt": "Ä°yi",
                    "qwen3_knowledge": "100% Korunur",
                    "maintenance": "Orta"
                },
                
                "Ã¶zel_tokenizer": {
                    "sÃ¼re": "6-12 hafta",
                    "maliyet": "$50,000-150,000",
                    "baÅŸarÄ±": "60-85%",
                    "risk": "YÃ¼ksek",
                    "tÃ¼rkÃ§e_opt": "MÃ¼kemmel",
                    "qwen3_knowledge": "Yeniden Ã¶ÄŸrenme",
                    "maintenance": "Basit"
                }
            },
            
            "PUAN_TABLOSU": {
                "criteria_weights": {
                    "hÄ±z": 0.25,       # Time to market
                    "maliyet": 0.20,   # Cost efficiency
                    "risk": 0.20,      # Risk management
                    "kalite": 0.35     # Turkish optimization + Qwen3 knowledge
                },
                
                "scores": {
                    "hibrit_yaklaÅŸÄ±m": {
                        "hÄ±z": 9,      # Very fast
                        "maliyet": 9,  # Very cost effective
                        "risk": 8,     # Low risk
                        "kalite": 6,   # Good but not optimal
                        "toplam": 7.6
                    },
                    "vocabulary_extension": {
                        "hÄ±z": 7,      # Fast
                        "maliyet": 7,  # Cost effective
                        "risk": 7,     # Low-medium risk
                        "kalite": 8,   # Very good balance
                        "toplam": 7.4
                    },
                    "Ã¶zel_tokenizer": {
                        "hÄ±z": 3,      # Slow
                        "maliyet": 4,  # Expensive
                        "risk": 4,     # High risk
                        "kalite": 9,   # Excellent
                        "toplam": 5.4
                    }
                }
            }
        }
    
    def beklenen_sonuÃ§lar(self):
        """Vocabulary Extension'dan beklenen sonuÃ§lar"""
        
        return {
            "PERFORMANS_Ä°YÄ°LEÅTÄ°RMELERÄ°": {
                "loss_improvement": {
                    "current": "5.2383",
                    "expected": "2.0-3.0",
                    "improvement": "40-60% iyileÅŸtirme"
                },
                "tokenization_efficiency": {
                    "turkish_text_compression": "15-25% daha kÄ±sa sequences",
                    "oov_reduction": "40-60% azalma",
                    "speed_improvement": "5-15% inference hÄ±zlanmasÄ±"
                },
                "model_performance": {
                    "turkish_understanding": "25-40% iyileÅŸtirme",
                    "generation_quality": "Daha doÄŸal TÃ¼rkÃ§e Ã¼retim",
                    "domain_accuracy": "Specialized domains'de iyileÅŸtirme"
                }
            },
            
            "TEKNÄ°K_METRÄ°KLER": {
                "vocabulary_stats": {
                    "total_size": "171,936 tokens",
                    "qwen3_preserved": "151,936 (100%)",
                    "turkish_added": "20,000 (new)",
                    "overlap_handled": "Intelligent deduplication"
                },
                "memory_impact": {
                    "additional_parameters": "~320MB",
                    "training_memory": "+15-20% increase",
                    "inference_memory": "+10-15% increase",
                    "acceptable_overhead": "Good ROI trade-off"
                },
                "compatibility": {
                    "qwen3_compatibility": "100% maintained",
                    "backward_compatibility": "Full support",
                    "api_compatibility": "Seamless integration",
                    "deployment": "Standard pipeline"
                }
            },
            
            "Ä°Å DEÄERÄ°": {
                "immediate_benefits": {
                    "training_success": "Loss 5.2+ â†’ 2.0-3.0",
                    "turkish_capability": "Significantly improved",
                    "time_to_market": "4-6 hafta vs 12+ hafta",
                    "cost_efficiency": "2-5x cheaper than custom tokenizer"
                },
                "strategic_value": {
                    "technology_ownership": "Partial IP ownership",
                    "competitive_advantage": "Good Turkish optimization",
                    "scalability": "Foundation for future improvements",
                    "learning": "Knowledge for future custom tokenizer"
                }
            }
        }
    
    def implementation_timeline(self):
        """DetaylÄ± implementation timeline"""
        
        return {
            "HAFTA_1": {
                "focus": "Turkish Token Analysis",
                "tasks": [
                    "100GB Turkish corpus collection",
                    "Token frequency analysis",
                    "Morphological pattern extraction",
                    "Qwen3 vocabulary overlap analysis",
                    "Optimal 20K token selection"
                ],
                "deliverable": "20,000 optimal Turkish token list",
                "effort": "40-50 hours"
            },
            
            "HAFTA_2": {
                "focus": "Vocabulary Integration",
                "tasks": [
                    "Extended tokenizer implementation",
                    "Model architecture adaptation",
                    "Smart embedding initialization",
                    "Compatibility testing",
                    "Initial validation"
                ],
                "deliverable": "Extended model with 171,936 tokens",
                "effort": "35-45 hours"
            },
            
            "HAFTA_3-4": {
                "focus": "Phase 1 Training",
                "tasks": [
                    "New embeddings training (frozen original)",
                    "Training pipeline setup",
                    "Performance monitoring",
                    "Quality validation",
                    "Hyperparameter tuning"
                ],
                "deliverable": "Phase 1 trained model",
                "effort": "60-80 hours + compute time"
            },
            
            "HAFTA_5-6": {
                "focus": "Joint Training & Validation",
                "tasks": [
                    "Gradual unfreezing strategy",
                    "Joint training execution",
                    "Comprehensive evaluation",
                    "Performance benchmarking",
                    "Production preparation"
                ],
                "deliverable": "Production-ready model",
                "effort": "50-70 hours + compute time"
            },
            
            "TOPLAM_TAHMINI": {
                "sÃ¼re": "4-6 hafta",
                "insan_gÃ¼cÃ¼": "180-245 hours",
                "compute_time": "100-200 GPU hours",
                "success_probability": "75-85%"
            }
        }

def main():
    """Ana analysis fonksiyonu"""
    
    analyzer = Qwen3TurkceVocabularyExtension()
    
    print("ğŸ¯ QWEN3 TÃœRKÃ‡E VOCABULARY EXTENSION ANALÄ°ZÄ°")
    print("=" * 70)
    
    # Temel yaklaÅŸÄ±m analizi
    yaklaÅŸÄ±m = analyzer.yaklaÅŸim_analizi()
    print("\nğŸ“‹ TEMEL KONSEPT:")
    print(f"â€¢ TanÄ±m: {yaklaÅŸÄ±m['TEMEL KONSEPT']['tanÄ±m']}")
    print(f"â€¢ Strateji: {yaklaÅŸÄ±m['TEMEL KONSEPT']['strateji']}")
    print(f"â€¢ Hedef: {yaklaÅŸÄ±m['TEMEL KONSEPT']['hedef']}")
    
    # Avantajlar
    avantajlar = analyzer.avantajlar_detaylÄ±()
    print("\nâœ… TEMEL AVANTAJLAR:")
    print(f"â€¢ SÃ¼re: {avantajlar['ğŸš€ HIZLI IMPLEMENTATION']['sÃ¼re']}")
    print(f"â€¢ Maliyet: {avantajlar['ğŸ’° MALIYET ETKÄ°NLÄ°ÄÄ°']['development_cost']}")
    print(f"â€¢ BaÅŸarÄ± OranÄ±: {avantajlar['ğŸš€ HIZLI IMPLEMENTATION']['baÅŸarÄ±_oranÄ±']}")
    print(f"â€¢ Qwen3 Knowledge: {avantajlar['ğŸ”’ RÄ°SK MÄ°TÄ°GATION']['qwen3_knowledge']}")
    
    # Dezavantajlar
    dezavantajlar = analyzer.dezavantajlar_detaylÄ±()
    print("\nâŒ TEMEL DEZAVANTAJLAR:")
    print(f"â€¢ Optimizasyon KÄ±sÄ±tÄ±: {dezavantajlar['âš ï¸ SINIRLI OPTÄ°MÄ°ZASYON']['kÄ±sÄ±t']}")
    print(f"â€¢ Model Size: {dezavantajlar['ğŸ“ˆ MODEL SIZE ARTIÅI']['vocabulary_increase']}")
    print(f"â€¢ Complexity: {dezavantajlar['ğŸ”€ HÄ°BRÄ°T COMPLEXITY']['dual_tokenization']}")
    
    # KarÅŸÄ±laÅŸtÄ±rma
    karÅŸÄ±laÅŸtÄ±rma = analyzer.karÅŸÄ±laÅŸtÄ±rma_matrisi()
    print("\nğŸ“Š YAKLAÅIM KARÅILAÅTIRMA:")
    scores = karÅŸÄ±laÅŸtÄ±rma["PUAN_TABLOSU"]["scores"]
    print(f"â€¢ Hibrit YaklaÅŸÄ±m: {scores['hibrit_yaklaÅŸÄ±m']['toplam']}/10")
    print(f"â€¢ Vocabulary Extension: {scores['vocabulary_extension']['toplam']}/10")
    print(f"â€¢ Ã–zel Tokenizer: {scores['Ã¶zel_tokenizer']['toplam']}/10")
    
    # Timeline
    timeline = analyzer.implementation_timeline()
    print(f"\nâ° IMPLEMENTATION SÃœRESÄ°:")
    print(f"â€¢ Toplam SÃ¼re: {timeline['TOPLAM_TAHMINI']['sÃ¼re']}")
    print(f"â€¢ Ä°nsan GÃ¼cÃ¼: {timeline['TOPLAM_TAHMINI']['insan_gÃ¼cÃ¼']}")
    print(f"â€¢ BaÅŸarÄ± Ä°htimali: {timeline['TOPLAM_TAHMINI']['success_probability']}")
    
    # Beklenen sonuÃ§lar
    sonuÃ§lar = analyzer.beklenen_sonuÃ§lar()
    print(f"\nğŸ¯ BEKLENEN SONUÃ‡LAR:")
    print(f"â€¢ Loss Ä°yileÅŸtirme: {sonuÃ§lar['PERFORMANS_Ä°YÄ°LEÅTÄ°RMELERÄ°']['loss_improvement']['current']} â†’ {sonuÃ§lar['PERFORMANS_Ä°YÄ°LEÅTÄ°RMELERÄ°']['loss_improvement']['expected']}")
    print(f"â€¢ Tokenization: {sonuÃ§lar['PERFORMANS_Ä°YÄ°LEÅTÄ°RMELERÄ°']['tokenization_efficiency']['turkish_text_compression']}")
    print(f"â€¢ Turkish Quality: {sonuÃ§lar['PERFORMANS_Ä°YÄ°LEÅTÄ°RMELERÄ°']['model_performance']['turkish_understanding']}")
    
    print("\nğŸ† SONUÃ‡: Vocabulary Extension, hibrit yaklaÅŸÄ±m ile Ã¶zel tokenizer arasÄ±nda optimal denge saÄŸlar!")
    print("ğŸ“Œ Ã–neri: Hibrit yaklaÅŸÄ±mla hÄ±zlÄ± baÅŸlangÄ±Ã§ + Paralel vocabulary extension geliÅŸtirme")
    
    return analyzer

if __name__ == "__main__":
    analyzer = main()