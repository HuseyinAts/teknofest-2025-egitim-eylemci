name: TEKNOFEST 2025 - Production Test Suite

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.9'
  NODE_VERSION: '18'
  COVERAGE_THRESHOLD: 80

jobs:
  test-backend:
    name: Backend Tests - Python ${{ matrix.python-version }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: ['3.9', '3.10', '3.11']
        
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: teknofest
          POSTGRES_PASSWORD: teknofest_test
          POSTGRES_DB: teknofest_test_db
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          
      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.local/share/virtualenvs
        key: ${{ runner.os }}-python-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-python-${{ matrix.python-version }}-
          ${{ runner.os }}-python-

    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libpq-dev

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install -r requirements.txt
        pip install -r requirements-production.txt
        pip install pytest-xdist  # For parallel testing

    - name: Create test environment file
      run: |
        cp .env.test .env
        echo "DATABASE_URL=postgresql://teknofest:teknofest_test@localhost:5432/teknofest_test_db" >> .env
        echo "REDIS_URL=redis://localhost:6379/0" >> .env

    - name: Run database migrations
      run: |
        python manage_db.py upgrade head

    - name: Run unit tests with coverage
      run: |
        pytest tests/unit/ \
          --cov=src \
          --cov-report=xml:coverage-unit.xml \
          --cov-report=term-missing \
          --cov-report=html:htmlcov-unit \
          --junit-xml=test-results-unit.xml \
          -v \
          -n auto \
          --maxfail=5

    - name: Run integration tests
      run: |
        pytest tests/integration/ \
          --cov=src \
          --cov-report=xml:coverage-integration.xml \
          --cov-report=term-missing \
          --junit-xml=test-results-integration.xml \
          -v \
          --maxfail=3

    - name: Run agent tests
      run: |
        pytest tests/test_agents_comprehensive.py \
          --cov=src.agents \
          --cov-report=xml:coverage-agents.xml \
          --cov-report=term-missing \
          --junit-xml=test-results-agents.xml \
          -v

    - name: Run database tests
      run: |
        pytest tests/test_database_comprehensive.py \
          --cov=src.database \
          --cov-report=xml:coverage-database.xml \
          --cov-report=term-missing \
          --junit-xml=test-results-database.xml \
          -v

    - name: Run API tests
      run: |
        pytest tests/test_api_comprehensive.py \
          --cov=src.api \
          --cov-report=xml:coverage-api.xml \
          --cov-report=term-missing \
          --junit-xml=test-results-api.xml \
          -v

    - name: Run Turkish NLP tests
      run: |
        pytest tests/test_turkish_nlp_modules.py \
          --cov=src.nlp \
          --cov-report=xml:coverage-nlp.xml \
          --cov-report=term-missing \
          --junit-xml=test-results-nlp.xml \
          -v

    - name: Combine coverage reports
      if: matrix.os == 'ubuntu-latest'
      run: |
        pip install coverage
        coverage combine coverage-*.xml
        coverage report --fail-under=${{ env.COVERAGE_THRESHOLD }}
        coverage xml -o coverage-final.xml

    - name: Upload coverage to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.9'
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage-final.xml
        flags: backend
        name: backend-coverage
        fail_ci_if_error: true

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          test-results-*.xml
          coverage-*.xml
          htmlcov*/

    - name: Check code quality
      if: matrix.os == 'ubuntu-latest'
      run: |
        pip install black flake8 mypy isort
        black --check src/ tests/
        flake8 src/ tests/ --max-line-length=120
        isort --check-only src/ tests/
        mypy src/ --ignore-missing-imports

  test-performance:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: test-backend
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install pytest-benchmark locust

    - name: Run performance benchmarks
      run: |
        pytest tests/ -m performance \
          --benchmark-only \
          --benchmark-json=benchmark-results.json \
          --benchmark-autosave

    - name: Run load tests
      run: |
        locust -f tests/load_testing/locustfile.py \
          --headless \
          --users 100 \
          --spawn-rate 10 \
          --run-time 60s \
          --host http://localhost:8000 \
          --html load-test-report.html

    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: |
          benchmark-results.json
          load-test-report.html

  test-security:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: test-backend
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        pip install safety bandit pip-audit

    - name: Check for vulnerable dependencies
      run: |
        safety check --json > safety-report.json || true
        pip-audit --desc > pip-audit-report.txt || true

    - name: Run security linting
      run: |
        bandit -r src/ -f json -o bandit-report.json || true

    - name: Run OWASP dependency check
      uses: dependency-check/Dependency-Check_Action@main
      with:
        project: 'TEKNOFEST-2025'
        path: '.'
        format: 'ALL'

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          safety-report.json
          pip-audit-report.txt
          bandit-report.json
          dependency-check-report.*

  test-e2e:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: test-backend
    
    services:
      app:
        image: ghcr.io/${{ github.repository }}:latest
        ports:
          - 8000:8000
        env:
          DATABASE_URL: postgresql://teknofest:teknofest@postgres:5432/teknofest_db
          REDIS_URL: redis://redis:6379
        options: >-
          --health-cmd "curl -f http://localhost:8000/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install test dependencies
      run: |
        pip install pytest playwright httpx

    - name: Install Playwright browsers
      run: |
        playwright install chromium

    - name: Run E2E tests
      run: |
        pytest tests/test_e2e_production.py \
          --base-url=http://localhost:8000 \
          --junit-xml=test-results-e2e.xml \
          -v

    - name: Upload E2E test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-results
        path: test-results-e2e.xml

  publish-test-results:
    name: Publish Test Results
    runs-on: ubuntu-latest
    needs: [test-backend, test-performance, test-security, test-e2e]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3

    - name: Publish test results
      uses: EnricoMi/publish-unit-test-result-action@v2
      if: always()
      with:
        files: |
          **/test-results-*.xml
        check_name: Test Results
        comment_title: Test Results Summary

    - name: Create test summary
      if: always()
      run: |
        echo "## Test Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Coverage Report" >> $GITHUB_STEP_SUMMARY
        echo "- Backend: See coverage reports in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Performance" >> $GITHUB_STEP_SUMMARY
        echo "- Benchmark results available in artifacts" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### Security" >> $GITHUB_STEP_SUMMARY
        echo "- Security scan results available in artifacts" >> $GITHUB_STEP_SUMMARY

  deploy-test-report:
    name: Deploy Test Report
    runs-on: ubuntu-latest
    needs: publish-test-results
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Download test artifacts
      uses: actions/download-artifact@v3

    - name: Setup Pages
      uses: actions/configure-pages@v3

    - name: Upload to Pages
      uses: actions/upload-pages-artifact@v2
      with:
        path: ./

    - name: Deploy to GitHub Pages
      uses: actions/deploy-pages@v2
      with:
        token: ${{ secrets.GITHUB_TOKEN }}