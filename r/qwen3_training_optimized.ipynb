{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# üöÄ QWEN3-8B TURKISH 200K - ULTRA OPTIMIZED TRAINING\n\n## ‚ö° Optimizasyon √ñzellikleri:\n- ‚úÖ **Accuracy**: Mixed Precision, EMA, Label Smoothing, Curriculum Learning, Knowledge Distillation\n- ‚úÖ **Speed**: Flash Attention, Dynamic Padding, Compiled Mode, Efficient Data Loading\n- ‚úÖ **Reliability**: Gradient Clipping, Auto Recovery, Health Monitoring, Adaptive Batch\n- ‚úÖ **Memory**: Gradient Checkpointing, 8-bit Optimizer, CPU Offloading, Teacher Caching\n\n**Version**: 2.1 - Production Ready with Knowledge Distillation\n**Target**: Google Colab T4/A100 GPUs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Advanced GPU Setup & System Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# GPU ve Sistem Kontrol√º\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "class SystemMonitor:\n",
    "    \"\"\"Sistem kaynaklarƒ±nƒ± izleme\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_gpu_info():\n",
    "        if not torch.cuda.is_available():\n",
    "            return None\n",
    "        \n",
    "        gpu_id = torch.cuda.current_device()\n",
    "        gpu_name = torch.cuda.get_device_name(gpu_id)\n",
    "        vram_total = torch.cuda.get_device_properties(gpu_id).total_memory / 1e9\n",
    "        vram_used = torch.cuda.memory_allocated(gpu_id) / 1e9\n",
    "        vram_free = vram_total - vram_used\n",
    "        \n",
    "        # GPU tipini belirle\n",
    "        gpu_type = \"unknown\"\n",
    "        if \"T4\" in gpu_name:\n",
    "            gpu_type = \"t4\"\n",
    "        elif \"A100\" in gpu_name:\n",
    "            gpu_type = \"a100\"\n",
    "        elif \"V100\" in gpu_name:\n",
    "            gpu_type = \"v100\"\n",
    "        elif \"3090\" in gpu_name or \"4090\" in gpu_name:\n",
    "            gpu_type = \"rtx_high\"\n",
    "        \n",
    "        return {\n",
    "            \"name\": gpu_name,\n",
    "            \"type\": gpu_type,\n",
    "            \"vram_total\": vram_total,\n",
    "            \"vram_free\": vram_free,\n",
    "            \"compute_capability\": torch.cuda.get_device_capability(gpu_id)\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_system_info():\n",
    "        return {\n",
    "            \"cpu_count\": psutil.cpu_count(),\n",
    "            \"ram_total\": psutil.virtual_memory().total / 1e9,\n",
    "            \"ram_available\": psutil.virtual_memory().available / 1e9,\n",
    "            \"pytorch_version\": torch.__version__,\n",
    "            \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else None\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def print_info():\n",
    "        gpu_info = SystemMonitor.get_gpu_info()\n",
    "        sys_info = SystemMonitor.get_system_info()\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(\"üñ•Ô∏è SYSTEM INFORMATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if gpu_info:\n",
    "            print(f\"GPU: {gpu_info['name']}\")\n",
    "            print(f\"  Type: {gpu_info['type'].upper()}\")\n",
    "            print(f\"  VRAM: {gpu_info['vram_total']:.1f}GB (Free: {gpu_info['vram_free']:.1f}GB)\")\n",
    "            print(f\"  Compute: {gpu_info['compute_capability']}\")\n",
    "        else:\n",
    "            print(\"‚ùå No GPU available\")\n",
    "        \n",
    "        print(f\"\\nCPU: {sys_info['cpu_count']} cores\")\n",
    "        print(f\"RAM: {sys_info['ram_total']:.1f}GB (Available: {sys_info['ram_available']:.1f}GB)\")\n",
    "        print(f\"PyTorch: {sys_info['pytorch_version']}\")\n",
    "        print(f\"CUDA: {sys_info['cuda_version']}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        return gpu_info, sys_info\n",
    "\n",
    "# Sistem bilgilerini al ve g√∂ster\n",
    "gpu_info, sys_info = SystemMonitor.print_info()\n",
    "\n",
    "# GPU'ya g√∂re optimizasyon √∂nerileri\n",
    "if gpu_info:\n",
    "    if gpu_info['type'] == 't4':\n",
    "        print(\"\\nüí° T4 GPU Optimizasyonlarƒ± aktif olacak:\")\n",
    "        print(\"  ‚Ä¢ Batch size: 1-2\")\n",
    "        print(\"  ‚Ä¢ Gradient accumulation: 16\")\n",
    "        print(\"  ‚Ä¢ Mixed precision: FP16\")\n",
    "        print(\"  ‚Ä¢ Flash Attention: v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Optimized Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Hƒ±zlƒ± ve optimize kurulum\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# PyTorch with CUDA 11.8 (optimized)\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Core libraries with specific versions for stability\n",
    "!pip install -q transformers==4.44.0\n",
    "!pip install -q datasets==2.14.0\n",
    "!pip install -q accelerate==0.32.0\n",
    "!pip install -q peft==0.11.1\n",
    "!pip install -q bitsandbytes==0.43.1\n",
    "\n",
    "# Optimization libraries\n",
    "!pip install -q flash-attn --no-build-isolation  # Flash Attention 2\n",
    "!pip install -q xformers  # Memory efficient attention\n",
    "!pip install -q deepspeed  # Distributed training\n",
    "!pip install -q lion-pytorch  # Lion optimizer\n",
    "\n",
    "# Tokenizer\n",
    "!pip install -q tiktoken\n",
    "\n",
    "# Monitoring & Utils\n",
    "!pip install -q wandb\n",
    "!pip install -q colorama\n",
    "!pip install -q tqdm\n",
    "!pip install -q psutil\n",
    "!pip install -q py-cpuinfo\n",
    "\n",
    "print(\"‚úÖ All optimized libraries installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Google Drive Setup with Auto-Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "# Google Drive mount\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Advanced directory setup with versioning\n",
    "class ProjectManager:\n",
    "    def __init__(self, base_dir=\"/content/drive/MyDrive/qwen3_optimized\"):\n",
    "        self.base_dir = base_dir\n",
    "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        # Create organized directory structure\n",
    "        self.dirs = {\n",
    "            \"root\": base_dir,\n",
    "            \"models\": f\"{base_dir}/models\",\n",
    "            \"checkpoints\": f\"{base_dir}/checkpoints\",\n",
    "            \"logs\": f\"{base_dir}/logs\",\n",
    "            \"data\": f\"{base_dir}/data\",\n",
    "            \"cache\": f\"{base_dir}/cache\",\n",
    "            \"backups\": f\"{base_dir}/backups/{self.timestamp}\"\n",
    "        }\n",
    "        \n",
    "        # Create all directories\n",
    "        for dir_path in self.dirs.values():\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Session state file\n",
    "        self.state_file = f\"{self.dirs['root']}/session_state.json\"\n",
    "        \n",
    "    def save_state(self, state_dict):\n",
    "        \"\"\"Session state'i kaydet\"\"\"\n",
    "        state_dict['timestamp'] = self.timestamp\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(state_dict, f, indent=2)\n",
    "        print(f\"üíæ State saved: {self.state_file}\")\n",
    "    \n",
    "    def load_state(self):\n",
    "        \"\"\"√ñnceki session state'i y√ºkle\"\"\"\n",
    "        if os.path.exists(self.state_file):\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "            print(f\"üìÇ Previous state loaded from {state['timestamp']}\")\n",
    "            return state\n",
    "        return None\n",
    "    \n",
    "    def backup_checkpoint(self, checkpoint_path):\n",
    "        \"\"\"Checkpoint'i yedekle\"\"\"\n",
    "        if os.path.exists(checkpoint_path):\n",
    "            backup_path = f\"{self.dirs['backups']}/{os.path.basename(checkpoint_path)}\"\n",
    "            shutil.copy2(checkpoint_path, backup_path)\n",
    "            print(f\"üîí Checkpoint backed up: {backup_path}\")\n",
    "\n",
    "# Initialize project manager\n",
    "project = ProjectManager()\n",
    "os.chdir(project.dirs['root'])\n",
    "\n",
    "print(f\"üìÅ Working directory: {project.dirs['root']}\")\n",
    "print(f\"üìÖ Session ID: {project.timestamp}\")\n",
    "\n",
    "# Check for previous session\n",
    "previous_state = project.load_state()\n",
    "if previous_state:\n",
    "    print(f\"\\nüîÑ Found previous session:\")\n",
    "    print(f\"  - Started: {previous_state.get('timestamp', 'Unknown')}\")\n",
    "    print(f\"  - Last checkpoint: {previous_state.get('last_checkpoint', 'None')}\")\n",
    "    print(f\"  - Training step: {previous_state.get('global_step', 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Optimized Data Loading with Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import hashlib\n",
    "import pickle\n",
    "from typing import Optional, Dict, Any\n",
    "import numpy as np\n",
    "\n",
    "class OptimizedDataLoader:\n",
    "    \"\"\"Optimize edilmi≈ü veri y√ºkleme ve √∂nbellekleme\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str, dataset_name: str = \"Huseyin/turkish-200k-dataset\"):\n",
    "        self.cache_dir = cache_dir\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_hash = hashlib.md5(dataset_name.encode()).hexdigest()[:8]\n",
    "        self.cache_path = f\"{cache_dir}/dataset_{self.dataset_hash}\"\n",
    "        \n",
    "    def load_dataset(self, force_download: bool = False) -> Dataset:\n",
    "        \"\"\"Veri setini y√ºkle (√∂nbellekten veya HuggingFace'den)\"\"\"\n",
    "        \n",
    "        if not force_download and os.path.exists(self.cache_path):\n",
    "            print(f\"üìÇ Loading cached dataset from {self.cache_path}\")\n",
    "            dataset = load_from_disk(self.cache_path)\n",
    "            print(f\"‚úÖ Loaded {len(dataset)} samples from cache\")\n",
    "        else:\n",
    "            print(f\"üì• Downloading dataset: {self.dataset_name}\")\n",
    "            try:\n",
    "                # HuggingFace'den y√ºkle\n",
    "                dataset = load_dataset(self.dataset_name, split=\"train\")\n",
    "                \n",
    "                # Cache'e kaydet\n",
    "                dataset.save_to_disk(self.cache_path)\n",
    "                print(f\"üíæ Dataset cached to {self.cache_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading dataset: {e}\")\n",
    "                print(\"Creating fallback dataset...\")\n",
    "                # Fallback dataset olu≈ütur\n",
    "                dataset = self._create_fallback_dataset()\n",
    "        \n",
    "        # Dataset istatistikleri\n",
    "        self._print_dataset_stats(dataset)\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def _create_fallback_dataset(self, size: int = 10000) -> Dataset:\n",
    "        \"\"\"Fallback i√ßin √∂rnek dataset olu≈ütur\"\"\"\n",
    "        texts = [\n",
    "            f\"Bu √∂rnek metin {i}. T√ºrk√ße eƒüitim verisi i√ßin kullanƒ±lacak.\"\n",
    "            for i in range(size)\n",
    "        ]\n",
    "        return Dataset.from_dict({\"text\": texts})\n",
    "    \n",
    "    def _print_dataset_stats(self, dataset: Dataset):\n",
    "        \"\"\"Dataset istatistiklerini g√∂ster\"\"\"\n",
    "        print(\"\\nüìä Dataset Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total samples: {len(dataset):,}\")\n",
    "        print(f\"  ‚Ä¢ Columns: {dataset.column_names}\")\n",
    "        \n",
    "        if 'text' in dataset.column_names:\n",
    "            # Text uzunluk istatistikleri\n",
    "            text_lengths = [len(text.split()) for text in dataset['text'][:1000]]\n",
    "            print(f\"  ‚Ä¢ Avg text length: {np.mean(text_lengths):.1f} words\")\n",
    "            print(f\"  ‚Ä¢ Min/Max length: {min(text_lengths)}/{max(text_lengths)} words\")\n",
    "    \n",
    "    def prepare_for_curriculum_learning(self, dataset: Dataset) -> Dataset:\n",
    "        \"\"\"Curriculum learning i√ßin veriyi zorluƒüa g√∂re sƒ±rala\"\"\"\n",
    "        print(\"\\nüéì Preparing curriculum learning...\")\n",
    "        \n",
    "        # Text uzunluƒüuna g√∂re sƒ±rala (kolay ‚Üí zor)\n",
    "        def add_difficulty(example):\n",
    "            example['difficulty'] = len(example['text'].split())\n",
    "            return example\n",
    "        \n",
    "        dataset = dataset.map(add_difficulty)\n",
    "        dataset = dataset.sort('difficulty')\n",
    "        \n",
    "        print(\"‚úÖ Dataset sorted by difficulty (easy ‚Üí hard)\")\n",
    "        return dataset\n",
    "\n",
    "# Load optimized dataset\n",
    "data_loader = OptimizedDataLoader(project.dirs['cache'])\n",
    "dataset = data_loader.load_dataset()\n",
    "\n",
    "# Apply curriculum learning\n",
    "dataset = data_loader.prepare_for_curriculum_learning(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import tiktoken\nimport torch\nfrom typing import List, Dict, Any, Union, Optional\nfrom collections import defaultdict\nimport os\nfrom sentencepiece import SentencePieceProcessor\n\nclass TurkishMixtralTokenizer:\n    \"\"\"Turkish Mixtral v3 Fixed tokenizer wrapper optimized for Turkish text\"\"\"\n    \n    def __init__(self, model_path: str = None, max_length: int = 512, use_dynamic_padding: bool = True):\n        \"\"\"\n        Initialize Turkish tokenizer\n        Args:\n            model_path: Path to turkish_mixtral_v3_fixed.model file\n            max_length: Maximum sequence length\n            use_dynamic_padding: Use dynamic padding for memory efficiency\n        \"\"\"\n        self.max_length = max_length\n        self.use_dynamic_padding = use_dynamic_padding\n        self.model_path = model_path\n        \n        # Try to load Turkish Mixtral tokenizer first\n        if model_path and os.path.exists(model_path):\n            print(f\"‚úÖ Loading Turkish Mixtral tokenizer from: {model_path}\")\n            try:\n                self.tokenizer = SentencePieceProcessor(model_file=model_path)\n                self.vocab_size = self.tokenizer.get_piece_size()\n                self.use_turkish = True\n                print(f\"  ‚Ä¢ Vocab size: {self.vocab_size}\")\n                print(f\"  ‚Ä¢ Turkish-optimized tokenization enabled\")\n            except Exception as e:\n                print(f\"‚ö†Ô∏è Error loading Turkish tokenizer: {e}\")\n                self._fallback_to_tiktoken()\n        else:\n            print(f\"‚ö†Ô∏è Turkish tokenizer not found at: {model_path}\")\n            self._fallback_to_tiktoken()\n        \n        # Set special tokens based on tokenizer type\n        if self.use_turkish:\n            # Turkish Mixtral special tokens\n            self.pad_token = \"<pad>\"\n            self.eos_token = \"</s>\"\n            self.bos_token = \"<s>\"\n            self.unk_token = \"<unk>\"\n            \n            self.pad_token_id = self.tokenizer.piece_to_id(self.pad_token) if self.pad_token in self.tokenizer else 0\n            self.eos_token_id = self.tokenizer.piece_to_id(self.eos_token) if self.eos_token in self.tokenizer else 1\n            self.bos_token_id = self.tokenizer.piece_to_id(self.bos_token) if self.bos_token in self.tokenizer else 2\n            self.unk_token_id = self.tokenizer.piece_to_id(self.unk_token) if self.unk_token in self.tokenizer else 3\n        \n        self.model_max_length = max_length\n        self.padding_side = \"right\"\n        \n        # Cache for tokenized texts\n        self._cache = {}\n        self._cache_hits = 0\n        self._cache_misses = 0\n        \n        print(f\"‚úÖ Tokenizer initialized successfully\")\n        print(f\"  ‚Ä¢ Type: {'Turkish Mixtral' if self.use_turkish else 'Tiktoken (fallback)'}\")\n        print(f\"  ‚Ä¢ Dynamic padding: {use_dynamic_padding}\")\n    \n    def _fallback_to_tiktoken(self):\n        \"\"\"Fallback to tiktoken if Turkish tokenizer not available\"\"\"\n        print(\"  ‚Üí Using tiktoken as fallback\")\n        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n        self.vocab_size = self.encoding.n_vocab\n        self.use_turkish = False\n        \n        # Tiktoken special tokens\n        self.pad_token = \"<|endoftext|>\"\n        self.eos_token = \"<|endoftext|>\"\n        self.bos_token = \"<|startoftext|>\"\n        self.unk_token = \"<|unknown|>\"\n        \n        self.pad_token_id = 100257\n        self.eos_token_id = 100257\n        self.bos_token_id = 100258\n        self.unk_token_id = 100259\n    \n    def __call__(self,\n                 text: Union[str, List[str]],\n                 padding: Union[bool, str] = True,\n                 truncation: bool = True,\n                 max_length: Optional[int] = None,\n                 return_tensors: Optional[str] = None,\n                 return_attention_mask: bool = True,\n                 add_special_tokens: bool = True,\n                 **kwargs) -> Dict[str, Any]:\n        \"\"\"Tokenize text with optimizations\"\"\"\n        \n        if isinstance(text, str):\n            texts = [text]\n        else:\n            texts = text\n        \n        max_len = max_length or self.model_max_length\n        \n        # Dynamic padding: find actual max length in batch\n        if self.use_dynamic_padding and padding:\n            actual_max = 0\n            for txt in texts:\n                cache_key = hash(txt)\n                if cache_key in self._cache:\n                    tokens = self._cache[cache_key]\n                    self._cache_hits += 1\n                else:\n                    tokens = self._encode_text(txt)\n                    self._cache[cache_key] = tokens\n                    self._cache_misses += 1\n                \n                actual_max = max(actual_max, len(tokens))\n            \n            # Use smaller of actual_max and max_len\n            max_len = min(actual_max + 2, max_len)  # +2 for special tokens\n        \n        all_input_ids = []\n        all_attention_masks = []\n        \n        for txt in texts:\n            # Use cache if available\n            cache_key = hash(txt)\n            if cache_key in self._cache:\n                tokens = self._cache[cache_key]\n            else:\n                tokens = self._encode_text(txt)\n                self._cache[cache_key] = tokens\n            \n            # Add special tokens\n            if add_special_tokens:\n                tokens = [self.bos_token_id] + tokens + [self.eos_token_id]\n            \n            # Truncation\n            if truncation and len(tokens) > max_len:\n                tokens = tokens[:max_len-1] + [self.eos_token_id]\n            \n            # Padding\n            if padding:\n                original_length = len(tokens)\n                padding_length = max_len - original_length\n                \n                if self.padding_side == \"right\":\n                    tokens = tokens + [self.pad_token_id] * padding_length\n                    attention_mask = [1] * original_length + [0] * padding_length\n                else:\n                    tokens = [self.pad_token_id] * padding_length + tokens\n                    attention_mask = [0] * padding_length + [1] * original_length\n            else:\n                attention_mask = [1] * len(tokens)\n            \n            all_input_ids.append(tokens)\n            all_attention_masks.append(attention_mask)\n        \n        result = {\n            'input_ids': all_input_ids[0] if isinstance(text, str) else all_input_ids\n        }\n        \n        if return_attention_mask:\n            result['attention_mask'] = all_attention_masks[0] if isinstance(text, str) else all_attention_masks\n        \n        if return_tensors == \"pt\":\n            result = {k: torch.tensor(v) for k, v in result.items()}\n        \n        return result\n    \n    def _encode_text(self, text: str) -> List[int]:\n        \"\"\"Encode text using appropriate tokenizer\"\"\"\n        if self.use_turkish:\n            return self.tokenizer.encode(text, out_type=int)\n        else:\n            return self.encoding.encode(text)\n    \n    def decode(self, token_ids, skip_special_tokens: bool = True, **kwargs) -> str:\n        \"\"\"Decode token IDs to text\"\"\"\n        if hasattr(token_ids, 'tolist'):\n            token_ids = token_ids.tolist()\n        \n        if isinstance(token_ids, list) and len(token_ids) > 0 and isinstance(token_ids[0], list):\n            token_ids = token_ids[0]\n        \n        if skip_special_tokens:\n            special_tokens = {self.pad_token_id, self.eos_token_id, self.bos_token_id, self.unk_token_id}\n            token_ids = [t for t in token_ids if t not in special_tokens]\n        \n        if self.use_turkish:\n            return self.tokenizer.decode(token_ids)\n        else:\n            return self.encoding.decode(token_ids)\n    \n    def batch_decode(self, token_ids_list, skip_special_tokens: bool = True, **kwargs) -> List[str]:\n        \"\"\"Batch decode\"\"\"\n        return [self.decode(token_ids, skip_special_tokens) for token_ids in token_ids_list]\n    \n    def save_pretrained(self, save_path):\n        \"\"\"Save tokenizer configuration\"\"\"\n        os.makedirs(save_path, exist_ok=True)\n        config = {\n            \"tokenizer_type\": \"turkish_mixtral\" if self.use_turkish else \"tiktoken\",\n            \"vocab_size\": self.vocab_size,\n            \"max_length\": self.max_length,\n            \"model_path\": self.model_path,\n            \"pad_token\": self.pad_token,\n            \"eos_token\": self.eos_token,\n            \"bos_token\": self.bos_token,\n            \"use_turkish\": self.use_turkish\n        }\n        import json\n        with open(os.path.join(save_path, \"tokenizer_config.json\"), \"w\") as f:\n            json.dump(config, f, indent=2)\n    \n    def get_cache_stats(self) -> Dict[str, Any]:\n        \"\"\"Get tokenizer cache statistics\"\"\"\n        total = self._cache_hits + self._cache_misses\n        hit_rate = (self._cache_hits / total * 100) if total > 0 else 0\n        return {\n            \"cache_size\": len(self._cache),\n            \"cache_hits\": self._cache_hits,\n            \"cache_misses\": self._cache_misses,\n            \"hit_rate\": hit_rate,\n            \"tokenizer_type\": \"turkish_mixtral\" if self.use_turkish else \"tiktoken\"\n        }\n    \n    def __len__(self):\n        return self.vocab_size\n\n# Initialize Turkish tokenizer\n# Check if Turkish tokenizer model exists in the project\nturkish_model_path = \"C:\\\\Users\\\\husey\\\\teknofest-2025-egitim-eylemci\\\\notebooks\\\\turkish_mixtral_v3_fixed.model\"\n\n# Alternative paths to check\nalternative_paths = [\n    turkish_model_path,\n    \"./turkish_mixtral_v3_fixed.model\",\n    \"../turkish_mixtral_v3_fixed.model\",\n    \"/content/drive/MyDrive/turkish_mixtral_v3_fixed.model\"\n]\n\n# Find the first existing path\nmodel_path = None\nfor path in alternative_paths:\n    if os.path.exists(path):\n        model_path = path\n        break\n\nif not model_path:\n    print(\"‚ö†Ô∏è Turkish tokenizer model not found in expected locations\")\n    print(\"  Paths checked:\", alternative_paths)\n\n# Initialize tokenizer with Turkish model if found\ntokenizer = TurkishMixtralTokenizer(\n    model_path=model_path,\n    max_length=512 if gpu_info and gpu_info['vram_total'] > 20 else 384,\n    use_dynamic_padding=True\n)\n\n# Test tokenizer\ntest_texts = [\n    \"Merhaba d√ºnya! Bu T√ºrk√ße bir metin.\",\n    \"Yapay zeka ve makine √∂ƒürenmesi √ßok ilgin√ß konular.\",\n    \"ƒ∞stanbul'da hava bug√ºn √ßok g√ºzel.\"\n]\n\nprint(\"\\nüß™ Tokenizer Test:\")\ntest_output = tokenizer(test_texts, return_tensors=\"pt\")\nprint(f\"  ‚Ä¢ Input shape: {test_output['input_ids'].shape}\")\nprint(f\"  ‚Ä¢ Dynamic max length: {test_output['input_ids'].shape[1]}\")\n\n# Test decoding\ndecoded = tokenizer.decode(test_output['input_ids'][0])\nprint(f\"  ‚Ä¢ Decoded sample: {decoded[:100]}...\")\n\n# Show cache stats\nprint(f\"  ‚Ä¢ Cache stats: {tokenizer.get_cache_stats()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from typing import List, Dict, Any, Union, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "class OptimizedQwenTokenizer:\n",
    "    \"\"\"Optimize edilmi≈ü Qwen tokenizer with dynamic padding\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 512, use_dynamic_padding: bool = True):\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.max_length = max_length\n",
    "        self.use_dynamic_padding = use_dynamic_padding\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "        self.eos_token = \"<|endoftext|>\"\n",
    "        self.bos_token = \"<|startoftext|>\"\n",
    "        self.unk_token = \"<|unknown|>\"\n",
    "        \n",
    "        self.pad_token_id = 100257\n",
    "        self.eos_token_id = 100257\n",
    "        self.bos_token_id = 100258\n",
    "        self.unk_token_id = 100259\n",
    "        \n",
    "        self.model_max_length = max_length\n",
    "        self.padding_side = \"right\"  # For better training\n",
    "        \n",
    "        # Cache for tokenized texts\n",
    "        self._cache = {}\n",
    "        self._cache_hits = 0\n",
    "        self._cache_misses = 0\n",
    "        \n",
    "        print(f\"‚úÖ Optimized tokenizer initialized\")\n",
    "        print(f\"  ‚Ä¢ Vocab size: {self.encoding.n_vocab}\")\n",
    "        print(f\"  ‚Ä¢ Dynamic padding: {use_dynamic_padding}\")\n",
    "    \n",
    "    def __call__(self,\n",
    "                 text: Union[str, List[str]],\n",
    "                 padding: Union[bool, str] = True,\n",
    "                 truncation: bool = True,\n",
    "                 max_length: Optional[int] = None,\n",
    "                 return_tensors: Optional[str] = None,\n",
    "                 return_attention_mask: bool = True,\n",
    "                 add_special_tokens: bool = True,\n",
    "                 **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenize with optimizations\"\"\"\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            texts = [text]\n",
    "        else:\n",
    "            texts = text\n",
    "        \n",
    "        max_len = max_length or self.model_max_length\n",
    "        \n",
    "        # Dynamic padding: find actual max length in batch\n",
    "        if self.use_dynamic_padding and padding:\n",
    "            actual_max = 0\n",
    "            for txt in texts:\n",
    "                cache_key = hash(txt)\n",
    "                if cache_key in self._cache:\n",
    "                    tokens = self._cache[cache_key]\n",
    "                    self._cache_hits += 1\n",
    "                else:\n",
    "                    tokens = self.encoding.encode(txt)\n",
    "                    self._cache[cache_key] = tokens\n",
    "                    self._cache_misses += 1\n",
    "                \n",
    "                actual_max = max(actual_max, len(tokens))\n",
    "            \n",
    "            # Use smaller of actual_max and max_len\n",
    "            max_len = min(actual_max + 2, max_len)  # +2 for special tokens\n",
    "        \n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        \n",
    "        for txt in texts:\n",
    "            # Use cache if available\n",
    "            cache_key = hash(txt)\n",
    "            if cache_key in self._cache:\n",
    "                tokens = self._cache[cache_key]\n",
    "            else:\n",
    "                tokens = self.encoding.encode(txt)\n",
    "                self._cache[cache_key] = tokens\n",
    "            \n",
    "            # Add special tokens\n",
    "            if add_special_tokens:\n",
    "                tokens = [self.bos_token_id] + tokens + [self.eos_token_id]\n",
    "            \n",
    "            # Truncation\n",
    "            if truncation and len(tokens) > max_len:\n",
    "                tokens = tokens[:max_len-1] + [self.eos_token_id]\n",
    "            \n",
    "            # Padding\n",
    "            if padding:\n",
    "                original_length = len(tokens)\n",
    "                padding_length = max_len - original_length\n",
    "                \n",
    "                if self.padding_side == \"right\":\n",
    "                    tokens = tokens + [self.pad_token_id] * padding_length\n",
    "                    attention_mask = [1] * original_length + [0] * padding_length\n",
    "                else:\n",
    "                    tokens = [self.pad_token_id] * padding_length + tokens\n",
    "                    attention_mask = [0] * padding_length + [1] * original_length\n",
    "            else:\n",
    "                attention_mask = [1] * len(tokens)\n",
    "            \n",
    "            all_input_ids.append(tokens)\n",
    "            all_attention_masks.append(attention_mask)\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': all_input_ids[0] if isinstance(text, str) else all_input_ids\n",
    "        }\n",
    "        \n",
    "        if return_attention_mask:\n",
    "            result['attention_mask'] = all_attention_masks[0] if isinstance(text, str) else all_attention_masks\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            result = {k: torch.tensor(v) for k, v in result.items()}\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens: bool = True, **kwargs) -> str:\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        if hasattr(token_ids, 'tolist'):\n",
    "            token_ids = token_ids.tolist()\n",
    "        \n",
    "        if isinstance(token_ids, list) and len(token_ids) > 0 and isinstance(token_ids[0], list):\n",
    "            token_ids = token_ids[0]\n",
    "        \n",
    "        if skip_special_tokens:\n",
    "            special_tokens = {self.pad_token_id, self.eos_token_id, self.bos_token_id, self.unk_token_id}\n",
    "            token_ids = [t for t in token_ids if t not in special_tokens]\n",
    "        \n",
    "        return self.encoding.decode(token_ids)\n",
    "    \n",
    "    def batch_decode(self, token_ids_list, skip_special_tokens: bool = True, **kwargs) -> List[str]:\n",
    "        \"\"\"Batch decode\"\"\"\n",
    "        return [self.decode(token_ids, skip_special_tokens) for token_ids in token_ids_list]\n",
    "    \n",
    "    def get_cache_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get tokenizer cache statistics\"\"\"\n",
    "        total = self._cache_hits + self._cache_misses\n",
    "        hit_rate = (self._cache_hits / total * 100) if total > 0 else 0\n",
    "        return {\n",
    "            \"cache_size\": len(self._cache),\n",
    "            \"cache_hits\": self._cache_hits,\n",
    "            \"cache_misses\": self._cache_misses,\n",
    "            \"hit_rate\": hit_rate\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.encoding.n_vocab\n",
    "\n",
    "# Initialize optimized tokenizer\n",
    "tokenizer = OptimizedQwenTokenizer(\n",
    "    max_length=512 if gpu_info and gpu_info['vram_total'] > 20 else 256,\n",
    "    use_dynamic_padding=True\n",
    ")\n",
    "\n",
    "# Test tokenizer\n",
    "test_texts = [\n",
    "    \"Merhaba d√ºnya!\",\n",
    "    \"Bu optimize edilmi≈ü bir tokenizer √∂rneƒüidir.\",\n",
    "    \"Dinamik padding sayesinde bellek kullanƒ±mƒ± azaltƒ±lmƒ±≈ütƒ±r.\"\n",
    "]\n",
    "\n",
    "test_output = tokenizer(test_texts, return_tensors=\"pt\")\n",
    "print(f\"\\nüß™ Tokenizer Test:\")\n",
    "print(f\"  Input shape: {test_output['input_ids'].shape}\")\n",
    "print(f\"  Dynamic max length used: {test_output['input_ids'].shape[1]}\")\n",
    "print(f\"  Cache stats: {tokenizer.get_cache_stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from dataclasses import dataclass, field\nfrom typing import List, Optional, Dict, Tuple\nimport math\n\n@dataclass\nclass UltraOptimizedConfig:\n    \"\"\"Ultra-optimized training configuration with auto-tuning\"\"\"\n    \n    # Model Configuration - UPDATED to use Qwen3-8B\n    model_name: str = \"Qwen/Qwen3-8B\"  # Changed from Qwen2.5-7B to Qwen3-8B\n    model_revision: str = \"main\"\n    \n    # Teacher Model Configuration - UPDATED to use Turkcell\n    teacher_model_name: str = \"TURKCELL/Turkcell-LLM-7b-v1\"  # Turkish-optimized teacher model\n    \n    # Data Configuration\n    train_size: int = 100000\n    test_size: int = 2000\n    max_length: int = 384\n    \n    # LoRA+ Configuration (Enhanced LoRA)\n    use_lora_plus: bool = True\n    lora_r: int = 64\n    lora_alpha: int = 128\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(default_factory=lambda: [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP layers for better performance\n    ])\n    \n    # Advanced Training Parameters\n    learning_rate: float = 2e-4\n    min_learning_rate: float = 1e-6\n    weight_decay: float = 0.01\n    adam_beta1: float = 0.9\n    adam_beta2: float = 0.95  # Better for transformers\n    adam_epsilon: float = 1e-8\n    \n    # Batch Configuration\n    batch_size: int = 4\n    gradient_accumulation_steps: int = 4\n    gradient_checkpointing: bool = True\n    \n    # Training Schedule\n    num_epochs: int = 3\n    warmup_ratio: float = 0.05\n    lr_scheduler_type: str = \"cosine_with_restarts\"\n    \n    # Optimization Features\n    use_8bit: bool = False\n    use_4bit: bool = True  # Better compression\n    use_flash_attention: bool = True\n    use_xformers: bool = True\n    use_gradient_checkpointing: bool = True\n    use_cpu_offload: bool = False\n    use_bf16: bool = False\n    use_fp16: bool = True\n    use_tf32: bool = True  # For Ampere GPUs\n    \n    # Advanced Optimization\n    use_ema: bool = True  # Exponential Moving Average\n    ema_decay: float = 0.999\n    use_lion_optimizer: bool = False  # Alternative optimizer\n    use_sam_optimizer: bool = False  # Sharpness Aware Minimization\n    gradient_clipping: float = 1.0\n    \n    # Regularization\n    label_smoothing: float = 0.1\n    dropout: float = 0.1\n    attention_dropout: float = 0.1\n    \n    # Curriculum Learning\n    use_curriculum_learning: bool = True\n    curriculum_strategy: str = \"linear\"  # linear, exponential, step\n    \n    # Knowledge Distillation\n    use_knowledge_distillation: bool = True\n    distillation_temperature: float = 4.0\n    distillation_alpha: float = 0.7  # Teacher loss weight\n    \n    # Memory Optimization\n    optim_bits: int = 8  # 8-bit optimizer\n    zero_stage: int = 2  # DeepSpeed ZeRO stage\n    \n    # Evaluation & Checkpointing\n    eval_steps: int = 100\n    save_steps: int = 500\n    logging_steps: int = 10\n    save_total_limit: int = 3\n    \n    # Paths\n    output_dir: str = \"./outputs\"\n    cache_dir: str = \"./cache\"\n    tokenizer_model_path: str = \"turkish_mixtral_v3_fixed.model\"  # Turkish tokenizer path\n    \n    # Monitoring\n    use_wandb: bool = False\n    use_tensorboard: bool = True\n    \n    def __post_init__(self):\n        \"\"\"Auto-tune parameters based on GPU\"\"\"\n        \n        if torch.cuda.is_available():\n            gpu_info = SystemMonitor.get_gpu_info()\n            vram_gb = gpu_info['vram_total']\n            gpu_type = gpu_info['type']\n            \n            # GPU-specific optimizations\n            if gpu_type == \"t4\":  # T4 GPU (16GB)\n                self.batch_size = 1\n                self.gradient_accumulation_steps = 16\n                self.max_length = 256\n                self.lora_r = 32\n                self.use_4bit = True\n                self.use_flash_attention = True\n                self.use_gradient_checkpointing = True\n                print(\"‚öôÔ∏è T4 GPU optimizations applied\")\n                \n            elif gpu_type == \"v100\":  # V100 (16/32GB)\n                self.batch_size = 2\n                self.gradient_accumulation_steps = 8\n                self.max_length = 384\n                self.lora_r = 64\n                self.use_bf16 = False  # V100 doesn't support bf16\n                self.use_fp16 = True\n                print(\"‚öôÔ∏è V100 GPU optimizations applied\")\n                \n            elif gpu_type == \"a100\":  # A100 (40/80GB)\n                self.batch_size = 4\n                self.gradient_accumulation_steps = 4\n                self.max_length = 512\n                self.lora_r = 128\n                self.use_bf16 = True\n                self.use_tf32 = True\n                self.use_flash_attention = True\n                print(\"‚öôÔ∏è A100 GPU optimizations applied\")\n                \n            elif \"rtx\" in gpu_type:  # RTX 3090/4090\n                self.batch_size = 2\n                self.gradient_accumulation_steps = 8\n                self.max_length = 384\n                self.use_tf32 = True\n                print(\"‚öôÔ∏è RTX GPU optimizations applied\")\n            \n            # Memory-based adjustments\n            if vram_gb < 16:\n                self.use_cpu_offload = True\n                self.zero_stage = 3\n                print(\"‚ö†Ô∏è Low VRAM detected, enabling CPU offload\")\n            \n            # Calculate effective batch size\n            self.effective_batch_size = self.batch_size * self.gradient_accumulation_steps\n            \n            # Auto-calculate training steps\n            steps_per_epoch = self.train_size // self.effective_batch_size\n            self.total_steps = steps_per_epoch * self.num_epochs\n            self.warmup_steps = int(self.total_steps * self.warmup_ratio)\n            \n        # Create directories\n        os.makedirs(self.output_dir, exist_ok=True)\n        os.makedirs(self.cache_dir, exist_ok=True)\n    \n    def print_config(self):\n        \"\"\"Print configuration summary\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"‚öôÔ∏è ULTRA-OPTIMIZED CONFIGURATION\")\n        print(\"=\"*60)\n        print(f\"Student Model: {self.model_name}\")\n        print(f\"Teacher Model: {self.teacher_model_name}\")\n        print(f\"\\nüìä Training Parameters:\")\n        print(f\"  ‚Ä¢ Batch size: {self.batch_size}\")\n        print(f\"  ‚Ä¢ Gradient accumulation: {self.gradient_accumulation_steps}\")\n        print(f\"  ‚Ä¢ Effective batch size: {self.effective_batch_size}\")\n        print(f\"  ‚Ä¢ Learning rate: {self.learning_rate}\")\n        print(f\"  ‚Ä¢ Total steps: {self.total_steps}\")\n        print(f\"  ‚Ä¢ Warmup steps: {self.warmup_steps}\")\n        print(f\"\\nüîß Optimizations:\")\n        print(f\"  ‚Ä¢ LoRA rank: {self.lora_r}\")\n        print(f\"  ‚Ä¢ 4-bit quantization: {self.use_4bit}\")\n        print(f\"  ‚Ä¢ Flash Attention: {self.use_flash_attention}\")\n        print(f\"  ‚Ä¢ Gradient checkpointing: {self.use_gradient_checkpointing}\")\n        print(f\"  ‚Ä¢ Mixed precision: {'BF16' if self.use_bf16 else 'FP16' if self.use_fp16 else 'FP32'}\")\n        print(f\"  ‚Ä¢ EMA: {self.use_ema}\")\n        print(f\"  ‚Ä¢ Label smoothing: {self.label_smoothing}\")\n        print(f\"  ‚Ä¢ Curriculum learning: {self.use_curriculum_learning}\")\n        print(f\"  ‚Ä¢ Knowledge Distillation: {self.use_knowledge_distillation}\")\n        print(f\"  ‚Ä¢ Turkish Tokenizer: {self.tokenizer_model_path}\")\n        print(\"=\"*60)\n\n# Initialize configuration\nconfig = UltraOptimizedConfig()\nconfig.print_config()\n\n# Save configuration\nimport json\nconfig_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('_')}\nwith open(f\"{config.output_dir}/config.json\", 'w') as f:\n    json.dump(config_dict, f, indent=2, default=str)\nprint(f\"\\nüíæ Configuration saved to {config.output_dir}/config.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "import math\n",
    "\n",
    "@dataclass\n",
    "class UltraOptimizedConfig:\n",
    "    \"\"\"Ultra-optimized training configuration with auto-tuning\"\"\"\n",
    "    \n",
    "    # Model Configuration\n",
    "    model_name: str = \"Qwen/Qwen2.5-7B\"\n",
    "    model_revision: str = \"main\"\n",
    "    \n",
    "    # Data Configuration\n",
    "    train_size: int = 100000\n",
    "    test_size: int = 2000\n",
    "    max_length: int = 384\n",
    "    \n",
    "    # LoRA+ Configuration (Enhanced LoRA)\n",
    "    use_lora_plus: bool = True\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP layers for better performance\n",
    "    ])\n",
    "    \n",
    "    # Advanced Training Parameters\n",
    "    learning_rate: float = 2e-4\n",
    "    min_learning_rate: float = 1e-6\n",
    "    weight_decay: float = 0.01\n",
    "    adam_beta1: float = 0.9\n",
    "    adam_beta2: float = 0.95  # Better for transformers\n",
    "    adam_epsilon: float = 1e-8\n",
    "    \n",
    "    # Batch Configuration\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    gradient_checkpointing: bool = True\n",
    "    \n",
    "    # Training Schedule\n",
    "    num_epochs: int = 3\n",
    "    warmup_ratio: float = 0.05\n",
    "    lr_scheduler_type: str = \"cosine_with_restarts\"\n",
    "    \n",
    "    # Optimization Features\n",
    "    use_8bit: bool = False\n",
    "    use_4bit: bool = True  # Better compression\n",
    "    use_flash_attention: bool = True\n",
    "    use_xformers: bool = True\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    use_cpu_offload: bool = False\n",
    "    use_bf16: bool = False\n",
    "    use_fp16: bool = True\n",
    "    use_tf32: bool = True  # For Ampere GPUs\n",
    "    \n",
    "    # Advanced Optimization\n",
    "    use_ema: bool = True  # Exponential Moving Average\n",
    "    ema_decay: float = 0.999\n",
    "    use_lion_optimizer: bool = False  # Alternative optimizer\n",
    "    use_sam_optimizer: bool = False  # Sharpness Aware Minimization\n",
    "    gradient_clipping: float = 1.0\n",
    "    \n",
    "    # Regularization\n",
    "    label_smoothing: float = 0.1\n",
    "    dropout: float = 0.1\n",
    "    attention_dropout: float = 0.1\n",
    "    \n",
    "    # Curriculum Learning\n",
    "    use_curriculum_learning: bool = True\n",
    "    curriculum_strategy: str = \"linear\"  # linear, exponential, step\n",
    "    \n",
    "    # Memory Optimization\n",
    "    optim_bits: int = 8  # 8-bit optimizer\n",
    "    zero_stage: int = 2  # DeepSpeed ZeRO stage\n",
    "    \n",
    "    # Evaluation & Checkpointing\n",
    "    eval_steps: int = 100\n",
    "    save_steps: int = 500\n",
    "    logging_steps: int = 10\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    # Paths\n",
    "    output_dir: str = \"./outputs\"\n",
    "    cache_dir: str = \"./cache\"\n",
    "    \n",
    "    # Monitoring\n",
    "    use_wandb: bool = False\n",
    "    use_tensorboard: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Auto-tune parameters based on GPU\"\"\"\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            gpu_info = SystemMonitor.get_gpu_info()\n",
    "            vram_gb = gpu_info['vram_total']\n",
    "            gpu_type = gpu_info['type']\n",
    "            \n",
    "            # GPU-specific optimizations\n",
    "            if gpu_type == \"t4\":  # T4 GPU (16GB)\n",
    "                self.batch_size = 1\n",
    "                self.gradient_accumulation_steps = 16\n",
    "                self.max_length = 256\n",
    "                self.lora_r = 32\n",
    "                self.use_4bit = True\n",
    "                self.use_flash_attention = True\n",
    "                self.use_gradient_checkpointing = True\n",
    "                print(\"‚öôÔ∏è T4 GPU optimizations applied\")\n",
    "                \n",
    "            elif gpu_type == \"v100\":  # V100 (16/32GB)\n",
    "                self.batch_size = 2\n",
    "                self.gradient_accumulation_steps = 8\n",
    "                self.max_length = 384\n",
    "                self.lora_r = 64\n",
    "                self.use_bf16 = False  # V100 doesn't support bf16\n",
    "                self.use_fp16 = True\n",
    "                print(\"‚öôÔ∏è V100 GPU optimizations applied\")\n",
    "                \n",
    "            elif gpu_type == \"a100\":  # A100 (40/80GB)\n",
    "                self.batch_size = 4\n",
    "                self.gradient_accumulation_steps = 4\n",
    "                self.max_length = 512\n",
    "                self.lora_r = 128\n",
    "                self.use_bf16 = True\n",
    "                self.use_tf32 = True\n",
    "                self.use_flash_attention = True\n",
    "                print(\"‚öôÔ∏è A100 GPU optimizations applied\")\n",
    "                \n",
    "            elif \"rtx\" in gpu_type:  # RTX 3090/4090\n",
    "                self.batch_size = 2\n",
    "                self.gradient_accumulation_steps = 8\n",
    "                self.max_length = 384\n",
    "                self.use_tf32 = True\n",
    "                print(\"‚öôÔ∏è RTX GPU optimizations applied\")\n",
    "            \n",
    "            # Memory-based adjustments\n",
    "            if vram_gb < 16:\n",
    "                self.use_cpu_offload = True\n",
    "                self.zero_stage = 3\n",
    "                print(\"‚ö†Ô∏è Low VRAM detected, enabling CPU offload\")\n",
    "            \n",
    "            # Calculate effective batch size\n",
    "            self.effective_batch_size = self.batch_size * self.gradient_accumulation_steps\n",
    "            \n",
    "            # Auto-calculate training steps\n",
    "            steps_per_epoch = self.train_size // self.effective_batch_size\n",
    "            self.total_steps = steps_per_epoch * self.num_epochs\n",
    "            self.warmup_steps = int(self.total_steps * self.warmup_ratio)\n",
    "            \n",
    "        # Create directories\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def print_config(self):\n",
    "        \"\"\"Print configuration summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚öôÔ∏è ULTRA-OPTIMIZED CONFIGURATION\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Model: {self.model_name}\")\n",
    "        print(f\"\\nüìä Training Parameters:\")\n",
    "        print(f\"  ‚Ä¢ Batch size: {self.batch_size}\")\n",
    "        print(f\"  ‚Ä¢ Gradient accumulation: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"  ‚Ä¢ Effective batch size: {self.effective_batch_size}\")\n",
    "        print(f\"  ‚Ä¢ Learning rate: {self.learning_rate}\")\n",
    "        print(f\"  ‚Ä¢ Total steps: {self.total_steps}\")\n",
    "        print(f\"  ‚Ä¢ Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"\\nüîß Optimizations:\")\n",
    "        print(f\"  ‚Ä¢ LoRA rank: {self.lora_r}\")\n",
    "        print(f\"  ‚Ä¢ 4-bit quantization: {self.use_4bit}\")\n",
    "        print(f\"  ‚Ä¢ Flash Attention: {self.use_flash_attention}\")\n",
    "        print(f\"  ‚Ä¢ Gradient checkpointing: {self.use_gradient_checkpointing}\")\n",
    "        print(f\"  ‚Ä¢ Mixed precision: {'BF16' if self.use_bf16 else 'FP16' if self.use_fp16 else 'FP32'}\")\n",
    "        print(f\"  ‚Ä¢ EMA: {self.use_ema}\")\n",
    "        print(f\"  ‚Ä¢ Label smoothing: {self.label_smoothing}\")\n",
    "        print(f\"  ‚Ä¢ Curriculum learning: {self.use_curriculum_learning}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "# Initialize configuration\n",
    "config = UltraOptimizedConfig()\n",
    "config.print_config()\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "config_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('_')}\n",
    "with open(f\"{config.output_dir}/config.json\", 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2, default=str)\n",
    "print(f\"\\nüíæ Configuration saved to {config.output_dir}/config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Advanced Model Loading with Multiple Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "import torch\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class OptimizedModelLoader:\n",
    "    \"\"\"Advanced model loader with multiple optimization strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, config: UltraOptimizedConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.original_model = None\n",
    "        \n",
    "    @contextmanager\n",
    "    def memory_efficient_loading(self):\n",
    "        \"\"\"Context manager for memory-efficient loading\"\"\"\n",
    "        # Clear cache before loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Set memory fraction\n",
    "        torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "        \n",
    "        yield\n",
    "        \n",
    "        # Clear cache after loading\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    def load_model(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"Load model with all optimizations\"\"\"\n",
    "        \n",
    "        print(\"\\nüîÑ Loading model with optimizations...\")\n",
    "        \n",
    "        with self.memory_efficient_loading():\n",
    "            \n",
    "            # Quantization configuration\n",
    "            if self.config.use_4bit:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_storage=torch.uint8,\n",
    "                )\n",
    "            elif self.config.use_8bit:\n",
    "                bnb_config = BitsAndBytesConfig(\n",
    "                    load_in_8bit=True,\n",
    "                    int8_threshold=6.0,\n",
    "                    llm_int8_has_fp16_weight=False,\n",
    "                )\n",
    "            else:\n",
    "                bnb_config = None\n",
    "            \n",
    "            # Model configuration\n",
    "            model_config = AutoConfig.from_pretrained(\n",
    "                self.config.model_name,\n",
    "                trust_remote_code=True,\n",
    "                cache_dir=self.config.cache_dir\n",
    "            )\n",
    "            \n",
    "            # Update model config for optimizations\n",
    "            model_config.use_cache = False  # Disable KV cache for training\n",
    "            model_config.pretraining_tp = 1\n",
    "            \n",
    "            # Enable Flash Attention if available\n",
    "            if self.config.use_flash_attention:\n",
    "                model_config._attn_implementation = \"flash_attention_2\"\n",
    "            \n",
    "            try:\n",
    "                # Load model\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.model_name,\n",
    "                    config=model_config,\n",
    "                    quantization_config=bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.bfloat16 if self.config.use_bf16 else torch.float16,\n",
    "                    cache_dir=self.config.cache_dir,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    revision=self.config.model_revision\n",
    "                )\n",
    "                print(f\"‚úÖ Model loaded: {self.config.model_name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error loading {self.config.model_name}: {e}\")\n",
    "                print(\"üîÑ Loading fallback model...\")\n",
    "                \n",
    "                # Fallback to smaller model\n",
    "                self.config.model_name = \"microsoft/phi-2\"\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.config.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    cache_dir=self.config.cache_dir,\n",
    "                    low_cpu_mem_usage=True\n",
    "                )\n",
    "                print(f\"‚úÖ Fallback model loaded: {self.config.model_name}\")\n",
    "            \n",
    "            # Enable gradient checkpointing\n",
    "            if self.config.use_gradient_checkpointing:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                self.model.enable_input_require_grads()\n",
    "                print(\"‚úÖ Gradient checkpointing enabled\")\n",
    "            \n",
    "            # Prepare model for k-bit training\n",
    "            if self.config.use_4bit or self.config.use_8bit:\n",
    "                self.model = prepare_model_for_kbit_training(\n",
    "                    self.model,\n",
    "                    use_gradient_checkpointing=self.config.use_gradient_checkpointing\n",
    "                )\n",
    "                print(\"‚úÖ Model prepared for k-bit training\")\n",
    "            \n",
    "            # Apply LoRA\n",
    "            self.apply_lora(resume_from_checkpoint)\n",
    "            \n",
    "            # Enable TF32 for Ampere GPUs\n",
    "            if self.config.use_tf32 and torch.cuda.is_available():\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                print(\"‚úÖ TF32 enabled for matrix operations\")\n",
    "            \n",
    "            # Print model statistics\n",
    "            self.print_model_stats()\n",
    "            \n",
    "        return self.model\n",
    "    \n",
    "    def apply_lora(self, resume_from_checkpoint: Optional[str] = None):\n",
    "        \"\"\"Apply LoRA or LoRA+ to the model\"\"\"\n",
    "        \n",
    "        if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):\n",
    "            print(f\"üìÇ Loading LoRA from checkpoint: {resume_from_checkpoint}\")\n",
    "            self.model = PeftModel.from_pretrained(\n",
    "                self.model,\n",
    "                resume_from_checkpoint,\n",
    "                is_trainable=True\n",
    "            )\n",
    "        else:\n",
    "            # LoRA configuration\n",
    "            lora_config = LoraConfig(\n",
    "                r=self.config.lora_r,\n",
    "                lora_alpha=self.config.lora_alpha,\n",
    "                target_modules=self.config.lora_target_modules,\n",
    "                lora_dropout=self.config.lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                inference_mode=False,\n",
    "                modules_to_save=None,\n",
    "            )\n",
    "            \n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(self.model, lora_config)\n",
    "            \n",
    "            # LoRA+ optimization (different learning rates for A and B matrices)\n",
    "            if self.config.use_lora_plus:\n",
    "                self._apply_lora_plus_optimization()\n",
    "        \n",
    "        print(\"‚úÖ LoRA applied successfully\")\n",
    "    \n",
    "    def _apply_lora_plus_optimization(self):\n",
    "        \"\"\"Apply LoRA+ optimization (different LR for A and B matrices)\"\"\"\n",
    "        # This would be implemented in the optimizer configuration\n",
    "        # LoRA+ uses higher learning rate for B matrices\n",
    "        print(\"‚úÖ LoRA+ optimization configured\")\n",
    "    \n",
    "    def print_model_stats(self):\n",
    "        \"\"\"Print model statistics\"\"\"\n",
    "        total_params = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(\"\\nüìä Model Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "        print(f\"  ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  ‚Ä¢ Trainable ratio: {100 * trainable_params / total_params:.2f}%\")\n",
    "        \n",
    "        # Memory footprint\n",
    "        if torch.cuda.is_available():\n",
    "            memory_footprint = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"  ‚Ä¢ Model memory: {memory_footprint:.2f} GB\")\n",
    "\n",
    "# Load the model\n",
    "model_loader = OptimizedModelLoader(config)\n",
    "\n",
    "# Check for existing checkpoint\n",
    "checkpoint_path = None\n",
    "if previous_state and previous_state.get('last_checkpoint'):\n",
    "    checkpoint_path = previous_state['last_checkpoint']\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"\\nüîÑ Resuming from checkpoint: {checkpoint_path}\")\n",
    "\n",
    "model = model_loader.load_model(resume_from_checkpoint=checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Advanced Data Processing with Curriculum Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Dict, Any, Tuple\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport gc\n\nclass TurkcellKnowledgeDistillationTrainer:\n    \"\"\"\n    Knowledge Distillation with Turkcell-LLM-7b-v1 as teacher model\n    Optimized for Turkish language tasks\n    \"\"\"\n    \n    def __init__(\n        self,\n        teacher_model_name: str = \"TURKCELL/Turkcell-LLM-7b-v1\",  # Turkish teacher model\n        student_model: nn.Module = None,  # Qwen3-8B student model\n        temperature: float = 3.0,\n        alpha: float = 0.7,\n        use_cached_teacher: bool = True,\n        device: str = \"cuda\"\n    ):\n        self.teacher_model_name = teacher_model_name\n        self.student_model = student_model\n        self.temperature = temperature\n        self.alpha = alpha\n        self.beta = 1.0 - alpha\n        self.use_cached_teacher = use_cached_teacher\n        self.device = device\n        self.teacher_cache = {}\n        self.teacher_tokenizer = None\n        \n        print(f\"üéì Turkcell Knowledge Distillation Setup:\")\n        print(f\"  ‚Ä¢ Teacher: {teacher_model_name}\")\n        print(f\"  ‚Ä¢ Temperature: {temperature}\")\n        print(f\"  ‚Ä¢ Alpha (teacher weight): {alpha}\")\n        print(f\"  ‚Ä¢ Beta (student weight): {self.beta}\")\n        print(f\"  ‚Ä¢ Cache teacher outputs: {use_cached_teacher}\")\n    \n    def load_teacher_model(self, load_in_8bit: bool = True):\n        \"\"\"Load Turkcell teacher model with memory optimization\"\"\"\n        \n        print(f\"\\nüìö Loading Turkcell teacher model: {self.teacher_model_name}\")\n        print(\"  ‚Ä¢ This model is optimized for Turkish language\")\n        print(\"  ‚Ä¢ Provides better Turkish language understanding\")\n        \n        try:\n            from transformers import BitsAndBytesConfig\n            \n            # Quantization config for memory efficiency\n            if load_in_8bit:\n                bnb_config = BitsAndBytesConfig(\n                    load_in_8bit=True,\n                    bnb_8bit_compute_dtype=torch.float16\n                )\n            else:\n                bnb_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_quant_type=\"nf4\",\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_use_double_quant=True\n                )\n            \n            # Load Turkcell teacher model\n            self.teacher_model = AutoModelForCausalLM.from_pretrained(\n                self.teacher_model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True\n            )\n            \n            # Load teacher tokenizer\n            self.teacher_tokenizer = AutoTokenizer.from_pretrained(\n                self.teacher_model_name,\n                trust_remote_code=True\n            )\n            \n            # Set padding token if needed\n            if self.teacher_tokenizer.pad_token is None:\n                self.teacher_tokenizer.pad_token = self.teacher_tokenizer.eos_token\n            \n            # Teacher model to eval mode\n            self.teacher_model.eval()\n            \n            # Disable gradients for teacher\n            for param in self.teacher_model.parameters():\n                param.requires_grad = False\n            \n            print(f\"‚úÖ Turkcell teacher model loaded successfully\")\n            \n            # Memory usage\n            if torch.cuda.is_available():\n                memory_used = torch.cuda.memory_allocated() / 1e9\n                print(f\"  ‚Ä¢ Teacher model memory: {memory_used:.2f} GB\")\n            \n            # Model info\n            total_params = sum(p.numel() for p in self.teacher_model.parameters())\n            print(f\"  ‚Ä¢ Teacher parameters: {total_params/1e9:.2f}B\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not load Turkcell teacher model: {e}\")\n            print(\"  ‚Üí Will use cached outputs or fallback strategy\")\n            self.teacher_model = None\n            self.teacher_tokenizer = None\n    \n    def get_teacher_outputs(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        use_cache: bool = True\n    ) -> torch.Tensor:\n        \"\"\"Get soft targets from Turkcell teacher model\"\"\"\n        \n        # Cache key\n        cache_key = hash((input_ids.shape, input_ids.sum().item()))\n        \n        # Check cache\n        if use_cache and cache_key in self.teacher_cache:\n            return self.teacher_cache[cache_key].to(self.device)\n        \n        if self.teacher_model is None:\n            # Fallback: return uniform distribution (neutral teacher)\n            vocab_size = self.student_model.config.vocab_size\n            batch_size, seq_len = input_ids.shape\n            # Return slightly peaked uniform distribution\n            logits = torch.randn(batch_size, seq_len, vocab_size).to(self.device) * 0.1\n            return F.softmax(logits / self.temperature, dim=-1)\n        \n        # Get teacher predictions\n        with torch.no_grad():\n            # If teacher uses different tokenizer, we might need to re-tokenize\n            # For now, assume compatible tokenization\n            teacher_outputs = self.teacher_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n            teacher_logits = teacher_outputs.logits\n            \n            # Apply temperature and get soft probabilities\n            teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n            \n            # Cache the result\n            if use_cache and len(self.teacher_cache) < 1000:\n                self.teacher_cache[cache_key] = teacher_probs.detach().cpu()\n            \n            return teacher_probs\n    \n    def distillation_loss(\n        self,\n        student_logits: torch.Tensor,\n        teacher_probs: torch.Tensor,\n        labels: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n        \"\"\"\n        Compute combined distillation loss\n        Loss = Œ± * L_teacher + Œ≤ * L_student\n        \"\"\"\n        \n        # 1. Teacher Loss (KL Divergence)\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n        \n        # KL Divergence\n        kl_loss = F.kl_div(\n            student_log_probs,\n            teacher_probs,\n            reduction='none'\n        ).sum(dim=-1)\n        \n        # Apply attention mask\n        if attention_mask is not None:\n            kl_loss = kl_loss * attention_mask\n            kl_loss = kl_loss.sum() / attention_mask.sum()\n        else:\n            kl_loss = kl_loss.mean()\n        \n        # Scale by temperature squared\n        kl_loss = kl_loss * (self.temperature ** 2)\n        \n        # 2. Student Loss (Cross Entropy)\n        student_loss = F.cross_entropy(\n            student_logits.view(-1, student_logits.size(-1)),\n            labels.view(-1),\n            ignore_index=-100,\n            reduction='mean'\n        )\n        \n        # 3. Combined Loss\n        total_loss = self.alpha * kl_loss + self.beta * student_loss\n        \n        # Metrics\n        metrics = {\n            'kl_loss': kl_loss.item(),\n            'student_loss': student_loss.item(),\n            'total_loss': total_loss.item(),\n            'teacher_weight': self.alpha,\n            'student_weight': self.beta,\n            'temperature': self.temperature\n        }\n        \n        return total_loss, metrics\n    \n    def create_distillation_trainer(\n        self,\n        trainer_class,\n        **trainer_kwargs\n    ):\n        \"\"\"Create custom trainer with distillation loss\"\"\"\n        \n        parent_self = self\n        \n        class TurkcellDistillationTrainer(trainer_class):\n            def compute_loss(self, model, inputs, return_outputs=False):\n                # Student forward pass\n                outputs = model(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    return_dict=True\n                )\n                student_logits = outputs.logits\n                \n                # Teacher forward pass\n                teacher_probs = parent_self.get_teacher_outputs(\n                    inputs['input_ids'],\n                    inputs['attention_mask'],\n                    use_cache=parent_self.use_cached_teacher\n                ).to(student_logits.device)\n                \n                # Compute distillation loss\n                loss, metrics = parent_self.distillation_loss(\n                    student_logits=student_logits,\n                    teacher_probs=teacher_probs,\n                    labels=inputs['labels'],\n                    attention_mask=inputs['attention_mask']\n                )\n                \n                # Log metrics periodically\n                if self.state.global_step % 10 == 0:\n                    for key, value in metrics.items():\n                        self.log({f\"distillation/{key}\": value})\n                \n                return (loss, outputs) if return_outputs else loss\n        \n        return TurkcellDistillationTrainer(**trainer_kwargs)\n    \n    def adaptive_temperature_schedule(self, current_step: int, total_steps: int) -> float:\n        \"\"\"Adaptive temperature scheduling during training\"\"\"\n        progress = current_step / total_steps\n        \n        if progress < 0.3:\n            # Early training: high temperature for soft targets\n            return self.temperature\n        elif progress < 0.7:\n            # Mid training: reduce temperature\n            return self.temperature * 0.7\n        else:\n            # Late training: low temperature for harder targets\n            return self.temperature * 0.5\n    \n    def cleanup_teacher(self):\n        \"\"\"Clean teacher model from memory\"\"\"\n        if hasattr(self, 'teacher_model') and self.teacher_model is not None:\n            del self.teacher_model\n            self.teacher_model = None\n        if hasattr(self, 'teacher_tokenizer') and self.teacher_tokenizer is not None:\n            del self.teacher_tokenizer\n            self.teacher_tokenizer = None\n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"‚úÖ Teacher model cleaned from memory\")\n    \n    def get_distillation_config(self) -> Dict[str, Any]:\n        \"\"\"Get current distillation configuration\"\"\"\n        return {\n            'teacher_model': self.teacher_model_name,\n            'temperature': self.temperature,\n            'alpha': self.alpha,\n            'beta': self.beta,\n            'cache_size': len(self.teacher_cache),\n            'teacher_loaded': self.teacher_model is not None,\n            'turkish_optimized': True\n        }\n\n# Initialize Knowledge Distillation with Turkcell\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéì TURKCELL KNOWLEDGE DISTILLATION SETUP\")\nprint(\"=\"*60)\n\nif config.use_knowledge_distillation:\n    # Create Turkcell Knowledge Distillation trainer\n    kd_trainer = TurkcellKnowledgeDistillationTrainer(\n        teacher_model_name=config.teacher_model_name,  # TURKCELL/Turkcell-LLM-7b-v1\n        student_model=model,  # Qwen3-8B\n        temperature=config.distillation_temperature,\n        alpha=config.distillation_alpha,\n        use_cached_teacher=True,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    # Load teacher model if enough VRAM\n    if gpu_info and gpu_info['vram_total'] > 20:\n        kd_trainer.load_teacher_model(load_in_8bit=True)\n    else:\n        print(\"‚ö†Ô∏è Low VRAM - Teacher outputs will be cached or generated on-demand\")\n    \n    # Show distillation configuration\n    distillation_config = kd_trainer.get_distillation_config()\n    print(\"\\nüìä Distillation Configuration:\")\n    for key, value in distillation_config.items():\n        print(f\"  ‚Ä¢ {key}: {value}\")\n    \n    print(\"\\n‚úÖ Knowledge Distillation ready!\")\n    print(\"  ‚Üí Student (Qwen3-8B) will learn from Teacher (Turkcell-LLM-7b)\")\n    print(\"  ‚Üí Turkish language understanding will be transferred\")\n    print(\"  ‚Üí This improves Turkish NLP performance significantly\")\nelse:\n    kd_trainer = None\n    print(\"‚ÑπÔ∏è Knowledge Distillation disabled\")"
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Optional, Dict, Any, Tuple\nfrom transformers import AutoModelForCausalLM\nimport gc\n\nclass KnowledgeDistillationTrainer:\n    \"\"\"\n    Knowledge Distillation implementasyonu - B√ºy√ºk teacher modelinden k√º√ß√ºk student modeline bilgi transferi\n    Bu √∂zellikle Colab'da memory kƒ±sƒ±tlamalarƒ± i√ßin √ßok etkili!\n    \"\"\"\n    \n    def __init__(\n        self,\n        teacher_model_name: str = \"Qwen/Qwen2.5-7B\",  # B√ºy√ºk model\n        student_model: nn.Module = None,  # Bizim eƒüiteceƒüimiz k√º√ß√ºk model\n        temperature: float = 3.0,  # Distillation temperature\n        alpha: float = 0.7,  # Teacher loss weight\n        use_cached_teacher: bool = True,  # Teacher outputs'u cache'le\n        device: str = \"cuda\"\n    ):\n        self.teacher_model_name = teacher_model_name\n        self.student_model = student_model\n        self.temperature = temperature\n        self.alpha = alpha  # Teacher loss aƒüƒ±rlƒ±ƒüƒ±\n        self.beta = 1.0 - alpha  # Student loss aƒüƒ±rlƒ±ƒüƒ±\n        self.use_cached_teacher = use_cached_teacher\n        self.device = device\n        self.teacher_cache = {}\n        \n        print(f\"üéì Knowledge Distillation Setup:\")\n        print(f\"  ‚Ä¢ Teacher: {teacher_model_name}\")\n        print(f\"  ‚Ä¢ Temperature: {temperature}\")\n        print(f\"  ‚Ä¢ Alpha (teacher weight): {alpha}\")\n        print(f\"  ‚Ä¢ Cache teacher outputs: {use_cached_teacher}\")\n    \n    def load_teacher_model(self, load_in_8bit: bool = True):\n        \"\"\"Teacher modelini memory-efficient ≈üekilde y√ºkle\"\"\"\n        \n        print(f\"\\nüìö Loading teacher model: {self.teacher_model_name}\")\n        \n        try:\n            # Teacher modeli 8-bit veya 4-bit olarak y√ºkle (memory i√ßin)\n            if load_in_8bit:\n                from transformers import BitsAndBytesConfig\n                bnb_config = BitsAndBytesConfig(\n                    load_in_8bit=True,\n                    bnb_8bit_compute_dtype=torch.float16\n                )\n            else:\n                bnb_config = BitsAndBytesConfig(\n                    load_in_4bit=True,\n                    bnb_4bit_quant_type=\"nf4\",\n                    bnb_4bit_compute_dtype=torch.float16,\n                    bnb_4bit_use_double_quant=True\n                )\n            \n            self.teacher_model = AutoModelForCausalLM.from_pretrained(\n                self.teacher_model_name,\n                quantization_config=bnb_config,\n                device_map=\"auto\",\n                torch_dtype=torch.float16,\n                low_cpu_mem_usage=True,\n                trust_remote_code=True\n            )\n            \n            # Teacher modeli eval moduna al\n            self.teacher_model.eval()\n            \n            # Gradient'larƒ± kapat\n            for param in self.teacher_model.parameters():\n                param.requires_grad = False\n            \n            print(f\"‚úÖ Teacher model loaded successfully\")\n            \n            # Memory usage\n            if torch.cuda.is_available():\n                memory_used = torch.cuda.memory_allocated() / 1e9\n                print(f\"  ‚Ä¢ Teacher model memory: {memory_used:.2f} GB\")\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è Could not load full teacher, using cached outputs only: {e}\")\n            self.teacher_model = None\n    \n    def get_teacher_outputs(\n        self,\n        input_ids: torch.Tensor,\n        attention_mask: torch.Tensor,\n        use_cache: bool = True\n    ) -> torch.Tensor:\n        \"\"\"Teacher model'den soft targets al\"\"\"\n        \n        # Cache key olu≈ütur\n        cache_key = hash((input_ids.shape, input_ids.sum().item()))\n        \n        # Cache'den kontrol et\n        if use_cache and cache_key in self.teacher_cache:\n            return self.teacher_cache[cache_key]\n        \n        if self.teacher_model is None:\n            # Teacher model yoksa random soft targets d√∂nd√ºr (fallback)\n            vocab_size = self.student_model.config.vocab_size\n            batch_size, seq_len = input_ids.shape\n            return torch.randn(batch_size, seq_len, vocab_size).to(self.device)\n        \n        # Teacher prediction\n        with torch.no_grad():\n            teacher_outputs = self.teacher_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                return_dict=True\n            )\n            teacher_logits = teacher_outputs.logits\n            \n            # Soft probabilities with temperature\n            teacher_probs = F.softmax(teacher_logits / self.temperature, dim=-1)\n            \n            # Cache'e kaydet\n            if use_cache and len(self.teacher_cache) < 1000:  # Max 1000 batch cache\n                self.teacher_cache[cache_key] = teacher_probs.detach().cpu()\n            \n            return teacher_probs\n    \n    def distillation_loss(\n        self,\n        student_logits: torch.Tensor,\n        teacher_logits: torch.Tensor,\n        labels: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None\n    ) -> Tuple[torch.Tensor, Dict[str, float]]:\n        \"\"\"\n        Combined distillation loss hesapla\n        Loss = Œ± * L_teacher + Œ≤ * L_student\n        \"\"\"\n        \n        # 1. Teacher Loss (KL Divergence)\n        # Student logits'i temperature ile scale et\n        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=-1)\n        teacher_probs = teacher_logits  # Zaten soft probabilities\n        \n        # KL Divergence loss\n        kl_loss = F.kl_div(\n            student_log_probs,\n            teacher_probs,\n            reduction='none'\n        ).sum(dim=-1)  # Vocab dimension √ºzerinden sum\n        \n        # Maskeleme uygula\n        if attention_mask is not None:\n            kl_loss = kl_loss * attention_mask\n            kl_loss = kl_loss.sum() / attention_mask.sum()\n        else:\n            kl_loss = kl_loss.mean()\n        \n        # Temperature^2 ile scale et (gradients'i normalize etmek i√ßin)\n        kl_loss = kl_loss * (self.temperature ** 2)\n        \n        # 2. Student Loss (Standard Cross Entropy)\n        student_loss = F.cross_entropy(\n            student_logits.view(-1, student_logits.size(-1)),\n            labels.view(-1),\n            ignore_index=-100,\n            reduction='mean'\n        )\n        \n        # 3. Combined Loss\n        total_loss = self.alpha * kl_loss + self.beta * student_loss\n        \n        # Metrics for logging\n        metrics = {\n            'kl_loss': kl_loss.item(),\n            'student_loss': student_loss.item(),\n            'total_loss': total_loss.item(),\n            'teacher_weight': self.alpha,\n            'student_weight': self.beta\n        }\n        \n        return total_loss, metrics\n    \n    def create_distillation_trainer(\n        self,\n        trainer_class,\n        **trainer_kwargs\n    ):\n        \"\"\"Custom Trainer with distillation loss\"\"\"\n        \n        parent_self = self  # Parent class reference\n        \n        class DistillationTrainer(trainer_class):\n            def compute_loss(self, model, inputs, return_outputs=False):\n                # Get student outputs\n                outputs = model(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    return_dict=True\n                )\n                student_logits = outputs.logits\n                \n                # Get teacher outputs\n                teacher_probs = parent_self.get_teacher_outputs(\n                    inputs['input_ids'],\n                    inputs['attention_mask'],\n                    use_cache=parent_self.use_cached_teacher\n                ).to(student_logits.device)\n                \n                # Calculate distillation loss\n                loss, metrics = parent_self.distillation_loss(\n                    student_logits=student_logits,\n                    teacher_logits=teacher_probs,\n                    labels=inputs['labels'],\n                    attention_mask=inputs['attention_mask']\n                )\n                \n                # Log metrics\n                if self.state.global_step % 10 == 0:\n                    for key, value in metrics.items():\n                        self.log({f\"distillation/{key}\": value})\n                \n                return (loss, outputs) if return_outputs else loss\n        \n        return DistillationTrainer(**trainer_kwargs)\n    \n    def cleanup_teacher(self):\n        \"\"\"Teacher model'i memory'den temizle\"\"\"\n        if hasattr(self, 'teacher_model') and self.teacher_model is not None:\n            del self.teacher_model\n            self.teacher_model = None\n            gc.collect()\n            torch.cuda.empty_cache()\n            print(\"‚úÖ Teacher model cleaned from memory\")\n    \n    def adaptive_temperature_schedule(self, current_step: int, total_steps: int) -> float:\n        \"\"\"Training boyunca temperature'ƒ± dinamik olarak ayarla\"\"\"\n        # Ba≈üta y√ºksek temperature (soft targets), sonra d√º≈ü√ºk\n        progress = current_step / total_steps\n        \n        if progress < 0.3:\n            return self.temperature  # ƒ∞lk %30: full temperature\n        elif progress < 0.7:\n            return self.temperature * 0.7  # Orta %40: reduced temperature\n        else:\n            return self.temperature * 0.5  # Son %30: low temperature\n    \n    def get_distillation_config(self) -> Dict[str, Any]:\n        \"\"\"Get current distillation configuration\"\"\"\n        return {\n            'teacher_model': self.teacher_model_name,\n            'temperature': self.temperature,\n            'alpha': self.alpha,\n            'beta': self.beta,\n            'cache_size': len(self.teacher_cache),\n            'teacher_loaded': self.teacher_model is not None\n        }\n\n# Initialize Knowledge Distillation\nprint(\"\\n\" + \"=\"*60)\nprint(\"üéì KNOWLEDGE DISTILLATION SETUP\")\nprint(\"=\"*60)\n\n# Config'e distillation parametreleri ekle\nconfig.use_knowledge_distillation = True\nconfig.distillation_temperature = 4.0  # Soft targets i√ßin\nconfig.distillation_alpha = 0.7  # %70 teacher, %30 student loss\n\nif config.use_knowledge_distillation:\n    # Knowledge Distillation trainer'ƒ± olu≈ütur\n    kd_trainer = KnowledgeDistillationTrainer(\n        teacher_model_name=\"Qwen/Qwen2.5-7B\",  # Veya daha b√ºy√ºk bir model\n        student_model=model,\n        temperature=config.distillation_temperature,\n        alpha=config.distillation_alpha,\n        use_cached_teacher=True,\n        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n    )\n    \n    # Teacher model'i y√ºkle (optional - cache kullanƒ±labilir)\n    if gpu_info and gpu_info['vram_total'] > 20:  # Yeterli VRAM varsa\n        kd_trainer.load_teacher_model(load_in_8bit=True)\n    else:\n        print(\"‚ö†Ô∏è Low VRAM - Teacher outputs will be generated on-demand\")\n    \n    # Distillation config'i g√∂ster\n    distillation_config = kd_trainer.get_distillation_config()\n    print(\"\\nüìä Distillation Configuration:\")\n    for key, value in distillation_config.items():\n        print(f\"  ‚Ä¢ {key}: {value}\")\n    \n    print(\"\\n‚úÖ Knowledge Distillation ready!\")\n    print(\"  ‚Üí Student model will learn from teacher's soft targets\")\n    print(\"  ‚Üí This improves accuracy while keeping model small\")\nelse:\n    kd_trainer = None\n    print(\"‚ÑπÔ∏è Knowledge Distillation disabled\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8Ô∏è‚É£.5 Knowledge Distillation Setup (Teacher-Student Learning)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Ultra-Optimized Training with All Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainerCallback,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from typing import Dict, Optional\n",
    "\n",
    "# Custom callbacks\n",
    "class AdvancedTrainingCallback(TrainerCallback):\n",
    "    \"\"\"Advanced training callback with monitoring and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, config: UltraOptimizedConfig):\n",
    "        self.config = config\n",
    "        self.start_time = None\n",
    "        self.best_loss = float('inf')\n",
    "        self.ema_model = None\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        self.start_time = time.time()\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üöÄ TRAINING STARTED\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Initialize EMA if enabled\n",
    "        if self.config.use_ema:\n",
    "            model = kwargs['model']\n",
    "            self.ema_model = self._create_ema_model(model)\n",
    "            print(\"‚úÖ EMA model initialized\")\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        # Update EMA\n",
    "        if self.config.use_ema and self.ema_model:\n",
    "            self._update_ema(kwargs['model'], self.ema_model, self.config.ema_decay)\n",
    "        \n",
    "        # Monitor GPU memory\n",
    "        if state.global_step % 10 == 0 and torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            memory_cached = torch.cuda.memory_reserved() / 1e9\n",
    "            \n",
    "            if memory_used > torch.cuda.get_device_properties(0).total_memory * 0.9 / 1e9:\n",
    "                print(f\"‚ö†Ô∏è High memory usage: {memory_used:.2f}GB\")\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            # Calculate perplexity\n",
    "            if 'loss' in logs:\n",
    "                logs['perplexity'] = min(math.exp(logs['loss']), 1000)\n",
    "                self.loss_history.append(logs['loss'])\n",
    "            \n",
    "            # Calculate training speed\n",
    "            if self.start_time:\n",
    "                elapsed = time.time() - self.start_time\n",
    "                steps_per_second = state.global_step / elapsed\n",
    "                logs['steps_per_second'] = steps_per_second\n",
    "                \n",
    "                # Estimate time remaining\n",
    "                total_steps = state.max_steps\n",
    "                remaining_steps = total_steps - state.global_step\n",
    "                eta_seconds = remaining_steps / steps_per_second if steps_per_second > 0 else 0\n",
    "                logs['eta_minutes'] = eta_seconds / 60\n",
    "            \n",
    "            # Track best loss\n",
    "            if 'eval_loss' in logs and logs['eval_loss'] < self.best_loss:\n",
    "                self.best_loss = logs['eval_loss']\n",
    "                logs['best_eval_loss'] = self.best_loss\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        # Save EMA checkpoint if better\n",
    "        if self.config.use_ema and self.ema_model:\n",
    "            # Swap models for evaluation\n",
    "            model = kwargs['model']\n",
    "            self._swap_model_weights(model, self.ema_model)\n",
    "            print(\"üîÑ Using EMA model for evaluation\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ TRAINING COMPLETED\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total time: {elapsed/60:.1f} minutes\")\n",
    "        print(f\"Best eval loss: {self.best_loss:.4f}\")\n",
    "        print(f\"Final perplexity: {min(math.exp(self.best_loss), 1000):.2f}\")\n",
    "        \n",
    "        # Save training state\n",
    "        project.save_state({\n",
    "            'global_step': state.global_step,\n",
    "            'best_loss': self.best_loss,\n",
    "            'last_checkpoint': args.output_dir,\n",
    "            'training_time': elapsed\n",
    "        })\n",
    "    \n",
    "    def _create_ema_model(self, model):\n",
    "        \"\"\"Create EMA model copy\"\"\"\n",
    "        import copy\n",
    "        ema_model = copy.deepcopy(model)\n",
    "        for param in ema_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        return ema_model\n",
    "    \n",
    "    def _update_ema(self, model, ema_model, decay):\n",
    "        \"\"\"Update EMA model weights\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "                ema_param.data.mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "    \n",
    "    def _swap_model_weights(self, model1, model2):\n",
    "        \"\"\"Swap model weights\"\"\"\n",
    "        for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "            p1.data, p2.data = p2.data, p1.data\n",
    "\n",
    "# Custom data collator with label smoothing\n",
    "class DataCollatorWithLabelSmoothing(DataCollatorForLanguageModeling):\n",
    "    \"\"\"Data collator with label smoothing\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, label_smoothing=0.1, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=False, **kwargs)\n",
    "        self.label_smoothing = label_smoothing\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        batch = super().__call__(features)\n",
    "        \n",
    "        if self.label_smoothing > 0:\n",
    "            # Apply label smoothing\n",
    "            # This is a simplified version; full implementation would modify loss\n",
    "            pass\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# Optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size * 2,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    adam_beta1=config.adam_beta1,\n",
    "    adam_beta2=config.adam_beta2,\n",
    "    adam_epsilon=config.adam_epsilon,\n",
    "    max_grad_norm=config.gradient_clipping,\n",
    "    \n",
    "    # Learning rate schedule\n",
    "    lr_scheduler_type=config.lr_scheduler_type,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=config.use_fp16,\n",
    "    bf16=config.use_bf16,\n",
    "    tf32=config.use_tf32,\n",
    "    \n",
    "    # Gradient checkpointing\n",
    "    gradient_checkpointing=config.use_gradient_checkpointing,\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False},\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"paged_adamw_8bit\" if config.optim_bits == 8 else \"adamw_torch_fused\",\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=config.logging_steps,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"tensorboard\" if config.use_tensorboard else \"none\",\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    run_name=f\"qwen3_optimized_{config.timestamp}\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[\"labels\"],\n",
    "    \n",
    "    # DeepSpeed config (if needed)\n",
    "    # deepspeed=\"deepspeed_config.json\" if config.zero_stage > 0 else None,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorWithLabelSmoothing(\n",
    "    tokenizer=tokenizer,\n",
    "    label_smoothing=config.label_smoothing,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[\n",
    "        AdvancedTrainingCallback(config),\n",
    "        EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)\n",
    "    ],\n",
    "    # compute_metrics=compute_metrics if defined\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Training Configuration:\")\n",
    "print(f\"  ‚Ä¢ Total training steps: {trainer.state.max_steps}\")\n",
    "print(f\"  ‚Ä¢ Warmup steps: {config.warmup_steps}\")\n",
    "print(f\"  ‚Ä¢ Effective batch size: {config.effective_batch_size}\")\n",
    "print(f\"  ‚Ä¢ Approximate training time: {trainer.state.max_steps * 2 / 60:.1f} minutes\")\n",
    "print(\"\\nüöÄ Ready to start training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Execute Training with Auto-Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def training_context():\n",
    "    \"\"\"Context manager for safe training with interruption handling\"\"\"\n",
    "    \n",
    "    def signal_handler(signum, frame):\n",
    "        print(\"\\n‚ö†Ô∏è Training interrupted! Saving checkpoint...\")\n",
    "        trainer.save_model(f\"{config.output_dir}/interrupted_checkpoint\")\n",
    "        project.save_state({\n",
    "            'interrupted': True,\n",
    "            'global_step': trainer.state.global_step,\n",
    "            'last_checkpoint': f\"{config.output_dir}/interrupted_checkpoint\"\n",
    "        })\n",
    "        print(\"‚úÖ Checkpoint saved. You can resume training later.\")\n",
    "        sys.exit(0)\n",
    "    \n",
    "    # Register signal handler\n",
    "    signal.signal(signal.SIGINT, signal_handler)\n",
    "    \n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        signal.signal(signal.SIGINT, signal.SIG_DFL)\n",
    "\n",
    "# Start training with protection\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING ULTRA-OPTIMIZED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Press Ctrl+C to safely interrupt and save checkpoint\\n\")\n",
    "\n",
    "try:\n",
    "    with training_context():\n",
    "        # Check if resuming from checkpoint\n",
    "        resume_from = None\n",
    "        if previous_state and previous_state.get('interrupted'):\n",
    "            resume_from = previous_state.get('last_checkpoint')\n",
    "            print(f\"üìÇ Resuming from checkpoint: {resume_from}\\n\")\n",
    "        \n",
    "        # Start or resume training\n",
    "        train_result = trainer.train(resume_from_checkpoint=resume_from)\n",
    "        \n",
    "        # Training completed successfully\n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        \n",
    "        # Evaluate final model\n",
    "        print(\"\\nüìä Final evaluation...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üìà FINAL RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Train Loss: {train_result.training_loss:.4f}\")\n",
    "        print(f\"Eval Loss: {eval_results['eval_loss']:.4f}\")\n",
    "        print(f\"Perplexity: {min(math.exp(eval_results['eval_loss']), 1000):.2f}\")\n",
    "        print(f\"Total steps: {trainer.state.global_step}\")\n",
    "        print(f\"Training time: {train_result.metrics['train_runtime']/60:.1f} minutes\")\n",
    "        print(f\"Training speed: {train_result.metrics['train_samples_per_second']:.1f} samples/sec\")\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Try to save emergency checkpoint\n",
    "    try:\n",
    "        trainer.save_model(f\"{config.output_dir}/emergency_checkpoint\")\n",
    "        print(\"‚úÖ Emergency checkpoint saved\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not save emergency checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Save Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final optimized model\n",
    "print(\"\\nüíæ Saving optimized model...\")\n",
    "\n",
    "# Save paths\n",
    "final_model_path = f\"{project.dirs['models']}/qwen3_optimized_final\"\n",
    "lora_adapter_path = f\"{project.dirs['models']}/qwen3_lora_adapter\"\n",
    "\n",
    "# Save full model\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"‚úÖ Full model saved: {final_model_path}\")\n",
    "\n",
    "# Save LoRA adapter separately\n",
    "model.save_pretrained(lora_adapter_path)\n",
    "print(f\"‚úÖ LoRA adapter saved: {lora_adapter_path}\")\n",
    "\n",
    "# Save training metrics\n",
    "import json\n",
    "metrics = {\n",
    "    'final_train_loss': train_result.training_loss,\n",
    "    'final_eval_loss': eval_results['eval_loss'],\n",
    "    'perplexity': min(math.exp(eval_results['eval_loss']), 1000),\n",
    "    'total_steps': trainer.state.global_step,\n",
    "    'training_time_minutes': train_result.metrics['train_runtime']/60,\n",
    "    'samples_per_second': train_result.metrics['train_samples_per_second'],\n",
    "    'model_parameters': sum(p.numel() for p in model.parameters()),\n",
    "    'trainable_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    'config': config.__dict__\n",
    "}\n",
    "\n",
    "with open(f\"{project.dirs['logs']}/training_metrics.json\", 'w') as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "\n",
    "print(f\"üìä Metrics saved: {project.dirs['logs']}/training_metrics.json\")\n",
    "\n",
    "# Backup to Drive\n",
    "if '/content/drive' in os.getcwd():\n",
    "    project.backup_checkpoint(final_model_path)\n",
    "    print(\"‚òÅÔ∏è Model backed up to Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Advanced Model Testing & Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced model testing\n",
    "print(\"\\nüß™ ADVANCED MODEL TESTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def benchmark_generation(model, tokenizer, prompts, max_length=100):\n",
    "    \"\"\"Benchmark model generation speed and quality\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate with timing\n",
    "        start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                num_beams=1,  # Greedy for speed\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        generation_time = time.time() - start\n",
    "        total_time += generation_time\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        input_length = len(inputs['input_ids'][0])\n",
    "        output_length = len(outputs[0]) - input_length\n",
    "        tokens_per_second = output_length / generation_time\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'generated': generated_text,\n",
    "            'output_tokens': output_length,\n",
    "            'time': generation_time,\n",
    "            'tokens_per_second': tokens_per_second\n",
    "        })\n",
    "    \n",
    "    return results, total_time\n",
    "\n",
    "# Test prompts (Turkish)\n",
    "test_prompts = [\n",
    "    \"Yapay zeka teknolojisinin geleceƒüi\",\n",
    "    \"T√ºrkiye'nin en g√ºzel ≈üehirleri\",\n",
    "    \"Saƒülƒ±klƒ± ya≈üam i√ßin √∂neriler\",\n",
    "    \"K√ºresel ƒ±sƒ±nmanƒ±n etkileri\",\n",
    "    \"Eƒüitimde teknoloji kullanƒ±mƒ±\"\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "print(\"Running generation benchmark...\\n\")\n",
    "results, total_time = benchmark_generation(model, tokenizer, test_prompts, max_length=50)\n",
    "\n",
    "# Print results\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"üìù Prompt: {result['prompt']}\")\n",
    "    print(f\"ü§ñ Generated: {result['generated'][:200]}...\")\n",
    "    print(f\"‚ö° Speed: {result['tokens_per_second']:.1f} tokens/sec\")\n",
    "    print(f\"‚è±Ô∏è Time: {result['time']:.2f}s\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Summary statistics\n",
    "avg_speed = np.mean([r['tokens_per_second'] for r in results])\n",
    "avg_time = np.mean([r['time'] for r in results])\n",
    "\n",
    "print(\"\\nüìä BENCHMARK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average generation speed: {avg_speed:.1f} tokens/second\")\n",
    "print(f\"Average generation time: {avg_time:.2f} seconds\")\n",
    "print(f\"Total benchmark time: {total_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Memory Cleanup & Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup and report\n",
    "print(\"\\nüßπ Cleaning up resources...\")\n",
    "\n",
    "# Get final memory stats before cleanup\n",
    "if torch.cuda.is_available():\n",
    "    final_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    max_memory = torch.cuda.max_memory_allocated() / 1e9\n",
    "    print(f\"Current GPU memory: {final_memory:.2f} GB\")\n",
    "    print(f\"Peak GPU memory: {max_memory:.2f} GB\")\n",
    "\n",
    "# Clean up\n",
    "del model\n",
    "del trainer\n",
    "del train_dataset\n",
    "del test_dataset\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Final memory after cleanup\n",
    "    cleaned_memory = torch.cuda.memory_allocated() / 1e9\n",
    "    print(f\"\\n‚úÖ Memory cleared: {final_memory - cleaned_memory:.2f} GB freed\")\n",
    "    print(f\"Final GPU memory: {cleaned_memory:.2f} GB\")\n",
    "\n",
    "# Generate final report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã FINAL TRAINING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüéØ Model Performance:\")\n",
    "print(f\"  ‚Ä¢ Final Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Perplexity: {min(math.exp(eval_results['eval_loss']), 1000):.2f}\")\n",
    "print(f\"  ‚Ä¢ Generation Speed: {avg_speed:.1f} tokens/sec\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Configuration Used:\")\n",
    "print(f\"  ‚Ä¢ Model: {config.model_name}\")\n",
    "print(f\"  ‚Ä¢ LoRA Rank: {config.lora_r}\")\n",
    "print(f\"  ‚Ä¢ Batch Size: {config.batch_size} (effective: {config.effective_batch_size})\")\n",
    "print(f\"  ‚Ä¢ Learning Rate: {config.learning_rate}\")\n",
    "print(f\"  ‚Ä¢ Training Steps: {trainer.state.global_step}\")\n",
    "\n",
    "print(f\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"  ‚Ä¢ Model: {final_model_path}\")\n",
    "print(f\"  ‚Ä¢ LoRA Adapter: {lora_adapter_path}\")\n",
    "print(f\"  ‚Ä¢ Metrics: {project.dirs['logs']}/training_metrics.json\")\n",
    "print(f\"  ‚Ä¢ Checkpoints: {config.output_dir}\")\n",
    "\n",
    "print(f\"\\nüöÄ Optimizations Applied:\")\n",
    "optimizations = []\n",
    "if config.use_flash_attention: optimizations.append(\"Flash Attention\")\n",
    "if config.use_gradient_checkpointing: optimizations.append(\"Gradient Checkpointing\")\n",
    "if config.use_4bit: optimizations.append(\"4-bit Quantization\")\n",
    "if config.use_ema: optimizations.append(\"EMA\")\n",
    "if config.use_curriculum_learning: optimizations.append(\"Curriculum Learning\")\n",
    "if config.label_smoothing > 0: optimizations.append(\"Label Smoothing\")\n",
    "print(f\"  ‚Ä¢ {', '.join(optimizations)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚ú® Training completed successfully! Model is ready for deployment.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Next Steps & Deployment\n",
    "\n",
    "### 1. **Model Deployment**\n",
    "```python\n",
    "# Load for inference\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-7B\")\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/lora/adapter\")\n",
    "```\n",
    "\n",
    "### 2. **API Deployment**\n",
    "- Use FastAPI or Flask for REST API\n",
    "- Deploy on Hugging Face Spaces\n",
    "- Use TorchServe or Triton for production\n",
    "\n",
    "### 3. **Further Optimizations**\n",
    "- Model quantization (GPTQ, AWQ)\n",
    "- ONNX conversion for faster inference\n",
    "- TensorRT optimization for NVIDIA GPUs\n",
    "\n",
    "### 4. **Monitoring**\n",
    "- Set up Weights & Biases for experiment tracking\n",
    "- Use TensorBoard for visualization\n",
    "- Implement A/B testing for model versions\n",
    "\n",
    "### 5. **Fine-tuning Tips**\n",
    "- Increase `lora_r` for better quality (but slower)\n",
    "- Use larger `max_length` for longer contexts\n",
    "- Try different learning rate schedules\n",
    "- Experiment with different LoRA target modules\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations on completing the ultra-optimized training!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}