{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Qwen3-8B Turkish 200K Training - Google Colab Version\n",
    "\n",
    "Bu notebook, Qwen3-8B modelini T√ºrk√ße veri seti ile Google Colab'da eƒüitmek i√ßin optimize edilmi≈ütir.\n",
    "\n",
    "**√ñzellikler:**\n",
    "- ‚úÖ Tiktoken tokenizer desteƒüi\n",
    "- ‚úÖ LoRA fine-tuning\n",
    "- ‚úÖ Memory optimization\n",
    "- ‚úÖ Curriculum learning\n",
    "- ‚úÖ Auto batch size calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ GPU Kontrol√º ve Sistem Bilgisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU kontrol√º\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"\\nPyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Gerekli K√ºt√ºphanelerin Y√ºklenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Sessiz kurulum i√ßin %%capture kullanƒ±lƒ±yor\n",
    "\n",
    "# Temel k√ºt√ºphaneler\n",
    "!pip install -q --upgrade pip\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Transformers ve ilgili k√ºt√ºphaneler\n",
    "!pip install -q transformers==4.44.0\n",
    "!pip install -q datasets==2.14.0\n",
    "!pip install -q accelerate==0.32.0\n",
    "!pip install -q peft==0.11.1\n",
    "!pip install -q bitsandbytes==0.43.1\n",
    "\n",
    "# Tiktoken for Qwen3\n",
    "!pip install -q tiktoken\n",
    "\n",
    "# Diƒüer yardƒ±mcƒ± k√ºt√ºphaneler\n",
    "!pip install -q tqdm\n",
    "!pip install -q psutil\n",
    "!pip install -q sentencepiece\n",
    "\n",
    "print(\"‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Google Drive Baƒülantƒ±sƒ± (Opsiyonel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive'a baƒülan (veri seti ve model kaydetmek i√ßin)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# √áalƒ±≈üma dizini olu≈ütur\n",
    "import os\n",
    "WORK_DIR = '/content/drive/MyDrive/qwen3_training'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"üìÅ √áalƒ±≈üma dizini: {WORK_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Veri Setini Y√ºkleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini indir veya y√ºkle\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "import os\n",
    "\n",
    "DATASET_PATH = \"turkish_200k_dataset\"\n",
    "\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(\"üìÇ Mevcut veri seti y√ºkleniyor...\")\n",
    "    dataset = load_from_disk(DATASET_PATH)\n",
    "else:\n",
    "    print(\"üì• Hugging Face'den veri seti y√ºkleniyor...\")\n",
    "    \n",
    "    # Huseyin/turkish-200k-dataset veri setini y√ºkle\n",
    "    dataset = load_dataset(\"Huseyin/turkish-200k-dataset\", split=\"train\")\n",
    "    \n",
    "    # Veri setini yerel olarak kaydet\n",
    "    dataset.save_to_disk(DATASET_PATH)\n",
    "    print(\"üíæ Veri seti yerel olarak kaydedildi\")\n",
    "    \n",
    "print(f\"‚úÖ Veri seti hazƒ±r: {len(dataset)} √∂rnek\")\n",
    "print(f\"üìä Veri seti s√ºtunlarƒ±: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Tiktoken Tokenizer Kurulumu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen3 Tiktoken Tokenizer Wrapper\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "class Qwen3TiktokenTokenizer:\n",
    "    \"\"\"Qwen3 i√ßin tiktoken tabanlƒ± tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, max_length: int = 512):\n",
    "        self.encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        self.pad_token = \"<|endoftext|>\"\n",
    "        self.eos_token = \"<|endoftext|>\"\n",
    "        self.pad_token_id = 100257\n",
    "        self.eos_token_id = 100257\n",
    "        self.model_max_length = max_length\n",
    "        self.padding_side = \"left\"\n",
    "        print(f\"‚úÖ Tiktoken tokenizer y√ºklendi (vocab size: {self.encoding.n_vocab})\")\n",
    "    \n",
    "    def __call__(self, \n",
    "                 text: Union[str, List[str]], \n",
    "                 padding: bool = True,\n",
    "                 truncation: bool = True,\n",
    "                 max_length: Optional[int] = None,\n",
    "                 return_tensors: Optional[str] = None,\n",
    "                 **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Tokenize text\"\"\"\n",
    "        \n",
    "        if isinstance(text, str):\n",
    "            texts = [text]\n",
    "        else:\n",
    "            texts = text\n",
    "        \n",
    "        max_len = max_length or self.model_max_length\n",
    "        \n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        \n",
    "        for txt in texts:\n",
    "            tokens = self.encoding.encode(txt)\n",
    "            \n",
    "            if truncation and len(tokens) > max_len:\n",
    "                tokens = tokens[:max_len]\n",
    "            \n",
    "            if padding:\n",
    "                original_length = len(tokens)\n",
    "                if self.padding_side == \"left\":\n",
    "                    pad_length = max_len - original_length\n",
    "                    tokens = [self.pad_token_id] * pad_length + tokens\n",
    "                    attention_mask = [0] * pad_length + [1] * original_length\n",
    "                else:\n",
    "                    tokens = tokens + [self.pad_token_id] * (max_len - original_length)\n",
    "                    attention_mask = [1] * original_length + [0] * (max_len - original_length)\n",
    "            else:\n",
    "                attention_mask = [1] * len(tokens)\n",
    "            \n",
    "            all_input_ids.append(tokens)\n",
    "            all_attention_masks.append(attention_mask)\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': all_input_ids[0] if isinstance(text, str) else all_input_ids,\n",
    "            'attention_mask': all_attention_masks[0] if isinstance(text, str) else all_attention_masks\n",
    "        }\n",
    "        \n",
    "        if return_tensors == \"pt\":\n",
    "            result['input_ids'] = torch.tensor(result['input_ids'])\n",
    "            result['attention_mask'] = torch.tensor(result['attention_mask'])\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def encode(self, text: str, **kwargs) -> List[int]:\n",
    "        return self.encoding.encode(text)\n",
    "    \n",
    "    def decode(self, token_ids, skip_special_tokens: bool = True, **kwargs) -> str:\n",
    "        if hasattr(token_ids, 'tolist'):\n",
    "            token_ids = token_ids.tolist()\n",
    "        if isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "        if isinstance(token_ids, list) and len(token_ids) > 0 and isinstance(token_ids[0], list):\n",
    "            token_ids = token_ids[0]\n",
    "        if skip_special_tokens and isinstance(token_ids, list):\n",
    "            token_ids = [t for t in token_ids if t not in [self.pad_token_id, self.eos_token_id]]\n",
    "        return self.encoding.decode(token_ids)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.encoding.n_vocab\n",
    "\n",
    "# Test tokenizer\n",
    "tokenizer = Qwen3TiktokenTokenizer(max_length=512)\n",
    "test_text = \"Merhaba, bu bir test metnidir.\"\n",
    "tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"Test tokenization: {tokens['input_ids'].shape}\")\n",
    "decoded = tokenizer.decode(tokens['input_ids'][0])\n",
    "print(f\"Decoded: {decoded[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Optimized Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Dict\n",
    "import torch\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Colab i√ßin optimize edilmi≈ü training configuration\"\"\"\n",
    "    \n",
    "    # Model\n",
    "    model_name: str = \"Qwen/Qwen2.5-7B\"  # Stable version\n",
    "    \n",
    "    # Data\n",
    "    train_size: int = 50000  # Colab i√ßin azaltƒ±ldƒ±\n",
    "    test_size: int = 1000\n",
    "    max_length: int = 256  # Memory i√ßin azaltƒ±ldƒ±\n",
    "    \n",
    "    # LoRA\n",
    "    lora_r: int = 32  # Colab i√ßin optimize edildi\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Training\n",
    "    learning_rate: float = 5e-5\n",
    "    batch_size: int = 2  # Colab GPU i√ßin\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    num_epochs: int = 1\n",
    "    \n",
    "    # Optimization\n",
    "    use_8bit: bool = True  # 8-bit quantization\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    use_bf16: bool = False  # Colab T4 i√ßin False\n",
    "    use_fp16: bool = True  # T4 i√ßin FP16\n",
    "    \n",
    "    # Paths\n",
    "    output_dir: str = \"./outputs\"\n",
    "    cache_dir: str = \"./cache\"\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_steps: int = 500\n",
    "    save_steps: int = 1000\n",
    "    logging_steps: int = 50\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        # Auto-adjust for GPU\n",
    "        if torch.cuda.is_available():\n",
    "            vram_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "            \n",
    "            if vram_gb < 16:  # T4 GPU (15GB)\n",
    "                self.batch_size = 1\n",
    "                self.gradient_accumulation_steps = 16\n",
    "                self.max_length = 256\n",
    "                self.lora_r = 16\n",
    "                print(f\"‚öôÔ∏è T4 GPU detected ({vram_gb:.1f}GB) - Using conservative settings\")\n",
    "            elif vram_gb < 25:  # A100 40GB\n",
    "                self.batch_size = 2\n",
    "                self.gradient_accumulation_steps = 8\n",
    "                self.max_length = 384\n",
    "                print(f\"‚öôÔ∏è Mid-range GPU detected ({vram_gb:.1f}GB)\")\n",
    "            else:\n",
    "                self.batch_size = 4\n",
    "                self.gradient_accumulation_steps = 4\n",
    "                self.max_length = 512\n",
    "                print(f\"‚öôÔ∏è High-end GPU detected ({vram_gb:.1f}GB)\")\n",
    "        \n",
    "        # Create directories\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "\n",
    "# Initialize config\n",
    "config = TrainingConfig()\n",
    "print(f\"\\nüìã Configuration:\")\n",
    "print(f\"  ‚Ä¢ Batch size: {config.batch_size}\")\n",
    "print(f\"  ‚Ä¢ Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "print(f\"  ‚Ä¢ Max length: {config.max_length}\")\n",
    "print(f\"  ‚Ä¢ LoRA rank: {config.lora_r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Model Loading with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "def load_model_with_lora(config: TrainingConfig):\n",
    "    \"\"\"Model ve LoRA y√ºkleme\"\"\"\n",
    "    \n",
    "    print(\"üîÑ Model y√ºkleniyor...\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Quantization config\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=config.use_8bit,\n",
    "        load_in_4bit=not config.use_8bit,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16 if config.use_fp16 else torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16 if config.use_fp16 else torch.bfloat16,\n",
    "            cache_dir=config.cache_dir,\n",
    "            use_cache=False,\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è {config.model_name} y√ºklenemedi: {e}\")\n",
    "        print(\"üîÑ Alternatif model deneniyor...\")\n",
    "        \n",
    "        # Fallback to smaller model\n",
    "        config.model_name = \"microsoft/phi-2\"\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_name,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            cache_dir=config.cache_dir,\n",
    "            use_cache=False\n",
    "        )\n",
    "    \n",
    "    # Enable gradient checkpointing\n",
    "    if config.use_gradient_checkpointing:\n",
    "        model.gradient_checkpointing_enable()\n",
    "        model.enable_input_require_grads()\n",
    "    \n",
    "    # Prepare for LoRA\n",
    "    model = prepare_model_for_kbit_training(\n",
    "        model,\n",
    "        use_gradient_checkpointing=config.use_gradient_checkpointing\n",
    "    )\n",
    "    \n",
    "    # LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=config.lora_r,\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        target_modules=config.lora_target_modules,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False\n",
    "    )\n",
    "    \n",
    "    # Apply LoRA\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Print model info\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"‚úÖ Model y√ºklendi: {config.model_name}\")\n",
    "    print(f\"üìä Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "model = load_model_with_lora(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer, config):\n",
    "    \"\"\"Veri setini hazƒ±rla ve tokenize et\"\"\"\n",
    "    \n",
    "    print(\"üìù Veri seti hazƒ±rlanƒ±yor...\")\n",
    "    \n",
    "    # Shuffle and select\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    \n",
    "    if len(dataset) > config.train_size + config.test_size:\n",
    "        dataset = dataset.select(range(config.train_size + config.test_size))\n",
    "    \n",
    "    # Train/test split\n",
    "    split = dataset.train_test_split(\n",
    "        test_size=config.test_size,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    train_data = split['train']\n",
    "    test_data = split['test']\n",
    "    \n",
    "    print(f\"  Train: {len(train_data)} samples\")\n",
    "    print(f\"  Test: {len(test_data)} samples\")\n",
    "    \n",
    "    # Tokenize function\n",
    "    def tokenize_function(examples):\n",
    "        outputs = tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            max_length=config.max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors=None\n",
    "        )\n",
    "        outputs['labels'] = outputs['input_ids'].copy()\n",
    "        return outputs\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"üîÑ Tokenizing...\")\n",
    "    \n",
    "    tokenized_train = train_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=2,  # Colab i√ßin 2 process\n",
    "        remove_columns=train_data.column_names,\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    tokenized_test = test_data.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        num_proc=2,\n",
    "        remove_columns=test_data.column_names,\n",
    "        desc=\"Tokenizing test\"\n",
    "    )\n",
    "    \n",
    "    # Set format\n",
    "    tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    print(\"‚úÖ Veri seti hazƒ±r!\")\n",
    "    \n",
    "    return tokenized_train, tokenized_test\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset, test_dataset = prepare_dataset(dataset, tokenizer, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback,\n",
    "    TrainerCallback\n",
    ")\n",
    "import math\n",
    "\n",
    "# Custom callback for better logging\n",
    "class CustomCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and 'loss' in logs:\n",
    "            logs['perplexity'] = min(math.exp(logs['loss']), 1000)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    \n",
    "    # Training\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size * 2,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    \n",
    "    # Learning rate\n",
    "    learning_rate=config.learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=\"paged_adamw_8bit\" if config.use_8bit else \"adamw_torch\",\n",
    "    fp16=config.use_fp16,\n",
    "    bf16=config.use_bf16,\n",
    "    gradient_checkpointing=config.use_gradient_checkpointing,\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=config.logging_steps,\n",
    "    logging_first_step=True,\n",
    "    report_to=\"none\",  # Colab'da wandb kullanmƒ±yoruz\n",
    "    \n",
    "    # Best model\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Performance\n",
    "    dataloader_num_workers=2,\n",
    "    dataloader_pin_memory=True,\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    run_name=\"qwen3_turkish\",\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[CustomCallback(), EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "print(\"üöÄ Training ba≈ülƒ±yor...\")\n",
    "print(f\"  Total steps: {len(train_dataset) // (config.batch_size * config.gradient_accumulation_steps) * config.num_epochs}\")\n",
    "print(f\"  Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Train\n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_result = trainer.evaluate()\n",
    "    \n",
    "    # Print results\n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ TRAINING COMPLETED!\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"‚è±Ô∏è Total time: {elapsed_time:.1f} minutes\")\n",
    "    print(f\"üìâ Final train loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"üìâ Final eval loss: {eval_result['eval_loss']:.4f}\")\n",
    "    print(f\"üìä Perplexity: {min(math.exp(eval_result['eval_loss']), 1000):.2f}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "print(\"üíæ Model kaydediliyor...\")\n",
    "\n",
    "# Save to local\n",
    "output_dir = f\"{config.output_dir}/final_model\"\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model kaydedildi: {output_dir}\")\n",
    "\n",
    "# If using Google Drive\n",
    "if '/content/drive' in os.getcwd():\n",
    "    print(\"üìÅ Model Google Drive'a kaydedildi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "def generate_text(prompt, max_length=100):\n",
    "    \"\"\"Generate text with the trained model\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test examples\n",
    "test_prompts = [\n",
    "    \"T√ºrkiye'nin ba≈ükenti\",\n",
    "    \"Yapay zeka teknolojisi\",\n",
    "    \"Bug√ºn hava √ßok g√ºzel\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Model Test Sonu√ßlarƒ±:\\n\")\n",
    "for prompt in test_prompts:\n",
    "    result = generate_text(prompt, max_length=50)\n",
    "    print(f\"üìù Prompt: {prompt}\")\n",
    "    print(f\"ü§ñ Generated: {result}\\n\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Memory Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "import gc\n",
    "\n",
    "# Delete large objects\n",
    "del model\n",
    "del trainer\n",
    "del train_dataset\n",
    "del test_dataset\n",
    "\n",
    "# Garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"‚úÖ GPU memory cleared\")\n",
    "    print(f\"üìä Current GPU memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Notlar ve ƒ∞pu√ßlarƒ±\n",
    "\n",
    "### Colab √úcretsiz GPU Limitleri:\n",
    "- **T4 GPU**: ~15GB VRAM\n",
    "- **Session limiti**: 12 saat\n",
    "- **Idle timeout**: 90 dakika\n",
    "\n",
    "### Performans ƒ∞pu√ßlarƒ±:\n",
    "1. **Batch size**: GPU belleƒüi doluyorsa azaltƒ±n\n",
    "2. **Max length**: Bellek sorunlarƒ±nda 128'e d√º≈ü√ºr√ºn\n",
    "3. **LoRA rank**: Daha az parametre i√ßin 8-16 kullanƒ±n\n",
    "4. **Gradient accumulation**: Batch size d√º≈ü√ºkse artƒ±rƒ±n\n",
    "\n",
    "### Sorun Giderme:\n",
    "- **CUDA out of memory**: Batch size veya max_length azaltƒ±n\n",
    "- **Tokenizer hatasƒ±**: Tiktoken yerine GPT-2 tokenizer kullanƒ±n\n",
    "- **Model y√ºkleme hatasƒ±**: Daha k√º√ß√ºk model deneyin (phi-2, pythia-1.4b)\n",
    "\n",
    "### Model Kaydetme:\n",
    "- Google Drive'a kaydetmeyi unutmayƒ±n\n",
    "- Session biterse model kaybolur!\n",
    "\n",
    "### Veri Seti:\n",
    "- Kendi veri setinizi CSV veya JSON formatƒ±nda y√ºkleyebilirsiniz\n",
    "- Hugging Face'den hazƒ±r T√ºrk√ße veri setleri kullanabilirsiniz"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
