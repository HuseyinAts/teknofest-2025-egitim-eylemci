{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3-8B Model Fine-tuning\n",
    "## TEKNOFEST 2025 - Eƒüitim Teknolojileri\n",
    "\n",
    "Bu notebook, Qwen3-8B modelini 4-bit kuantizasyon ve LoRA ile fine-tune etmek i√ßin hazƒ±rlanmƒ±≈ütƒ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √ñnce runtime'ƒ± yeniden ba≈ülat ve temiz kurulum yap\n",
    "!pip uninstall transformers accelerate -y\n",
    "!pip install transformers==4.44.2 accelerate==0.34.2 bitsandbytes==0.43.3 peft==0.12.0 -q\n",
    "!pip install sentencepiece protobuf -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache temizliƒüi\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "cache_dir = os.path.expanduser('~/.cache/huggingface')\n",
    "if os.path.exists(cache_dir):\n",
    "    shutil.rmtree(cache_dir)\n",
    "    print(\"Cache temizlendi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime'ƒ± yeniden ba≈ülatmak i√ßin\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime yeniden ba≈üladƒ±ktan sonra buradan devam edin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli k√ºt√ºphaneleri import et\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MODEL Y√úKLEME - Qwen3-8B\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Versiyon kontrol√º\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # Daha stabil bir model\n",
    "print(f\"Model: {MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit kuantizasyon ayarlarƒ±\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "print(\"‚úÖ 4-bit kuantizasyon ayarlandƒ±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer y√ºkle\n",
    "print(\"\\n‚è≥ Tokenizer y√ºkleniyor...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"‚úÖ Tokenizer y√ºklendi\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model y√ºkle\n",
    "print(\"\\n‚è≥ Model y√ºkleniyor (5-10 dakika s√ºrebilir)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False\n",
    ")\n",
    "print(\"‚úÖ Model y√ºklendi (4-bit)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA konfig√ºrasyonu\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Model'i LoRA i√ßin hazƒ±rla\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ƒ∞statistikler\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"\\nüìä Model ƒ∞statistikleri:\")\n",
    "print(f\"   Toplam parametre: {all_params:,}\")\n",
    "print(f\"   Eƒüitilebilir: {trainable_params:,}\")\n",
    "print(f\"   Eƒüitilebilir %: {100 * trainable_params / all_params:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU bellek durumu\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"\\nüíæ GPU Bellek:\")\n",
    "    print(f\"   Kullanƒ±lan: {allocated:.2f} GB\")\n",
    "    print(f\"   Rezerve: {reserved:.2f} GB\")\n",
    "\n",
    "print(\"\\n‚úÖ MODEL HAZIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternatif: Daha K√º√ß√ºk Model Kullanƒ±mƒ±\n",
    "\n",
    "Eƒüer yukarƒ±daki model √ßok b√ºy√ºkse, daha k√º√ß√ºk bir model kullanabilirsiniz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatif: Daha k√º√ß√ºk Qwen modeli\n",
    "# MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "# veya\n",
    "# MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}