{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TÃ¼rkÃ§e Veri Seti HazÄ±rlama\n",
    "\n",
    "Bu notebook Hugging Face'den iki farklÄ± veri setini indirip, temizleyip, kalite kontrolÃ¼ yaparak 200K Ã¶rneklik yÃ¼ksek kaliteli TÃ¼rkÃ§e veri seti oluÅŸturur.\n",
    "\n",
    "- **muspdf**: Kalite eÅŸiÄŸi > %88\n",
    "- **CulturaX**: Kalite eÅŸiÄŸi > %95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle\n",
    "!pip install -q datasets transformers langdetect ftfy pandas numpy tqdm\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from langdetect import detect, LangDetectException\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Colab ortam kontrolÃ¼\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Google Colab ortamÄ±nda Ã§alÄ±ÅŸÄ±yor\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"ğŸ’» Lokal ortamda Ã§alÄ±ÅŸÄ±yor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive'Ä± baÄŸla (isteÄŸe baÄŸlÄ±)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/turkish_dataset'\n",
    "else:\n",
    "    OUTPUT_DIR = './turkish_dataset'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“ Ã‡Ä±ktÄ± dizini: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Temizleme ve Kalite Kontrol FonksiyonlarÄ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurkishTextCleaner:\n",
    "    \"\"\"TÃ¼rkÃ§e metinler iÃ§in temizleme ve kalite kontrol sÄ±nÄ±fÄ±\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # TÃ¼rkÃ§e karakterler\n",
    "        self.turkish_chars = set('abcÃ§defgÄŸhÄ±ijklmnoÃ¶prsÅŸtuÃ¼vyzABCÃ‡DEFGÄHIÄ°JKLMNOÃ–PRSÅTUÃœVYZ')\n",
    "        self.turkish_vowels = set('aeÄ±ioÃ¶uÃ¼AEIÄ°OÃ–UÃœ')\n",
    "        \n",
    "        # Temizleme regex'leri\n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        self.multiple_spaces = re.compile(r'\\s+')\n",
    "        self.multiple_newlines = re.compile(r'\\n{3,}')\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding dÃ¼zeltmesi\n",
    "        text = ftfy.fix_text(text)\n",
    "        \n",
    "        # URL, email ve HTML temizleme\n",
    "        text = self.url_pattern.sub(' ', text)\n",
    "        text = self.email_pattern.sub(' ', text)\n",
    "        text = self.html_pattern.sub(' ', text)\n",
    "        \n",
    "        # Ã–zel karakterleri temizle (TÃ¼rkÃ§e karakterleri koru)\n",
    "        cleaned_chars = []\n",
    "        for char in text:\n",
    "            if char.isalnum() or char.isspace() or char in self.turkish_chars or char in '.,!?;:\"-':\n",
    "                cleaned_chars.append(char)\n",
    "            else:\n",
    "                cleaned_chars.append(' ')\n",
    "        text = ''.join(cleaned_chars)\n",
    "        \n",
    "        # Fazla boÅŸluklarÄ± temizle\n",
    "        text = self.multiple_spaces.sub(' ', text)\n",
    "        text = self.multiple_newlines.sub('\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def is_turkish(self, text: str) -> bool:\n",
    "        \"\"\"Metnin TÃ¼rkÃ§e olup olmadÄ±ÄŸÄ±nÄ± kontrol et\"\"\"\n",
    "        try:\n",
    "            lang = detect(text[:500])  # Ä°lk 500 karaktere bak\n",
    "            return lang == 'tr'\n",
    "        except (LangDetectException, Exception):\n",
    "            # Dil algÄ±lanamadÄ±ysa TÃ¼rkÃ§e karakterlere bak\n",
    "            return self.calculate_turkish_char_ratio(text) > 0.05\n",
    "    \n",
    "    def calculate_turkish_char_ratio(self, text: str) -> float:\n",
    "        \"\"\"TÃ¼rkÃ§e Ã¶zel karakter oranÄ±nÄ± hesapla\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        turkish_specific = set('Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄÄ°Ã–ÅÃœ')\n",
    "        char_count = sum(1 for c in text if c.isalpha())\n",
    "        if char_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        turkish_count = sum(1 for c in text if c in turkish_specific)\n",
    "        return turkish_count / char_count\n",
    "    \n",
    "    def calculate_quality_score(self, text: str) -> float:\n",
    "        \"\"\"Metin kalitesi skorunu hesapla (0-100)\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        # 1. Uzunluk skoru (100-5000 karakter ideal)\n",
    "        text_len = len(text)\n",
    "        if 100 <= text_len <= 5000:\n",
    "            len_score = 100\n",
    "        elif text_len < 100:\n",
    "            len_score = text_len\n",
    "        else:\n",
    "            len_score = max(0, 100 - (text_len - 5000) / 100)\n",
    "        scores.append(len_score * 0.2)\n",
    "        \n",
    "        # 2. TÃ¼rkÃ§e karakter oranÄ±\n",
    "        turkish_ratio = self.calculate_turkish_char_ratio(text)\n",
    "        scores.append(min(turkish_ratio * 500, 100) * 0.3)  # %5 TÃ¼rkÃ§e karakter = 25 puan\n",
    "        \n",
    "        # 3. Kelime Ã§eÅŸitliliÄŸi\n",
    "        words = text.lower().split()\n",
    "        if len(words) > 0:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            scores.append(unique_ratio * 100 * 0.2)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        # 4. CÃ¼mle yapÄ±sÄ± (noktalama iÅŸaretleri)\n",
    "        punct_count = sum(1 for c in text if c in '.!?')\n",
    "        if len(words) > 0:\n",
    "            punct_ratio = punct_count / (len(words) / 10)  # Her 10 kelimede 1 noktalama beklenir\n",
    "            scores.append(min(punct_ratio * 100, 100) * 0.15)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        # 5. BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf dengesi\n",
    "        alpha_chars = [c for c in text if c.isalpha()]\n",
    "        if alpha_chars:\n",
    "            upper_ratio = sum(1 for c in alpha_chars if c.isupper()) / len(alpha_chars)\n",
    "            if 0.02 <= upper_ratio <= 0.15:  # %2-%15 arasÄ± bÃ¼yÃ¼k harf ideal\n",
    "                scores.append(100 * 0.15)\n",
    "            else:\n",
    "                scores.append(50 * 0.15)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        return sum(scores)\n",
    "    \n",
    "    def process_text(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"Metni iÅŸle ve sonuÃ§larÄ± dÃ¶ndÃ¼r\"\"\"\n",
    "        # Temizle\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # BoÅŸ veya Ã§ok kÄ±sa metinleri ele\n",
    "        if not cleaned or len(cleaned) < 50:\n",
    "            return None\n",
    "        \n",
    "        # TÃ¼rkÃ§e kontrolÃ¼\n",
    "        if not self.is_turkish(cleaned):\n",
    "            return None\n",
    "        \n",
    "        # Kalite skoru hesapla\n",
    "        quality_score = self.calculate_quality_score(cleaned)\n",
    "        \n",
    "        return {\n",
    "            'text': cleaned,\n",
    "            'quality_score': quality_score,\n",
    "            'length': len(cleaned),\n",
    "            'word_count': len(cleaned.split())\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdÄ±m 1: muspdf Veri Setini Ä°ndir ve Ä°ÅŸle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muspdf veri setini indir\n",
    "print(\"ğŸ“¥ muspdf veri seti indiriliyor...\")\n",
    "try:\n",
    "    # Huseyin/muspdf veri setini yÃ¼kle\n",
    "    muspdf_dataset = load_dataset(\"Huseyin/muspdf\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # Ä°lk 500K veriyi al (bellek yÃ¶netimi iÃ§in)\n",
    "    muspdf_data = []\n",
    "    for i, item in enumerate(tqdm(muspdf_dataset, desc=\"muspdf yÃ¼kleniyor\", total=500000)):\n",
    "        if i >= 500000:\n",
    "            break\n",
    "        # Veri yapÄ±sÄ±na gÃ¶re text alanÄ±nÄ± al\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        muspdf_data.append({'text': text})\n",
    "    \n",
    "    print(f\"âœ… {len(muspdf_data)} muspdf Ã¶rneÄŸi yÃ¼klendi\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ muspdf yÃ¼klenemedi: {e}\")\n",
    "    print(\"Alternatif veri seti kullanÄ±lÄ±yor...\")\n",
    "    # Alternatif olarak baÅŸka bir TÃ¼rkÃ§e veri seti kullan\n",
    "    muspdf_dataset = load_dataset(\"uonlp/CulturaX\", \"tr\", split=\"train\", streaming=True)\n",
    "    muspdf_data = []\n",
    "    for i, item in enumerate(tqdm(muspdf_dataset, desc=\"Alternatif veri yÃ¼kleniyor\", total=100000)):\n",
    "        if i >= 100000:\n",
    "            break\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        muspdf_data.append({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muspdf verilerini temizle ve kalite kontrolÃ¼ yap\n",
    "print(\"ğŸ§¹ muspdf verileri temizleniyor ve kalite kontrolÃ¼ yapÄ±lÄ±yor...\")\n",
    "\n",
    "cleaner = TurkishTextCleaner()\n",
    "muspdf_cleaned = []\n",
    "\n",
    "for item in tqdm(muspdf_data, desc=\"muspdf iÅŸleniyor\"):\n",
    "    result = cleaner.process_text(item['text'])\n",
    "    if result and result['quality_score'] >= 88:  # %88 Ã¼stÃ¼ kalite\n",
    "        muspdf_cleaned.append(result)\n",
    "\n",
    "print(f\"âœ… {len(muspdf_cleaned)} yÃ¼ksek kaliteli muspdf Ã¶rneÄŸi seÃ§ildi\")\n",
    "print(f\"ğŸ“Š Ortalama kalite skoru: {np.mean([x['quality_score'] for x in muspdf_cleaned]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdÄ±m 2: CulturaX Veri Setini Ä°ndir ve Ä°ÅŸle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CulturaX TÃ¼rkÃ§e veri setini indir\n",
    "print(\"ğŸ“¥ CulturaX TÃ¼rkÃ§e veri seti indiriliyor...\")\n",
    "\n",
    "try:\n",
    "    # CulturaX'dan TÃ¼rkÃ§e verileri yÃ¼kle (streaming mode)\n",
    "    culturax_dataset = load_dataset(\"uonlp/CulturaX\", \"tr\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # 300K veri al\n",
    "    culturax_data = []\n",
    "    for i, item in enumerate(tqdm(culturax_dataset, desc=\"CulturaX yÃ¼kleniyor\", total=300000)):\n",
    "        if i >= 300000:\n",
    "            break\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        culturax_data.append({'text': text})\n",
    "    \n",
    "    print(f\"âœ… {len(culturax_data)} CulturaX Ã¶rneÄŸi yÃ¼klendi\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ CulturaX yÃ¼klenemedi: {e}\")\n",
    "    culturax_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CulturaX verilerini temizle ve kalite kontrolÃ¼ yap\n",
    "print(\"ğŸ§¹ CulturaX verileri temizleniyor ve kalite kontrolÃ¼ yapÄ±lÄ±yor...\")\n",
    "\n",
    "culturax_cleaned = []\n",
    "\n",
    "for item in tqdm(culturax_data, desc=\"CulturaX iÅŸleniyor\"):\n",
    "    result = cleaner.process_text(item['text'])\n",
    "    if result and result['quality_score'] >= 95:  # %95 Ã¼stÃ¼ kalite\n",
    "        culturax_cleaned.append(result)\n",
    "\n",
    "print(f\"âœ… {len(culturax_cleaned)} yÃ¼ksek kaliteli CulturaX Ã¶rneÄŸi seÃ§ildi\")\n",
    "if culturax_cleaned:\n",
    "    print(f\"ğŸ“Š Ortalama kalite skoru: {np.mean([x['quality_score'] for x in culturax_cleaned]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdÄ±m 3: Veri Setlerini BirleÅŸtir ve 200K Ã–rnek SeÃ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ä°ki veri setini birleÅŸtir\n",
    "print(\"ğŸ”— Veri setleri birleÅŸtiriliyor...\")\n",
    "\n",
    "all_data = muspdf_cleaned + culturax_cleaned\n",
    "print(f\"ğŸ“Š Toplam temizlenmiÅŸ veri: {len(all_data)} Ã¶rnek\")\n",
    "\n",
    "# Kalite skoruna gÃ¶re sÄ±rala\n",
    "all_data_sorted = sorted(all_data, key=lambda x: x['quality_score'], reverse=True)\n",
    "\n",
    "# En iyi 200K Ã¶rneÄŸi seÃ§\n",
    "final_dataset = all_data_sorted[:200000]\n",
    "print(f\"âœ… En yÃ¼ksek kaliteli {len(final_dataset)} Ã¶rnek seÃ§ildi\")\n",
    "print(f\"ğŸ“Š Final veri seti ortalama kalite skoru: {np.mean([x['quality_score'] for x in final_dataset]):.2f}\")\n",
    "print(f\"ğŸ“Š Min kalite skoru: {min([x['quality_score'] for x in final_dataset]):.2f}\")\n",
    "print(f\"ğŸ“Š Max kalite skoru: {max([x['quality_score'] for x in final_dataset]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri seti istatistikleri\n",
    "print(\"\\nğŸ“ˆ Veri Seti Ä°statistikleri:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lengths = [x['length'] for x in final_dataset]\n",
    "word_counts = [x['word_count'] for x in final_dataset]\n",
    "quality_scores = [x['quality_score'] for x in final_dataset]\n",
    "\n",
    "print(f\"ğŸ“ Ortalama metin uzunluÄŸu: {np.mean(lengths):.0f} karakter\")\n",
    "print(f\"ğŸ“ Medyan metin uzunluÄŸu: {np.median(lengths):.0f} karakter\")\n",
    "print(f\"ğŸ“ Ortalama kelime sayÄ±sÄ±: {np.mean(word_counts):.0f} kelime\")\n",
    "print(f\"ğŸ“Š Kalite skoru daÄŸÄ±lÄ±mÄ±:\")\n",
    "print(f\"   - Q1 (25%): {np.percentile(quality_scores, 25):.2f}\")\n",
    "print(f\"   - Q2 (50%): {np.percentile(quality_scores, 50):.2f}\")\n",
    "print(f\"   - Q3 (75%): {np.percentile(quality_scores, 75):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdÄ±m 4: Veri Setini Kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "print(\"ğŸ’¾ Veri seti kaydediliyor...\")\n",
    "\n",
    "# Dataset oluÅŸtur\n",
    "dataset_dict = {\n",
    "    'text': [item['text'] for item in final_dataset],\n",
    "    'quality_score': [item['quality_score'] for item in final_dataset],\n",
    "    'length': [item['length'] for item in final_dataset],\n",
    "    'word_count': [item['word_count'] for item in final_dataset]\n",
    "}\n",
    "\n",
    "hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# FarklÄ± formatlarda kaydet\n",
    "# 1. Hugging Face format\n",
    "hf_dataset.save_to_disk(os.path.join(OUTPUT_DIR, 'turkish_200k_dataset'))\n",
    "print(f\"âœ… Hugging Face formatÄ±nda kaydedildi: {OUTPUT_DIR}/turkish_200k_dataset\")\n",
    "\n",
    "# 2. CSV formatÄ±\n",
    "df = pd.DataFrame(dataset_dict)\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"âœ… CSV formatÄ±nda kaydedildi: {csv_path}\")\n",
    "\n",
    "# 3. JSONL formatÄ± (her satÄ±r bir JSON objesi)\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.jsonl')\n",
    "df.to_json(jsonl_path, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"âœ… JSONL formatÄ±nda kaydedildi: {jsonl_path}\")\n",
    "\n",
    "# 4. Parquet formatÄ± (daha verimli depolama)\n",
    "parquet_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.parquet')\n",
    "df.to_parquet(parquet_path)\n",
    "print(f\"âœ… Parquet formatÄ±nda kaydedildi: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ã–rnek metinleri gÃ¶ster\n",
    "print(\"\\nğŸ“„ Ã–rnek Metinler:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(final_dataset))):\n",
    "    print(f\"\\nğŸ”¹ Ã–rnek {i+1}:\")\n",
    "    print(f\"Kalite Skoru: {final_dataset[i]['quality_score']:.2f}\")\n",
    "    print(f\"Uzunluk: {final_dataset[i]['length']} karakter\")\n",
    "    print(f\"Kelime SayÄ±sÄ±: {final_dataset[i]['word_count']} kelime\")\n",
    "    print(f\"Metin (ilk 200 karakter): {final_dataset[i]['text'][:200]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini yÃ¼kleme testi\n",
    "print(\"\\nğŸ”„ Veri seti yÃ¼kleme testi...\")\n",
    "\n",
    "try:\n",
    "    # Kaydedilen veri setini tekrar yÃ¼kle\n",
    "    loaded_dataset = Dataset.load_from_disk(os.path.join(OUTPUT_DIR, 'turkish_200k_dataset'))\n",
    "    print(f\"âœ… Veri seti baÅŸarÄ±yla yÃ¼klendi: {len(loaded_dataset)} Ã¶rnek\")\n",
    "    print(f\"SÃ¼tunlar: {loaded_dataset.column_names}\")\n",
    "    print(f\"Ä°lk Ã¶rnek: {loaded_dataset[0]['text'][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Veri seti yÃ¼klenemedi: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KullanÄ±m Ã–rneÄŸi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini kullanma Ã¶rneÄŸi\n",
    "print(\"\\nğŸ’¡ Veri Setini Kullanma Ã–rneÄŸi:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "# Veri setini yÃ¼klemek iÃ§in:\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Hugging Face formatÄ±ndan yÃ¼kle\n",
    "dataset = load_from_disk('turkish_200k_dataset')\n",
    "\n",
    "# veya CSV'den yÃ¼kle\n",
    "import pandas as pd\n",
    "df = pd.read_csv('turkish_200k_dataset.csv')\n",
    "\n",
    "# Model eÄŸitimi iÃ§in kullan\n",
    "for example in dataset:\n",
    "    text = example['text']\n",
    "    quality = example['quality_score']\n",
    "    # Model eÄŸitim kodu...\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nâœ¨ Veri seti hazÄ±rlama tamamlandÄ±!\")\n",
    "print(f\"ğŸ“ TÃ¼m dosyalar {OUTPUT_DIR} dizinine kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Alternatif yÃ¼kleme yÃ¶ntemi: push_to_hub kullanarak\nprint(\"\\nğŸ“¤ Alternatif yÃ¼kleme yÃ¶ntemi (push_to_hub)...\")\n\ntry:\n    # Dataset'i doÄŸrudan push_to_hub ile yÃ¼kle\n    hf_dataset.push_to_hub(\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        commit_message=\"Upload Turkish 200K clean dataset\",\n        private=False  # Public repo iÃ§in False\n    )\n    \n    print(f\"âœ… Veri seti push_to_hub ile baÅŸarÄ±yla yÃ¼klendi!\")\n    print(f\"ğŸ”— Veri setinize eriÅŸin: https://huggingface.co/datasets/{REPO_ID}\")\n    \n    # KullanÄ±m Ã¶rneÄŸi\n    print(\"\\nğŸ“š Veri setinizi kullanmak iÃ§in:\")\n    print(f\"```python\")\n    print(f\"from datasets import load_dataset\")\n    print(f\"dataset = load_dataset('{REPO_ID}')\")\n    print(f\"print(dataset['train'][0])\")\n    print(f\"```\")\n    \nexcept Exception as e:\n    print(f\"âŒ push_to_hub hatasÄ±: {e}\")\n    print(\"Token ve repository adÄ±nÄ± kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Veri setini Hugging Face'e yÃ¼kle\nfrom huggingface_hub import HfApi, create_repo, upload_folder\nimport shutil\n\nprint(f\"\\nğŸš€ Veri seti Hugging Face'e yÃ¼kleniyor: {REPO_ID}\")\n\ntry:\n    api = HfApi()\n    \n    # 1. Repository oluÅŸtur (yoksa)\n    try:\n        create_repo(\n            repo_id=REPO_ID,\n            repo_type=\"dataset\",\n            private=False,  # Public yapmak iÃ§in False, private iÃ§in True\n            token=HF_TOKEN\n        )\n        print(f\"âœ… Repository oluÅŸturuldu: {REPO_ID}\")\n    except Exception as e:\n        if \"already exists\" in str(e).lower():\n            print(f\"â„¹ï¸ Repository zaten mevcut: {REPO_ID}\")\n        else:\n            raise e\n    \n    # 2. YÃ¼klenecek dosyalarÄ± hazÄ±rla\n    upload_dir = os.path.join(OUTPUT_DIR, 'hf_upload')\n    os.makedirs(upload_dir, exist_ok=True)\n    \n    # Dataset dosyalarÄ±nÄ± kopyala\n    dataset_dir = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset')\n    if os.path.exists(dataset_dir):\n        # Dataset klasÃ¶rÃ¼nÃ¼ kopyala\n        for item in os.listdir(dataset_dir):\n            src = os.path.join(dataset_dir, item)\n            dst = os.path.join(upload_dir, item)\n            if os.path.isfile(src):\n                shutil.copy2(src, dst)\n            else:\n                shutil.copytree(src, dst, dirs_exist_ok=True)\n    \n    # README'yi kopyala\n    shutil.copy2(readme_path, os.path.join(upload_dir, 'README.md'))\n    \n    # 3. DosyalarÄ± yÃ¼kle\n    print(\"ğŸ“¤ Dosyalar yÃ¼kleniyor...\")\n    upload_folder(\n        folder_path=upload_dir,\n        repo_id=REPO_ID,\n        repo_type=\"dataset\",\n        token=HF_TOKEN,\n        commit_message=\"Initial dataset upload: Turkish 200K clean dataset\"\n    )\n    \n    print(f\"âœ… Veri seti baÅŸarÄ±yla yÃ¼klendi!\")\n    print(f\"ğŸ”— Veri setinize eriÅŸin: https://huggingface.co/datasets/{REPO_ID}\")\n    \n    # Temizlik\n    shutil.rmtree(upload_dir)\n    \nexcept Exception as e:\n    print(f\"âŒ YÃ¼kleme hatasÄ±: {e}\")\n    print(\"LÃ¼tfen token'Ä±nÄ±zÄ± ve internet baÄŸlantÄ±nÄ±zÄ± kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# README.md oluÅŸtur\nreadme_content = f\"\"\"---\nlanguage:\n- tr\nlicense: apache-2.0\nsize_categories:\n- 100K<n<1M\ntask_categories:\n- text-generation\n- text2text-generation\npretty_name: Turkish Clean Dataset 200K\ntags:\n- turkish\n- nlp\n- clean-data\n- high-quality\ndataset_info:\n  features:\n  - name: text\n    dtype: string\n  - name: quality_score\n    dtype: float32\n  - name: length\n    dtype: int32\n  - name: word_count\n    dtype: int32\n  splits:\n  - name: train\n    num_examples: {len(final_dataset)}\n---\n\n# ğŸ‡¹ğŸ‡· Turkish Clean Dataset 200K\n\n## Veri Seti HakkÄ±nda\n\nBu veri seti, TÃ¼rkÃ§e NLP modellerinin eÄŸitimi iÃ§in hazÄ±rlanmÄ±ÅŸ yÃ¼ksek kaliteli 200.000 metin Ã¶rneÄŸi iÃ§erir.\n\n### ğŸ“Š Ã–zellikler\n\n- **Toplam Ã–rnek SayÄ±sÄ±**: {len(final_dataset):,}\n- **Ortalama Metin UzunluÄŸu**: {np.mean(lengths):.0f} karakter\n- **Ortalama Kelime SayÄ±sÄ±**: {np.mean(word_counts):.0f} kelime\n- **Ortalama Kalite Skoru**: {np.mean(quality_scores):.2f}/100\n- **Min Kalite Skoru**: {min(quality_scores):.2f}/100\n- **Max Kalite Skoru**: {max(quality_scores):.2f}/100\n\n### ğŸ”§ Veri Ä°ÅŸleme SÃ¼reci\n\n1. **Kaynak Veri Setleri**:\n   - muspdf dataset (500K Ã¶rnek indirildi, kalite eÅŸiÄŸi: %88)\n   - CulturaX Turkish (300K Ã¶rnek indirildi, kalite eÅŸiÄŸi: %95)\n\n2. **Temizleme Ä°ÅŸlemleri**:\n   - URL, email ve HTML etiketlerinin kaldÄ±rÄ±lmasÄ±\n   - Encoding problemlerinin dÃ¼zeltilmesi (ftfy)\n   - Fazla boÅŸluk ve satÄ±r sonlarÄ±nÄ±n temizlenmesi\n   - TÃ¼rkÃ§e karakterlerin korunmasÄ±\n\n3. **Kalite Kontrol Kriterleri**:\n   - TÃ¼rkÃ§e dil tespiti (langdetect)\n   - TÃ¼rkÃ§e Ã¶zel karakter oranÄ± kontrolÃ¼\n   - Metin uzunluÄŸu deÄŸerlendirmesi (100-5000 karakter ideal)\n   - Kelime Ã§eÅŸitliliÄŸi analizi\n   - Noktalama iÅŸareti dengesi\n   - BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf oranÄ±\n\n### ğŸ“¦ Veri FormatÄ±\n\nVeri seti 4 alan iÃ§erir:\n- `text`: TemizlenmiÅŸ metin\n- `quality_score`: Kalite skoru (0-100)\n- `length`: Metin uzunluÄŸu (karakter)\n- `word_count`: Kelime sayÄ±sÄ±\n\n### ğŸ’» KullanÄ±m\n\n```python\nfrom datasets import load_dataset\n\n# Veri setini yÃ¼kle\ndataset = load_dataset(\"{REPO_ID}\")\n\n# Ä°lk Ã¶rneÄŸi gÃ¶rÃ¼ntÃ¼le\nprint(dataset['train'][0])\n\n# Pandas DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r\ndf = dataset['train'].to_pandas()\n```\n\n### ğŸ“ˆ Kalite Skoru DaÄŸÄ±lÄ±mÄ±\n\n- Q1 (25%): {np.percentile(quality_scores, 25):.2f}\n- Q2 (50%): {np.percentile(quality_scores, 50):.2f}\n- Q3 (75%): {np.percentile(quality_scores, 75):.2f}\n\n### ğŸ¯ KullanÄ±m AlanlarÄ±\n\n- TÃ¼rkÃ§e dil modelleri eÄŸitimi\n- Metin Ã¼retimi (text generation)\n- Metin sÄ±nÄ±flandÄ±rma\n- Dil modelleme\n- Transfer Ã¶ÄŸrenme\n\n### ğŸ“„ Lisans\n\nApache 2.0\n\n### ğŸ¤ KatkÄ±da Bulunma\n\nVeri setini geliÅŸtirmek iÃ§in Ã¶nerilerinizi ve katkÄ±larÄ±nÄ±zÄ± bekliyoruz!\n\n### ğŸ“® Ä°letiÅŸim\n\nSorularÄ±nÄ±z iÃ§in issue aÃ§abilir veya tartÄ±ÅŸma bÃ¶lÃ¼mÃ¼nÃ¼ kullanabilirsiniz.\n\"\"\"\n\n# README'yi kaydet\nreadme_path = os.path.join(OUTPUT_DIR, 'README.md')\nwith open(readme_path, 'w', encoding='utf-8') as f:\n    f.write(readme_content)\nprint(f\"âœ… README.md oluÅŸturuldu: {readme_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Veri seti bilgilerini ayarla\nDATASET_NAME = input(\"Veri seti adÄ±nÄ± girin (Ã¶rn: turkish-200k-clean): \").strip()\nif not DATASET_NAME:\n    DATASET_NAME = \"turkish-200k-clean\"\n\nUSERNAME = input(\"Hugging Face kullanÄ±cÄ± adÄ±nÄ±zÄ± girin: \").strip()\nif not USERNAME:\n    print(\"âŒ KullanÄ±cÄ± adÄ± gerekli!\")\nelse:\n    REPO_ID = f\"{USERNAME}/{DATASET_NAME}\"\n    print(f\"\\nğŸ“¦ Veri seti repo ID: {REPO_ID}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Hugging Face Hub'a giriÅŸ yap\nfrom huggingface_hub import HfApi, login, create_repo\nimport getpass\n\nprint(\"ğŸ” Hugging Face'e giriÅŸ yapÄ±lÄ±yor...\")\nprint(\"Token'Ä±nÄ±zÄ± https://huggingface.co/settings/tokens adresinden alabilirsiniz.\")\nprint(\"Token'Ä±nÄ±z gÃ¼venli bir ÅŸekilde saklanacaktÄ±r.\\n\")\n\n# Token'Ä± gÃ¼venli ÅŸekilde al\nif IN_COLAB:\n    from google.colab import userdata\n    try:\n        # Colab secrets'tan token'Ä± almayÄ± dene\n        HF_TOKEN = userdata.get('HF_TOKEN')\n        print(\"âœ… Token Colab secrets'tan alÄ±ndÄ±\")\n    except:\n        # Manuel olarak iste\n        HF_TOKEN = getpass.getpass(\"Hugging Face Token'Ä±nÄ±zÄ± girin: \")\nelse:\n    # Ã–nce Ã§evre deÄŸiÅŸkeninden kontrol et\n    HF_TOKEN = os.environ.get('HF_TOKEN')\n    if not HF_TOKEN:\n        HF_TOKEN = getpass.getpass(\"Hugging Face Token'Ä±nÄ±zÄ± girin: \")\n\n# GiriÅŸ yap\ntry:\n    login(token=HF_TOKEN, add_to_git_credential=True)\n    print(\"âœ… Hugging Face'e baÅŸarÄ±yla giriÅŸ yapÄ±ldÄ±!\")\nexcept Exception as e:\n    print(f\"âŒ GiriÅŸ baÅŸarÄ±sÄ±z: {e}\")\n    print(\"Token'Ä±nÄ±zÄ± kontrol edip tekrar deneyin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## AdÄ±m 5: Veri Setini Hugging Face'e YÃ¼kle",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}