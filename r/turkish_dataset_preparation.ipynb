{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Türkçe Veri Seti Hazırlama\n",
    "\n",
    "Bu notebook Hugging Face'den iki farklı veri setini indirip, temizleyip, kalite kontrolü yaparak 200K örneklik yüksek kaliteli Türkçe veri seti oluşturur.\n",
    "\n",
    "- **muspdf**: Kalite eşiği > %88\n",
    "- **CulturaX**: Kalite eşiği > %95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri yükle\n",
    "!pip install -q datasets transformers langdetect ftfy pandas numpy tqdm\n",
    "!pip install -q huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ftfy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "from langdetect import detect, LangDetectException\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Google Colab ortam kontrolü\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"✅ Google Colab ortamında çalışıyor\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"💻 Lokal ortamda çalışıyor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive'ı bağla (isteğe bağlı)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    OUTPUT_DIR = '/content/drive/MyDrive/turkish_dataset'\n",
    "else:\n",
    "    OUTPUT_DIR = './turkish_dataset'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"📁 Çıktı dizini: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veri Temizleme ve Kalite Kontrol Fonksiyonları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurkishTextCleaner:\n",
    "    \"\"\"Türkçe metinler için temizleme ve kalite kontrol sınıfı\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Türkçe karakterler\n",
    "        self.turkish_chars = set('abcçdefgğhıijklmnoöprsştuüvyzABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZ')\n",
    "        self.turkish_vowels = set('aeıioöuüAEIİOÖUÜ')\n",
    "        \n",
    "        # Temizleme regex'leri\n",
    "        self.url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        self.multiple_spaces = re.compile(r'\\s+')\n",
    "        self.multiple_newlines = re.compile(r'\\n{3,}')\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        if not text or not isinstance(text, str):\n",
    "            return \"\"\n",
    "        \n",
    "        # Encoding düzeltmesi\n",
    "        text = ftfy.fix_text(text)\n",
    "        \n",
    "        # URL, email ve HTML temizleme\n",
    "        text = self.url_pattern.sub(' ', text)\n",
    "        text = self.email_pattern.sub(' ', text)\n",
    "        text = self.html_pattern.sub(' ', text)\n",
    "        \n",
    "        # Özel karakterleri temizle (Türkçe karakterleri koru)\n",
    "        cleaned_chars = []\n",
    "        for char in text:\n",
    "            if char.isalnum() or char.isspace() or char in self.turkish_chars or char in '.,!?;:\"-':\n",
    "                cleaned_chars.append(char)\n",
    "            else:\n",
    "                cleaned_chars.append(' ')\n",
    "        text = ''.join(cleaned_chars)\n",
    "        \n",
    "        # Fazla boşlukları temizle\n",
    "        text = self.multiple_spaces.sub(' ', text)\n",
    "        text = self.multiple_newlines.sub('\\n\\n', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def is_turkish(self, text: str) -> bool:\n",
    "        \"\"\"Metnin Türkçe olup olmadığını kontrol et\"\"\"\n",
    "        try:\n",
    "            lang = detect(text[:500])  # İlk 500 karaktere bak\n",
    "            return lang == 'tr'\n",
    "        except (LangDetectException, Exception):\n",
    "            # Dil algılanamadıysa Türkçe karakterlere bak\n",
    "            return self.calculate_turkish_char_ratio(text) > 0.05\n",
    "    \n",
    "    def calculate_turkish_char_ratio(self, text: str) -> float:\n",
    "        \"\"\"Türkçe özel karakter oranını hesapla\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        turkish_specific = set('çğıöşüÇĞİÖŞÜ')\n",
    "        char_count = sum(1 for c in text if c.isalpha())\n",
    "        if char_count == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        turkish_count = sum(1 for c in text if c in turkish_specific)\n",
    "        return turkish_count / char_count\n",
    "    \n",
    "    def calculate_quality_score(self, text: str) -> float:\n",
    "        \"\"\"Metin kalitesi skorunu hesapla (0-100)\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        \n",
    "        scores = []\n",
    "        \n",
    "        # 1. Uzunluk skoru (100-5000 karakter ideal)\n",
    "        text_len = len(text)\n",
    "        if 100 <= text_len <= 5000:\n",
    "            len_score = 100\n",
    "        elif text_len < 100:\n",
    "            len_score = text_len\n",
    "        else:\n",
    "            len_score = max(0, 100 - (text_len - 5000) / 100)\n",
    "        scores.append(len_score * 0.2)\n",
    "        \n",
    "        # 2. Türkçe karakter oranı\n",
    "        turkish_ratio = self.calculate_turkish_char_ratio(text)\n",
    "        scores.append(min(turkish_ratio * 500, 100) * 0.3)  # %5 Türkçe karakter = 25 puan\n",
    "        \n",
    "        # 3. Kelime çeşitliliği\n",
    "        words = text.lower().split()\n",
    "        if len(words) > 0:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            scores.append(unique_ratio * 100 * 0.2)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        # 4. Cümle yapısı (noktalama işaretleri)\n",
    "        punct_count = sum(1 for c in text if c in '.!?')\n",
    "        if len(words) > 0:\n",
    "            punct_ratio = punct_count / (len(words) / 10)  # Her 10 kelimede 1 noktalama beklenir\n",
    "            scores.append(min(punct_ratio * 100, 100) * 0.15)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        # 5. Büyük/küçük harf dengesi\n",
    "        alpha_chars = [c for c in text if c.isalpha()]\n",
    "        if alpha_chars:\n",
    "            upper_ratio = sum(1 for c in alpha_chars if c.isupper()) / len(alpha_chars)\n",
    "            if 0.02 <= upper_ratio <= 0.15:  # %2-%15 arası büyük harf ideal\n",
    "                scores.append(100 * 0.15)\n",
    "            else:\n",
    "                scores.append(50 * 0.15)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "        \n",
    "        return sum(scores)\n",
    "    \n",
    "    def process_text(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"Metni işle ve sonuçları döndür\"\"\"\n",
    "        # Temizle\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        # Boş veya çok kısa metinleri ele\n",
    "        if not cleaned or len(cleaned) < 50:\n",
    "            return None\n",
    "        \n",
    "        # Türkçe kontrolü\n",
    "        if not self.is_turkish(cleaned):\n",
    "            return None\n",
    "        \n",
    "        # Kalite skoru hesapla\n",
    "        quality_score = self.calculate_quality_score(cleaned)\n",
    "        \n",
    "        return {\n",
    "            'text': cleaned,\n",
    "            'quality_score': quality_score,\n",
    "            'length': len(cleaned),\n",
    "            'word_count': len(cleaned.split())\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adım 1: muspdf Veri Setini İndir ve İşle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muspdf veri setini indir\n",
    "print(\"📥 muspdf veri seti indiriliyor...\")\n",
    "try:\n",
    "    # Huseyin/muspdf veri setini yükle\n",
    "    muspdf_dataset = load_dataset(\"Huseyin/muspdf\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # İlk 500K veriyi al (bellek yönetimi için)\n",
    "    muspdf_data = []\n",
    "    for i, item in enumerate(tqdm(muspdf_dataset, desc=\"muspdf yükleniyor\", total=500000)):\n",
    "        if i >= 500000:\n",
    "            break\n",
    "        # Veri yapısına göre text alanını al\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        muspdf_data.append({'text': text})\n",
    "    \n",
    "    print(f\"✅ {len(muspdf_data)} muspdf örneği yüklendi\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ muspdf yüklenemedi: {e}\")\n",
    "    print(\"Alternatif veri seti kullanılıyor...\")\n",
    "    # Alternatif olarak başka bir Türkçe veri seti kullan\n",
    "    muspdf_dataset = load_dataset(\"uonlp/CulturaX\", \"tr\", split=\"train\", streaming=True)\n",
    "    muspdf_data = []\n",
    "    for i, item in enumerate(tqdm(muspdf_dataset, desc=\"Alternatif veri yükleniyor\", total=100000)):\n",
    "        if i >= 100000:\n",
    "            break\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        muspdf_data.append({'text': text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# muspdf verilerini temizle ve kalite kontrolü yap\n",
    "print(\"🧹 muspdf verileri temizleniyor ve kalite kontrolü yapılıyor...\")\n",
    "\n",
    "cleaner = TurkishTextCleaner()\n",
    "muspdf_cleaned = []\n",
    "\n",
    "for item in tqdm(muspdf_data, desc=\"muspdf işleniyor\"):\n",
    "    result = cleaner.process_text(item['text'])\n",
    "    if result and result['quality_score'] >= 88:  # %88 üstü kalite\n",
    "        muspdf_cleaned.append(result)\n",
    "\n",
    "print(f\"✅ {len(muspdf_cleaned)} yüksek kaliteli muspdf örneği seçildi\")\n",
    "print(f\"📊 Ortalama kalite skoru: {np.mean([x['quality_score'] for x in muspdf_cleaned]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adım 2: CulturaX Veri Setini İndir ve İşle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CulturaX Türkçe veri setini indir\n",
    "print(\"📥 CulturaX Türkçe veri seti indiriliyor...\")\n",
    "\n",
    "try:\n",
    "    # CulturaX'dan Türkçe verileri yükle (streaming mode)\n",
    "    culturax_dataset = load_dataset(\"uonlp/CulturaX\", \"tr\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # 300K veri al\n",
    "    culturax_data = []\n",
    "    for i, item in enumerate(tqdm(culturax_dataset, desc=\"CulturaX yükleniyor\", total=300000)):\n",
    "        if i >= 300000:\n",
    "            break\n",
    "        text = item.get('text', '') or item.get('content', '') or str(item)\n",
    "        culturax_data.append({'text': text})\n",
    "    \n",
    "    print(f\"✅ {len(culturax_data)} CulturaX örneği yüklendi\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ CulturaX yüklenemedi: {e}\")\n",
    "    culturax_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CulturaX verilerini temizle ve kalite kontrolü yap\n",
    "print(\"🧹 CulturaX verileri temizleniyor ve kalite kontrolü yapılıyor...\")\n",
    "\n",
    "culturax_cleaned = []\n",
    "\n",
    "for item in tqdm(culturax_data, desc=\"CulturaX işleniyor\"):\n",
    "    result = cleaner.process_text(item['text'])\n",
    "    if result and result['quality_score'] >= 95:  # %95 üstü kalite\n",
    "        culturax_cleaned.append(result)\n",
    "\n",
    "print(f\"✅ {len(culturax_cleaned)} yüksek kaliteli CulturaX örneği seçildi\")\n",
    "if culturax_cleaned:\n",
    "    print(f\"📊 Ortalama kalite skoru: {np.mean([x['quality_score'] for x in culturax_cleaned]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adım 3: Veri Setlerini Birleştir ve 200K Örnek Seç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# İki veri setini birleştir\n",
    "print(\"🔗 Veri setleri birleştiriliyor...\")\n",
    "\n",
    "all_data = muspdf_cleaned + culturax_cleaned\n",
    "print(f\"📊 Toplam temizlenmiş veri: {len(all_data)} örnek\")\n",
    "\n",
    "# Kalite skoruna göre sırala\n",
    "all_data_sorted = sorted(all_data, key=lambda x: x['quality_score'], reverse=True)\n",
    "\n",
    "# En iyi 200K örneği seç\n",
    "final_dataset = all_data_sorted[:200000]\n",
    "print(f\"✅ En yüksek kaliteli {len(final_dataset)} örnek seçildi\")\n",
    "print(f\"📊 Final veri seti ortalama kalite skoru: {np.mean([x['quality_score'] for x in final_dataset]):.2f}\")\n",
    "print(f\"📊 Min kalite skoru: {min([x['quality_score'] for x in final_dataset]):.2f}\")\n",
    "print(f\"📊 Max kalite skoru: {max([x['quality_score'] for x in final_dataset]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri seti istatistikleri\n",
    "print(\"\\n📈 Veri Seti İstatistikleri:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "lengths = [x['length'] for x in final_dataset]\n",
    "word_counts = [x['word_count'] for x in final_dataset]\n",
    "quality_scores = [x['quality_score'] for x in final_dataset]\n",
    "\n",
    "print(f\"📝 Ortalama metin uzunluğu: {np.mean(lengths):.0f} karakter\")\n",
    "print(f\"📝 Medyan metin uzunluğu: {np.median(lengths):.0f} karakter\")\n",
    "print(f\"📝 Ortalama kelime sayısı: {np.mean(word_counts):.0f} kelime\")\n",
    "print(f\"📊 Kalite skoru dağılımı:\")\n",
    "print(f\"   - Q1 (25%): {np.percentile(quality_scores, 25):.2f}\")\n",
    "print(f\"   - Q2 (50%): {np.percentile(quality_scores, 50):.2f}\")\n",
    "print(f\"   - Q3 (75%): {np.percentile(quality_scores, 75):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adım 4: Veri Setini Kaydet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Dataset formatına dönüştür\n",
    "print(\"💾 Veri seti kaydediliyor...\")\n",
    "\n",
    "# Dataset oluştur\n",
    "dataset_dict = {\n",
    "    'text': [item['text'] for item in final_dataset],\n",
    "    'quality_score': [item['quality_score'] for item in final_dataset],\n",
    "    'length': [item['length'] for item in final_dataset],\n",
    "    'word_count': [item['word_count'] for item in final_dataset]\n",
    "}\n",
    "\n",
    "hf_dataset = Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Farklı formatlarda kaydet\n",
    "# 1. Hugging Face format\n",
    "hf_dataset.save_to_disk(os.path.join(OUTPUT_DIR, 'turkish_200k_dataset'))\n",
    "print(f\"✅ Hugging Face formatında kaydedildi: {OUTPUT_DIR}/turkish_200k_dataset\")\n",
    "\n",
    "# 2. CSV formatı\n",
    "df = pd.DataFrame(dataset_dict)\n",
    "csv_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.csv')\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ CSV formatında kaydedildi: {csv_path}\")\n",
    "\n",
    "# 3. JSONL formatı (her satır bir JSON objesi)\n",
    "jsonl_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.jsonl')\n",
    "df.to_json(jsonl_path, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"✅ JSONL formatında kaydedildi: {jsonl_path}\")\n",
    "\n",
    "# 4. Parquet formatı (daha verimli depolama)\n",
    "parquet_path = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset.parquet')\n",
    "df.to_parquet(parquet_path)\n",
    "print(f\"✅ Parquet formatında kaydedildi: {parquet_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Örnek metinleri göster\n",
    "print(\"\\n📄 Örnek Metinler:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(min(3, len(final_dataset))):\n",
    "    print(f\"\\n🔹 Örnek {i+1}:\")\n",
    "    print(f\"Kalite Skoru: {final_dataset[i]['quality_score']:.2f}\")\n",
    "    print(f\"Uzunluk: {final_dataset[i]['length']} karakter\")\n",
    "    print(f\"Kelime Sayısı: {final_dataset[i]['word_count']} kelime\")\n",
    "    print(f\"Metin (ilk 200 karakter): {final_dataset[i]['text'][:200]}...\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini yükleme testi\n",
    "print(\"\\n🔄 Veri seti yükleme testi...\")\n",
    "\n",
    "try:\n",
    "    # Kaydedilen veri setini tekrar yükle\n",
    "    loaded_dataset = Dataset.load_from_disk(os.path.join(OUTPUT_DIR, 'turkish_200k_dataset'))\n",
    "    print(f\"✅ Veri seti başarıyla yüklendi: {len(loaded_dataset)} örnek\")\n",
    "    print(f\"Sütunlar: {loaded_dataset.column_names}\")\n",
    "    print(f\"İlk örnek: {loaded_dataset[0]['text'][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Veri seti yüklenemedi: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullanım Örneği"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini kullanma örneği\n",
    "print(\"\\n💡 Veri Setini Kullanma Örneği:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\"\"\n",
    "# Veri setini yüklemek için:\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Hugging Face formatından yükle\n",
    "dataset = load_from_disk('turkish_200k_dataset')\n",
    "\n",
    "# veya CSV'den yükle\n",
    "import pandas as pd\n",
    "df = pd.read_csv('turkish_200k_dataset.csv')\n",
    "\n",
    "# Model eğitimi için kullan\n",
    "for example in dataset:\n",
    "    text = example['text']\n",
    "    quality = example['quality_score']\n",
    "    # Model eğitim kodu...\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n✨ Veri seti hazırlama tamamlandı!\")\n",
    "print(f\"📁 Tüm dosyalar {OUTPUT_DIR} dizinine kaydedildi.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Alternatif yükleme yöntemi: push_to_hub kullanarak\nprint(\"\\n📤 Alternatif yükleme yöntemi (push_to_hub)...\")\n\ntry:\n    # Dataset'i doğrudan push_to_hub ile yükle\n    hf_dataset.push_to_hub(\n        repo_id=REPO_ID,\n        token=HF_TOKEN,\n        commit_message=\"Upload Turkish 200K clean dataset\",\n        private=False  # Public repo için False\n    )\n    \n    print(f\"✅ Veri seti push_to_hub ile başarıyla yüklendi!\")\n    print(f\"🔗 Veri setinize erişin: https://huggingface.co/datasets/{REPO_ID}\")\n    \n    # Kullanım örneği\n    print(\"\\n📚 Veri setinizi kullanmak için:\")\n    print(f\"```python\")\n    print(f\"from datasets import load_dataset\")\n    print(f\"dataset = load_dataset('{REPO_ID}')\")\n    print(f\"print(dataset['train'][0])\")\n    print(f\"```\")\n    \nexcept Exception as e:\n    print(f\"❌ push_to_hub hatası: {e}\")\n    print(\"Token ve repository adını kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Veri setini Hugging Face'e yükle\nfrom huggingface_hub import HfApi, create_repo, upload_folder\nimport shutil\n\nprint(f\"\\n🚀 Veri seti Hugging Face'e yükleniyor: {REPO_ID}\")\n\ntry:\n    api = HfApi()\n    \n    # 1. Repository oluştur (yoksa)\n    try:\n        create_repo(\n            repo_id=REPO_ID,\n            repo_type=\"dataset\",\n            private=False,  # Public yapmak için False, private için True\n            token=HF_TOKEN\n        )\n        print(f\"✅ Repository oluşturuldu: {REPO_ID}\")\n    except Exception as e:\n        if \"already exists\" in str(e).lower():\n            print(f\"ℹ️ Repository zaten mevcut: {REPO_ID}\")\n        else:\n            raise e\n    \n    # 2. Yüklenecek dosyaları hazırla\n    upload_dir = os.path.join(OUTPUT_DIR, 'hf_upload')\n    os.makedirs(upload_dir, exist_ok=True)\n    \n    # Dataset dosyalarını kopyala\n    dataset_dir = os.path.join(OUTPUT_DIR, 'turkish_200k_dataset')\n    if os.path.exists(dataset_dir):\n        # Dataset klasörünü kopyala\n        for item in os.listdir(dataset_dir):\n            src = os.path.join(dataset_dir, item)\n            dst = os.path.join(upload_dir, item)\n            if os.path.isfile(src):\n                shutil.copy2(src, dst)\n            else:\n                shutil.copytree(src, dst, dirs_exist_ok=True)\n    \n    # README'yi kopyala\n    shutil.copy2(readme_path, os.path.join(upload_dir, 'README.md'))\n    \n    # 3. Dosyaları yükle\n    print(\"📤 Dosyalar yükleniyor...\")\n    upload_folder(\n        folder_path=upload_dir,\n        repo_id=REPO_ID,\n        repo_type=\"dataset\",\n        token=HF_TOKEN,\n        commit_message=\"Initial dataset upload: Turkish 200K clean dataset\"\n    )\n    \n    print(f\"✅ Veri seti başarıyla yüklendi!\")\n    print(f\"🔗 Veri setinize erişin: https://huggingface.co/datasets/{REPO_ID}\")\n    \n    # Temizlik\n    shutil.rmtree(upload_dir)\n    \nexcept Exception as e:\n    print(f\"❌ Yükleme hatası: {e}\")\n    print(\"Lütfen token'ınızı ve internet bağlantınızı kontrol edin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# README.md oluştur\nreadme_content = f\"\"\"---\nlanguage:\n- tr\nlicense: apache-2.0\nsize_categories:\n- 100K<n<1M\ntask_categories:\n- text-generation\n- text2text-generation\npretty_name: Turkish Clean Dataset 200K\ntags:\n- turkish\n- nlp\n- clean-data\n- high-quality\ndataset_info:\n  features:\n  - name: text\n    dtype: string\n  - name: quality_score\n    dtype: float32\n  - name: length\n    dtype: int32\n  - name: word_count\n    dtype: int32\n  splits:\n  - name: train\n    num_examples: {len(final_dataset)}\n---\n\n# 🇹🇷 Turkish Clean Dataset 200K\n\n## Veri Seti Hakkında\n\nBu veri seti, Türkçe NLP modellerinin eğitimi için hazırlanmış yüksek kaliteli 200.000 metin örneği içerir.\n\n### 📊 Özellikler\n\n- **Toplam Örnek Sayısı**: {len(final_dataset):,}\n- **Ortalama Metin Uzunluğu**: {np.mean(lengths):.0f} karakter\n- **Ortalama Kelime Sayısı**: {np.mean(word_counts):.0f} kelime\n- **Ortalama Kalite Skoru**: {np.mean(quality_scores):.2f}/100\n- **Min Kalite Skoru**: {min(quality_scores):.2f}/100\n- **Max Kalite Skoru**: {max(quality_scores):.2f}/100\n\n### 🔧 Veri İşleme Süreci\n\n1. **Kaynak Veri Setleri**:\n   - muspdf dataset (500K örnek indirildi, kalite eşiği: %88)\n   - CulturaX Turkish (300K örnek indirildi, kalite eşiği: %95)\n\n2. **Temizleme İşlemleri**:\n   - URL, email ve HTML etiketlerinin kaldırılması\n   - Encoding problemlerinin düzeltilmesi (ftfy)\n   - Fazla boşluk ve satır sonlarının temizlenmesi\n   - Türkçe karakterlerin korunması\n\n3. **Kalite Kontrol Kriterleri**:\n   - Türkçe dil tespiti (langdetect)\n   - Türkçe özel karakter oranı kontrolü\n   - Metin uzunluğu değerlendirmesi (100-5000 karakter ideal)\n   - Kelime çeşitliliği analizi\n   - Noktalama işareti dengesi\n   - Büyük/küçük harf oranı\n\n### 📦 Veri Formatı\n\nVeri seti 4 alan içerir:\n- `text`: Temizlenmiş metin\n- `quality_score`: Kalite skoru (0-100)\n- `length`: Metin uzunluğu (karakter)\n- `word_count`: Kelime sayısı\n\n### 💻 Kullanım\n\n```python\nfrom datasets import load_dataset\n\n# Veri setini yükle\ndataset = load_dataset(\"{REPO_ID}\")\n\n# İlk örneği görüntüle\nprint(dataset['train'][0])\n\n# Pandas DataFrame'e dönüştür\ndf = dataset['train'].to_pandas()\n```\n\n### 📈 Kalite Skoru Dağılımı\n\n- Q1 (25%): {np.percentile(quality_scores, 25):.2f}\n- Q2 (50%): {np.percentile(quality_scores, 50):.2f}\n- Q3 (75%): {np.percentile(quality_scores, 75):.2f}\n\n### 🎯 Kullanım Alanları\n\n- Türkçe dil modelleri eğitimi\n- Metin üretimi (text generation)\n- Metin sınıflandırma\n- Dil modelleme\n- Transfer öğrenme\n\n### 📄 Lisans\n\nApache 2.0\n\n### 🤝 Katkıda Bulunma\n\nVeri setini geliştirmek için önerilerinizi ve katkılarınızı bekliyoruz!\n\n### 📮 İletişim\n\nSorularınız için issue açabilir veya tartışma bölümünü kullanabilirsiniz.\n\"\"\"\n\n# README'yi kaydet\nreadme_path = os.path.join(OUTPUT_DIR, 'README.md')\nwith open(readme_path, 'w', encoding='utf-8') as f:\n    f.write(readme_content)\nprint(f\"✅ README.md oluşturuldu: {readme_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Veri seti bilgilerini ayarla\nDATASET_NAME = input(\"Veri seti adını girin (örn: turkish-200k-clean): \").strip()\nif not DATASET_NAME:\n    DATASET_NAME = \"turkish-200k-clean\"\n\nUSERNAME = input(\"Hugging Face kullanıcı adınızı girin: \").strip()\nif not USERNAME:\n    print(\"❌ Kullanıcı adı gerekli!\")\nelse:\n    REPO_ID = f\"{USERNAME}/{DATASET_NAME}\"\n    print(f\"\\n📦 Veri seti repo ID: {REPO_ID}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Hugging Face Hub'a giriş yap\nfrom huggingface_hub import HfApi, login, create_repo\nimport getpass\n\nprint(\"🔐 Hugging Face'e giriş yapılıyor...\")\nprint(\"Token'ınızı https://huggingface.co/settings/tokens adresinden alabilirsiniz.\")\nprint(\"Token'ınız güvenli bir şekilde saklanacaktır.\\n\")\n\n# Token'ı güvenli şekilde al\nif IN_COLAB:\n    from google.colab import userdata\n    try:\n        # Colab secrets'tan token'ı almayı dene\n        HF_TOKEN = userdata.get('HF_TOKEN')\n        print(\"✅ Token Colab secrets'tan alındı\")\n    except:\n        # Manuel olarak iste\n        HF_TOKEN = getpass.getpass(\"Hugging Face Token'ınızı girin: \")\nelse:\n    # Önce çevre değişkeninden kontrol et\n    HF_TOKEN = os.environ.get('HF_TOKEN')\n    if not HF_TOKEN:\n        HF_TOKEN = getpass.getpass(\"Hugging Face Token'ınızı girin: \")\n\n# Giriş yap\ntry:\n    login(token=HF_TOKEN, add_to_git_credential=True)\n    print(\"✅ Hugging Face'e başarıyla giriş yapıldı!\")\nexcept Exception as e:\n    print(f\"❌ Giriş başarısız: {e}\")\n    print(\"Token'ınızı kontrol edip tekrar deneyin.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Adım 5: Veri Setini Hugging Face'e Yükle",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}