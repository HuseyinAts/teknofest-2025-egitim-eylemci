{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Turkcell-LLM-7B Knowledge Distillation Pipeline\n",
    "## T√ºrk√ße Eƒüitim Asistanƒ± i√ßin Optimize Edilmi≈ü - TEKNOFEST 2025\n",
    "\n",
    "Bu notebook, **Turkcell-LLM-7B** teacher model kullanarak T√ºrk√ße i√ßin optimize edilmi≈ü knowledge distillation pipeline'ƒ± i√ßerir.\n",
    "\n",
    "### ‚úÖ √ñzellikler:\n",
    "- Turkcell-LLM-7B teacher model\n",
    "- T√ºrk√ße √∂zel tokenizer entegrasyonu\n",
    "- Mixed precision training (bf16)\n",
    "- Gradient checkpointing\n",
    "- Curriculum learning\n",
    "- Knowledge distillation yapƒ±landƒ±rmasƒ±\n",
    "- Kapsamlƒ± hata kurtarma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU kontrol√º ve sistem bilgisi\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def check_gpu_and_setup():\n",
    "    \"\"\"GPU kontrol√º ve optimal ayarlar\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"‚úÖ GPU Bulundu: {gpu_name}\")\n",
    "        \n",
    "        # A100 i√ßin √∂zel optimizasyonlar\n",
    "        if 'A100' in gpu_name:\n",
    "            print(\"üöÄ A100 GPU tespit edildi - √ñzel optimizasyonlar aktif\")\n",
    "            # TF32 precision for A100\n",
    "            torch.backends.cuda.matmul.allow_tf32 = True\n",
    "            torch.backends.cudnn.allow_tf32 = True\n",
    "            # B√ºy√ºk batch size kullanƒ±labilir\n",
    "            optimal_batch_size = 32\n",
    "        else:\n",
    "            optimal_batch_size = 16\n",
    "            \n",
    "        # CUDA √∂zellikleri\n",
    "        print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "        print(f\"  GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "        print(f\"  Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "        \n",
    "        return optimal_batch_size\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU bulunamadƒ±, CPU kullanƒ±lacak\")\n",
    "        return 8\n",
    "\n",
    "BATCH_SIZE = check_gpu_and_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli k√ºt√ºphaneleri y√ºkle\n",
    "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 bitsandbytes>=0.41.3 peft>=0.7.1\n",
    "!pip install -q datasets evaluate nltk rouge-score sacrebleu bert-score\n",
    "!pip install -q sentencepiece protobuf ftfy langdetect\n",
    "\n",
    "print(\"‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Turkcell Teacher Model Y√ºkleme\n",
    "\n",
    "T√ºrk√ße i√ßin √∂zel eƒüitilmi≈ü Turkcell-LLM-7B modelini teacher olarak kullanacaƒüƒ±z:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict, Any\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "@dataclass\n",
    "class TeacherModelConfig:\n",
    "    \"\"\"Teacher model konfig√ºrasyonu\"\"\"\n",
    "    model_id: str\n",
    "    turkish_optimized: bool\n",
    "    min_gpu_memory_gb: float\n",
    "    recommended_batch_size: int\n",
    "    distillation_temperature: float\n",
    "    description: str\n",
    "\n",
    "# T√ºrk√ße i√ßin optimize edilmi≈ü teacher model se√ßenekleri\n",
    "TEACHER_MODELS = {\n",
    "    \"turkcell/Turkcell-LLM-7b-v1\": TeacherModelConfig(\n",
    "        model_id=\"turkcell/Turkcell-LLM-7b-v1\",\n",
    "        turkish_optimized=True,\n",
    "        min_gpu_memory_gb=16,\n",
    "        recommended_batch_size=8,\n",
    "        distillation_temperature=3.0,\n",
    "        description=\"Turkcell'in T√ºrk√ße i√ßin √∂zel eƒüitilmi≈ü 7B modeli\"\n",
    "    ),\n",
    "    \"ytu-ce-cosmos/turkish-gpt2-large\": TeacherModelConfig(\n",
    "        model_id=\"ytu-ce-cosmos/turkish-gpt2-large\",\n",
    "        turkish_optimized=True,\n",
    "        min_gpu_memory_gb=8,\n",
    "        recommended_batch_size=16,\n",
    "        distillation_temperature=2.5,\n",
    "        description=\"YT√ú Turkish GPT-2 Large - Hafif ve hƒ±zlƒ±\"\n",
    "    ),\n",
    "    \"google/mt5-xl\": TeacherModelConfig(\n",
    "        model_id=\"google/mt5-xl\",\n",
    "        turkish_optimized=True,\n",
    "        min_gpu_memory_gb=24,\n",
    "        recommended_batch_size=4,\n",
    "        distillation_temperature=3.5,\n",
    "        description=\"Google mT5-XL - √áok dilli, T√ºrk√ße dahil\"\n",
    "    ),\n",
    "    \"dbmdz/bert-base-turkish-cased\": TeacherModelConfig(\n",
    "        model_id=\"dbmdz/bert-base-turkish-cased\",\n",
    "        turkish_optimized=True,\n",
    "        min_gpu_memory_gb=4,\n",
    "        recommended_batch_size=32,\n",
    "        distillation_temperature=2.0,\n",
    "        description=\"Turkish BERT - Encoder model, hafif\"\n",
    "    ),\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\": TeacherModelConfig(\n",
    "        model_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        turkish_optimized=False,\n",
    "        min_gpu_memory_gb=16,\n",
    "        recommended_batch_size=8,\n",
    "        distillation_temperature=4.0,\n",
    "        description=\"Qwen 2.5 - G√º√ßl√º √ßok dilli model\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def select_optimal_teacher(gpu_memory_gb: float = 40) -> TeacherModelConfig:\n",
    "    \"\"\"GPU belleƒüine g√∂re optimal teacher model se√ß\"\"\"\n",
    "    suitable_models = [\n",
    "        config for config in TEACHER_MODELS.values()\n",
    "        if config.min_gpu_memory_gb <= gpu_memory_gb\n",
    "    ]\n",
    "    \n",
    "    # T√ºrk√ße optimize modelleri √∂nceliklendir\n",
    "    turkish_models = [m for m in suitable_models if m.turkish_optimized]\n",
    "    \n",
    "    if turkish_models:\n",
    "        # En b√ºy√ºk T√ºrk√ße modeli se√ß\n",
    "        return max(turkish_models, key=lambda x: x.min_gpu_memory_gb)\n",
    "    elif suitable_models:\n",
    "        return suitable_models[0]\n",
    "    else:\n",
    "        return TEACHER_MODELS[\"dbmdz/bert-base-turkish-cased\"]  # Fallback\n",
    "\n",
    "# A100 i√ßin optimal teacher se√ßimi (40GB veya 80GB)\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "else:\n",
    "    gpu_memory = 16  # Default\n",
    "\n",
    "selected_teacher = select_optimal_teacher(gpu_memory)\n",
    "print(f\"\\nüìö Se√ßilen Teacher Model: {selected_teacher.model_id}\")\n",
    "print(f\"   {selected_teacher.description}\")\n",
    "print(f\"   √ñnerilen batch size: {selected_teacher.recommended_batch_size}\")\n",
    "print(f\"   Distillation temperature: {selected_teacher.distillation_temperature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß 2. Advanced Data Validation & Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass, field\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@dataclass\n",
    "class DataQualityReport:\n",
    "    \"\"\"Veri kalite raporu\"\"\"\n",
    "    total_samples: int\n",
    "    valid_samples: int\n",
    "    duplicate_samples: int\n",
    "    contaminated_samples: int\n",
    "    low_quality_samples: int\n",
    "    class_distribution: Dict[str, int]\n",
    "    length_statistics: Dict[str, float]\n",
    "    quality_scores: List[float]\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "    \n",
    "class AdvancedDataValidator:\n",
    "    \"\"\"Geli≈ümi≈ü veri doƒürulama ve kalite kontrol\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 min_length: int = 10,\n",
    "                 max_length: int = 2048,\n",
    "                 min_quality_score: float = 0.7,\n",
    "                 contamination_threshold: float = 0.8):\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.min_quality_score = min_quality_score\n",
    "        self.contamination_threshold = contamination_threshold\n",
    "        \n",
    "        # T√ºrk√ße √∂zel karakterler\n",
    "        self.turkish_chars = set('√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú')\n",
    "        \n",
    "        # Temizleme regex'leri\n",
    "        self.url_pattern = re.compile(r'https?://\\S+')\n",
    "        self.email_pattern = re.compile(r'\\S+@\\S+')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        \n",
    "        # Veri hash'leri (tekille≈ütirme i√ßin)\n",
    "        self.seen_hashes = set()\n",
    "        \n",
    "    def calculate_quality_score(self, text: str) -> float:\n",
    "        \"\"\"Metin kalite skoru hesapla (0-1)\"\"\"\n",
    "        if not text:\n",
    "            return 0.0\n",
    "            \n",
    "        scores = []\n",
    "        \n",
    "        # 1. Uzunluk skoru\n",
    "        text_len = len(text)\n",
    "        if self.min_length <= text_len <= self.max_length:\n",
    "            scores.append(1.0)\n",
    "        elif text_len < self.min_length:\n",
    "            scores.append(text_len / self.min_length)\n",
    "        else:\n",
    "            scores.append(max(0, 1 - (text_len - self.max_length) / self.max_length))\n",
    "            \n",
    "        # 2. T√ºrk√ße karakter oranƒ±\n",
    "        turkish_ratio = sum(1 for c in text if c in self.turkish_chars) / max(len(text), 1)\n",
    "        scores.append(min(turkish_ratio * 50, 1.0))  # %2 T√ºrk√ße karakter = tam puan\n",
    "        \n",
    "        # 3. Kelime √ße≈üitliliƒüi\n",
    "        words = text.lower().split()\n",
    "        if words:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            scores.append(unique_ratio)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "            \n",
    "        # 4. Alfanumerik oran (√ßok fazla sembol olmamalƒ±)\n",
    "        alnum_ratio = sum(1 for c in text if c.isalnum() or c.isspace()) / max(len(text), 1)\n",
    "        scores.append(alnum_ratio)\n",
    "        \n",
    "        # 5. C√ºmle yapƒ±sƒ± (noktalama)\n",
    "        sentence_endings = sum(1 for c in text if c in '.!?')\n",
    "        expected_sentences = len(words) / 15 if words else 0  # Ortalama 15 kelime/c√ºmle\n",
    "        if expected_sentences > 0:\n",
    "            sentence_score = min(sentence_endings / expected_sentences, 1.0)\n",
    "            scores.append(sentence_score)\n",
    "        else:\n",
    "            scores.append(0.5)\n",
    "            \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def is_duplicate(self, text: str) -> bool:\n",
    "        \"\"\"Tekillik kontrol√º\"\"\"\n",
    "        text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "        if text_hash in self.seen_hashes:\n",
    "            return True\n",
    "        self.seen_hashes.add(text_hash)\n",
    "        return False\n",
    "    \n",
    "    def check_contamination(self, train_text: str, test_texts: List[str]) -> bool:\n",
    "        \"\"\"Test/train veri kirlenmesi kontrol√º\"\"\"\n",
    "        if not test_texts:\n",
    "            return False\n",
    "            \n",
    "        train_words = set(train_text.lower().split())\n",
    "        \n",
    "        for test_text in test_texts:\n",
    "            test_words = set(test_text.lower().split())\n",
    "            overlap = len(train_words.intersection(test_words))\n",
    "            similarity = overlap / max(len(train_words), len(test_words), 1)\n",
    "            \n",
    "            if similarity > self.contamination_threshold:\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Metni temizle\"\"\"\n",
    "        # HTML, URL, email temizle\n",
    "        text = self.html_pattern.sub('', text)\n",
    "        text = self.url_pattern.sub('[URL]', text)\n",
    "        text = self.email_pattern.sub('[EMAIL]', text)\n",
    "        \n",
    "        # Fazla bo≈üluklarƒ± temizle\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def validate_dataset(self, \n",
    "                        train_data: List[Dict],\n",
    "                        test_data: Optional[List[Dict]] = None) -> DataQualityReport:\n",
    "        \"\"\"Veri setini doƒürula ve rapor olu≈ütur\"\"\"\n",
    "        \n",
    "        valid_samples = []\n",
    "        duplicate_count = 0\n",
    "        contaminated_count = 0\n",
    "        low_quality_count = 0\n",
    "        quality_scores = []\n",
    "        lengths = []\n",
    "        class_counts = Counter()\n",
    "        warnings = []\n",
    "        \n",
    "        # Test metinleri hazƒ±rla (kirlenme kontrol√º i√ßin)\n",
    "        test_texts = [item.get('text', '') for item in test_data] if test_data else []\n",
    "        \n",
    "        for item in train_data:\n",
    "            text = item.get('text', '')\n",
    "            label = item.get('label', 'unknown')\n",
    "            \n",
    "            # Temizle\n",
    "            text = self.clean_text(text)\n",
    "            \n",
    "            # Uzunluk kontrol√º\n",
    "            if len(text) < self.min_length or len(text) > self.max_length:\n",
    "                low_quality_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Tekillik kontrol√º\n",
    "            if self.is_duplicate(text):\n",
    "                duplicate_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Kalite skoru\n",
    "            quality = self.calculate_quality_score(text)\n",
    "            quality_scores.append(quality)\n",
    "            \n",
    "            if quality < self.min_quality_score:\n",
    "                low_quality_count += 1\n",
    "                continue\n",
    "                \n",
    "            # Kirlenme kontrol√º\n",
    "            if test_texts and self.check_contamination(text, test_texts[:100]):  # ƒ∞lk 100 test √∂rneƒüi\n",
    "                contaminated_count += 1\n",
    "                warnings.append(f\"Potential contamination detected in sample\")\n",
    "                continue\n",
    "                \n",
    "            # Valid sample\n",
    "            valid_samples.append({\n",
    "                'text': text,\n",
    "                'label': label,\n",
    "                'quality_score': quality\n",
    "            })\n",
    "            \n",
    "            lengths.append(len(text))\n",
    "            class_counts[label] += 1\n",
    "            \n",
    "        # Sƒ±nƒ±f dengesizliƒüi kontrol√º\n",
    "        if class_counts:\n",
    "            max_class = max(class_counts.values())\n",
    "            min_class = min(class_counts.values())\n",
    "            if max_class > 3 * min_class:\n",
    "                warnings.append(\"Severe class imbalance detected\")\n",
    "                \n",
    "        # ƒ∞statistikler\n",
    "        length_stats = {\n",
    "            'mean': np.mean(lengths) if lengths else 0,\n",
    "            'std': np.std(lengths) if lengths else 0,\n",
    "            'min': min(lengths) if lengths else 0,\n",
    "            'max': max(lengths) if lengths else 0\n",
    "        }\n",
    "        \n",
    "        return DataQualityReport(\n",
    "            total_samples=len(train_data),\n",
    "            valid_samples=len(valid_samples),\n",
    "            duplicate_samples=duplicate_count,\n",
    "            contaminated_samples=contaminated_count,\n",
    "            low_quality_samples=low_quality_count,\n",
    "            class_distribution=dict(class_counts),\n",
    "            length_statistics=length_stats,\n",
    "            quality_scores=quality_scores,\n",
    "            warnings=warnings\n",
    "        )\n",
    "\n",
    "# √ñrnek kullanƒ±m\n",
    "validator = AdvancedDataValidator()\n",
    "print(\"‚úÖ Advanced Data Validator hazƒ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 3. Knowledge Distillation with Unified Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "import gc\n",
    "\n",
    "class UnifiedTokenizerWrapper:\n",
    "    \"\"\"Teacher ve Student model i√ßin unified tokenizer wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, teacher_tokenizer, student_tokenizer):\n",
    "        self.teacher_tokenizer = teacher_tokenizer\n",
    "        self.student_tokenizer = student_tokenizer\n",
    "        \n",
    "        # Padding token ayarla\n",
    "        if self.teacher_tokenizer.pad_token is None:\n",
    "            self.teacher_tokenizer.pad_token = self.teacher_tokenizer.eos_token\n",
    "        if self.student_tokenizer.pad_token is None:\n",
    "            self.student_tokenizer.pad_token = self.student_tokenizer.eos_token\n",
    "            \n",
    "    def encode_for_teacher(self, texts, max_length=512, return_tensors='pt'):\n",
    "        \"\"\"Teacher model i√ßin encode\"\"\"\n",
    "        return self.teacher_tokenizer(\n",
    "            texts,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=return_tensors\n",
    "        )\n",
    "        \n",
    "    def encode_for_student(self, texts, max_length=512, return_tensors='pt'):\n",
    "        \"\"\"Student model i√ßin encode\"\"\"\n",
    "        return self.student_tokenizer(\n",
    "            texts,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=return_tensors\n",
    "        )\n",
    "        \n",
    "    def decode(self, token_ids, skip_special_tokens=True, use_teacher=True):\n",
    "        \"\"\"Token'larƒ± decode et\"\"\"\n",
    "        tokenizer = self.teacher_tokenizer if use_teacher else self.student_tokenizer\n",
    "        return tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "class KnowledgeDistillationTrainer:\n",
    "    \"\"\"Bilgi damƒ±tma trainer\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 teacher_model,\n",
    "                 student_model,\n",
    "                 unified_tokenizer,\n",
    "                 temperature: float = 3.0,\n",
    "                 alpha: float = 0.7,\n",
    "                 use_gradient_checkpointing: bool = True,\n",
    "                 mixed_precision: str = 'bf16'):\n",
    "        \n",
    "        self.teacher_model = teacher_model\n",
    "        self.student_model = student_model\n",
    "        self.unified_tokenizer = unified_tokenizer\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha  # KD loss weight\n",
    "        self.mixed_precision = mixed_precision\n",
    "        \n",
    "        # Teacher modeli eval moduna al ve dondur\n",
    "        self.teacher_model.eval()\n",
    "        for param in self.teacher_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Gradient checkpointing\n",
    "        if use_gradient_checkpointing:\n",
    "            self.student_model.gradient_checkpointing_enable()\n",
    "            print(\"‚úÖ Gradient checkpointing aktif\")\n",
    "            \n",
    "        # Mixed precision setup\n",
    "        if mixed_precision == 'bf16' and torch.cuda.is_bf16_supported():\n",
    "            self.use_amp = True\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"‚úÖ BF16 mixed precision aktif\")\n",
    "        else:\n",
    "            self.use_amp = False\n",
    "            \n",
    "    def compute_distillation_loss(self, student_logits, teacher_logits, labels):\n",
    "        \"\"\"Distillation loss hesapla\"\"\"\n",
    "        # KL Divergence loss\n",
    "        kd_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_logits / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        # Student CE loss\n",
    "        student_loss = F.cross_entropy(\n",
    "            student_logits.view(-1, student_logits.size(-1)),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * kd_loss + (1 - self.alpha) * student_loss\n",
    "        \n",
    "        return total_loss, kd_loss, student_loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_teacher_logits(self, input_ids, attention_mask):\n",
    "        \"\"\"Teacher model'den logits al\"\"\"\n",
    "        outputs = self.teacher_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        return outputs.logits\n",
    "    \n",
    "    def training_step(self, batch, optimizer):\n",
    "        \"\"\"Bir training step\"\"\"\n",
    "        # Teacher encoding\n",
    "        teacher_inputs = self.unified_tokenizer.encode_for_teacher(\n",
    "            batch['text'], \n",
    "            return_tensors='pt'\n",
    "        ).to(self.teacher_model.device)\n",
    "        \n",
    "        # Student encoding\n",
    "        student_inputs = self.unified_tokenizer.encode_for_student(\n",
    "            batch['text'],\n",
    "            return_tensors='pt'\n",
    "        ).to(self.student_model.device)\n",
    "        \n",
    "        # Teacher forward (no grad)\n",
    "        teacher_logits = self.get_teacher_logits(\n",
    "            teacher_inputs['input_ids'],\n",
    "            teacher_inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Student forward with mixed precision\n",
    "        if self.use_amp:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                student_outputs = self.student_model(\n",
    "                    input_ids=student_inputs['input_ids'],\n",
    "                    attention_mask=student_inputs['attention_mask']\n",
    "                )\n",
    "                student_logits = student_outputs.logits\n",
    "                \n",
    "                # Compute loss\n",
    "                total_loss, kd_loss, student_loss = self.compute_distillation_loss(\n",
    "                    student_logits,\n",
    "                    teacher_logits,\n",
    "                    student_inputs['input_ids']\n",
    "                )\n",
    "        else:\n",
    "            student_outputs = self.student_model(\n",
    "                input_ids=student_inputs['input_ids'],\n",
    "                attention_mask=student_inputs['attention_mask']\n",
    "            )\n",
    "            student_logits = student_outputs.logits\n",
    "            \n",
    "            total_loss, kd_loss, student_loss = self.compute_distillation_loss(\n",
    "                student_logits,\n",
    "                teacher_logits,\n",
    "                student_inputs['input_ids']\n",
    "            )\n",
    "        \n",
    "        # Backward\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(total_loss).backward()\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'kd_loss': kd_loss.item(),\n",
    "            'student_loss': student_loss.item()\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Knowledge Distillation Trainer hazƒ±r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. Turkish-Specific Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# NLTK data indir\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class TurkishEvaluationMetrics:\n",
    "    \"\"\"T√ºrk√ße i√ßin √∂zelle≈ütirilmi≈ü deƒüerlendirme metrikleri\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Metrik y√ºkleyiciler\n",
    "        try:\n",
    "            self.bertscore = load(\"bertscore\")\n",
    "            self.sacrebleu = load(\"sacrebleu\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Bazƒ± metrikler y√ºklenemedi\")\n",
    "            self.bertscore = None\n",
    "            self.sacrebleu = None\n",
    "            \n",
    "        self.rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)\n",
    "        \n",
    "        # T√ºrk√ße √∂zel karakterler i√ßin normalizasyon\n",
    "        self.turkish_chars = {\n",
    "            'ƒ±': 'i', 'ƒü': 'g', '√º': 'u', '≈ü': 's', '√∂': 'o', '√ß': 'c',\n",
    "            'ƒ∞': 'I', 'ƒû': 'G', '√ú': 'U', '≈û': 'S', '√ñ': 'O', '√á': 'C'\n",
    "        }\n",
    "        \n",
    "    def normalize_turkish(self, text: str) -> str:\n",
    "        \"\"\"T√ºrk√ße karakterleri normalize et (opsiyonel)\"\"\"\n",
    "        for tr_char, eng_char in self.turkish_chars.items():\n",
    "            text = text.replace(tr_char, eng_char)\n",
    "        return text\n",
    "        \n",
    "    def calculate_bleu(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"BLEU skorlarƒ± hesapla\"\"\"\n",
    "        bleu_scores = []\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            # Tokenize\n",
    "            pred_tokens = pred.split()\n",
    "            ref_tokens = [ref.split()]\n",
    "            \n",
    "            # BLEU-1 to BLEU-4\n",
    "            scores = []\n",
    "            for n in range(1, 5):\n",
    "                score = sentence_bleu(\n",
    "                    ref_tokens, \n",
    "                    pred_tokens,\n",
    "                    weights=tuple([1/n] * n + [0] * (4-n))\n",
    "                )\n",
    "                scores.append(score)\n",
    "                \n",
    "            bleu_scores.append(scores)\n",
    "            \n",
    "        bleu_scores = np.array(bleu_scores)\n",
    "        \n",
    "        return {\n",
    "            'bleu1': np.mean(bleu_scores[:, 0]),\n",
    "            'bleu2': np.mean(bleu_scores[:, 1]),\n",
    "            'bleu3': np.mean(bleu_scores[:, 2]),\n",
    "            'bleu4': np.mean(bleu_scores[:, 3]),\n",
    "        }\n",
    "        \n",
    "    def calculate_rouge(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"ROUGE skorlarƒ± hesapla\"\"\"\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for pred, ref in zip(predictions, references):\n",
    "            scores = self.rouge.score(pred, ref)\n",
    "            for key in rouge_scores:\n",
    "                rouge_scores[key].append(scores[key].fmeasure)\n",
    "                \n",
    "        return {\n",
    "            key: np.mean(values) \n",
    "            for key, values in rouge_scores.items()\n",
    "        }\n",
    "        \n",
    "    def calculate_bertscore(self, predictions: List[str], references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"BERTScore hesapla\"\"\"\n",
    "        if self.bertscore is None:\n",
    "            return {'bertscore_f1': 0.0}\n",
    "            \n",
    "        results = self.bertscore.compute(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "            lang=\"tr\"  # T√ºrk√ße i√ßin\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'bertscore_precision': np.mean(results['precision']),\n",
    "            'bertscore_recall': np.mean(results['recall']),\n",
    "            'bertscore_f1': np.mean(results['f1'])\n",
    "        }\n",
    "        \n",
    "    def calculate_turkish_specific_metrics(self, \n",
    "                                          predictions: List[str], \n",
    "                                          references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"T√ºrk√ße'ye √∂zel metrikler\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # T√ºrk√ße karakter korunumu\n",
    "        tr_char_preservation = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            ref_tr_chars = sum(1 for c in ref if c in '√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú')\n",
    "            pred_tr_chars = sum(1 for c in pred if c in '√ßƒüƒ±√∂≈ü√º√áƒûƒ∞√ñ≈û√ú')\n",
    "            \n",
    "            if ref_tr_chars > 0:\n",
    "                preservation = min(pred_tr_chars / ref_tr_chars, 1.0)\n",
    "                tr_char_preservation.append(preservation)\n",
    "                \n",
    "        metrics['turkish_char_preservation'] = np.mean(tr_char_preservation) if tr_char_preservation else 0.0\n",
    "        \n",
    "        # Kelime uzunluƒüu benzerliƒüi (T√ºrk√ße'de √∂nemli)\n",
    "        length_similarity = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            pred_len = len(pred.split())\n",
    "            ref_len = len(ref.split())\n",
    "            if ref_len > 0:\n",
    "                similarity = 1 - abs(pred_len - ref_len) / ref_len\n",
    "                length_similarity.append(max(0, similarity))\n",
    "                \n",
    "        metrics['length_similarity'] = np.mean(length_similarity) if length_similarity else 0.0\n",
    "        \n",
    "        return metrics\n",
    "        \n",
    "    def evaluate_all(self, \n",
    "                    predictions: List[str], \n",
    "                    references: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"T√ºm metrikleri hesapla\"\"\"\n",
    "        all_metrics = {}\n",
    "        \n",
    "        # BLEU\n",
    "        all_metrics.update(self.calculate_bleu(predictions, references))\n",
    "        \n",
    "        # ROUGE\n",
    "        all_metrics.update(self.calculate_rouge(predictions, references))\n",
    "        \n",
    "        # BERTScore\n",
    "        if len(predictions) < 100:  # B√ºy√ºk veri setleri i√ßin yava≈ü olabilir\n",
    "            all_metrics.update(self.calculate_bertscore(predictions, references))\n",
    "            \n",
    "        # T√ºrk√ße √∂zel\n",
    "        all_metrics.update(self.calculate_turkish_specific_metrics(predictions, references))\n",
    "        \n",
    "        return all_metrics\n",
    "\n",
    "# Test\n",
    "evaluator = TurkishEvaluationMetrics()\n",
    "print(\"‚úÖ Turkish Evaluation Metrics hazƒ±r\")\n",
    "\n",
    "# √ñrnek deƒüerlendirme\n",
    "test_preds = [\"Bu bir test c√ºmlesidir.\"]\n",
    "test_refs = [\"Bu bir deneme c√ºmlesidir.\"]\n",
    "test_metrics = evaluator.evaluate_all(test_preds, test_refs)\n",
    "print(\"\\nüìä √ñrnek metrikler:\")\n",
    "for key, value in test_metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ 5. Curriculum Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CurriculumLearningDataset(Dataset):\n",
    "    \"\"\"M√ºfredat √∂ƒürenmesi i√ßin veri seti\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: List[Dict],\n",
    "                 difficulty_scorer=None,\n",
    "                 curriculum_strategy: str = 'linear'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Veri listesi\n",
    "            difficulty_scorer: Zorluk skoru hesaplayan fonksiyon\n",
    "            curriculum_strategy: 'linear', 'exponential', 'adaptive'\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.curriculum_strategy = curriculum_strategy\n",
    "        \n",
    "        # Zorluk skorlarƒ±nƒ± hesapla\n",
    "        if difficulty_scorer is None:\n",
    "            self.difficulty_scorer = self._default_difficulty_scorer\n",
    "        else:\n",
    "            self.difficulty_scorer = difficulty_scorer\n",
    "            \n",
    "        self._calculate_difficulties()\n",
    "        self._sort_by_difficulty()\n",
    "        \n",
    "        # Curriculum state\n",
    "        self.current_phase = 0\n",
    "        self.total_phases = 5\n",
    "        self.samples_per_phase = len(self.data) // self.total_phases\n",
    "        \n",
    "    def _default_difficulty_scorer(self, item: Dict) -> float:\n",
    "        \"\"\"Varsayƒ±lan zorluk skoru hesaplama\"\"\"\n",
    "        text = item.get('text', '')\n",
    "        \n",
    "        # Zorluk fakt√∂rleri\n",
    "        factors = []\n",
    "        \n",
    "        # 1. Uzunluk (daha uzun = daha zor)\n",
    "        length_score = min(len(text) / 1000, 1.0)\n",
    "        factors.append(length_score)\n",
    "        \n",
    "        # 2. Kelime √ße≈üitliliƒüi\n",
    "        words = text.lower().split()\n",
    "        if words:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            factors.append(unique_ratio)\n",
    "        else:\n",
    "            factors.append(0)\n",
    "            \n",
    "        # 3. Ortalama kelime uzunluƒüu\n",
    "        if words:\n",
    "            avg_word_len = np.mean([len(w) for w in words])\n",
    "            factors.append(min(avg_word_len / 10, 1.0))\n",
    "        else:\n",
    "            factors.append(0)\n",
    "            \n",
    "        # 4. C√ºmle karma≈üƒ±klƒ±ƒüƒ± (noktalama sayƒ±sƒ±)\n",
    "        punct_count = sum(1 for c in text if c in '.,;:!?')\n",
    "        factors.append(min(punct_count / 20, 1.0))\n",
    "        \n",
    "        return np.mean(factors)\n",
    "        \n",
    "    def _calculate_difficulties(self):\n",
    "        \"\"\"Her √∂rnek i√ßin zorluk skoru hesapla\"\"\"\n",
    "        for item in self.data:\n",
    "            item['difficulty'] = self.difficulty_scorer(item)\n",
    "            \n",
    "    def _sort_by_difficulty(self):\n",
    "        \"\"\"Veriyi zorluƒüa g√∂re sƒ±rala\"\"\"\n",
    "        self.data.sort(key=lambda x: x['difficulty'])\n",
    "        \n",
    "    def advance_curriculum(self):\n",
    "        \"\"\"M√ºfredatta ilerleme\"\"\"\n",
    "        if self.current_phase < self.total_phases - 1:\n",
    "            self.current_phase += 1\n",
    "            print(f\"üìö Curriculum advanced to phase {self.current_phase + 1}/{self.total_phases}\")\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def get_current_data(self) -> List[Dict]:\n",
    "        \"\"\"Mevcut fazdaki veriyi getir\"\"\"\n",
    "        if self.curriculum_strategy == 'linear':\n",
    "            # Lineer ilerleme\n",
    "            end_idx = min(\n",
    "                (self.current_phase + 1) * self.samples_per_phase,\n",
    "                len(self.data)\n",
    "            )\n",
    "            return self.data[:end_idx]\n",
    "            \n",
    "        elif self.curriculum_strategy == 'exponential':\n",
    "            # √ústel ilerleme\n",
    "            ratio = (self.current_phase + 1) / self.total_phases\n",
    "            end_idx = int(len(self.data) * (ratio ** 2))\n",
    "            return self.data[:max(1, end_idx)]\n",
    "            \n",
    "        elif self.curriculum_strategy == 'adaptive':\n",
    "            # Adaptif (performance bazlƒ± - ≈üimdilik linear)\n",
    "            return self.get_current_data_linear()\n",
    "            \n",
    "        else:\n",
    "            return self.data\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.get_current_data())\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        current_data = self.get_current_data()\n",
    "        return current_data[idx]\n",
    "        \n",
    "    def get_curriculum_stats(self) -> Dict:\n",
    "        \"\"\"M√ºfredat istatistikleri\"\"\"\n",
    "        current_data = self.get_current_data()\n",
    "        difficulties = [item['difficulty'] for item in current_data]\n",
    "        \n",
    "        return {\n",
    "            'current_phase': self.current_phase + 1,\n",
    "            'total_phases': self.total_phases,\n",
    "            'current_samples': len(current_data),\n",
    "            'total_samples': len(self.data),\n",
    "            'avg_difficulty': np.mean(difficulties) if difficulties else 0,\n",
    "            'min_difficulty': min(difficulties) if difficulties else 0,\n",
    "            'max_difficulty': max(difficulties) if difficulties else 0,\n",
    "        }\n",
    "\n",
    "# Test\n",
    "test_data = [\n",
    "    {'text': 'Kƒ±sa metin.'},\n",
    "    {'text': 'Bu biraz daha uzun bir metin √∂rneƒüi.'},\n",
    "    {'text': '√áok uzun ve karma≈üƒ±k bir metin √∂rneƒüi ile devam ediyoruz, noktalama i≈üaretleri de var!'},\n",
    "]\n",
    "\n",
    "curriculum_dataset = CurriculumLearningDataset(\n",
    "    test_data,\n",
    "    curriculum_strategy='linear'\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Curriculum Learning Dataset hazƒ±r\")\n",
    "print(\"\\nüìä Curriculum ƒ∞statistikleri:\")\n",
    "stats = curriculum_dataset.get_curriculum_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 6. Complete Training Pipeline with Error Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Eƒüitim konfig√ºrasyonu\"\"\"\n",
    "    # Model configs\n",
    "    teacher_model_id: str = \"turkcell/Turkcell-LLM-7b-v1\"\n",
    "    student_model_id: str = \"ytu-ce-cosmos/turkish-gpt2-large\"\n",
    "    \n",
    "    # Training configs\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 3\n",
    "    warmup_steps: int = 500\n",
    "    gradient_accumulation_steps: int = 2\n",
    "    \n",
    "    # Distillation configs\n",
    "    temperature: float = 3.0\n",
    "    alpha: float = 0.7\n",
    "    \n",
    "    # Optimization configs\n",
    "    use_gradient_checkpointing: bool = True\n",
    "    mixed_precision: str = \"bf16\"\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Curriculum learning\n",
    "    use_curriculum: bool = True\n",
    "    curriculum_strategy: str = \"linear\"\n",
    "    \n",
    "    # Checkpointing\n",
    "    save_steps: int = 1000\n",
    "    eval_steps: int = 500\n",
    "    checkpoint_dir: str = \"./checkpoints\"\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 3\n",
    "    early_stopping_threshold: float = 0.001\n",
    "\n",
    "class RobustTrainingPipeline:\n",
    "    \"\"\"Hata kurtarma √∂zellikli eƒüitim pipeline'ƒ±\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Checkpoint dizini\n",
    "        self.checkpoint_dir = Path(config.checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Training state\n",
    "        self.global_step = 0\n",
    "        self.current_epoch = 0\n",
    "        self.best_eval_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.training_history = []\n",
    "        \n",
    "    def save_checkpoint(self, \n",
    "                       model, \n",
    "                       optimizer, \n",
    "                       scheduler,\n",
    "                       metrics: Dict,\n",
    "                       is_best: bool = False):\n",
    "        \"\"\"Checkpoint kaydet\"\"\"\n",
    "        checkpoint = {\n",
    "            'global_step': self.global_step,\n",
    "            'epoch': self.current_epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "            'metrics': metrics,\n",
    "            'config': asdict(self.config),\n",
    "            'training_history': self.training_history\n",
    "        }\n",
    "        \n",
    "        # Regular checkpoint\n",
    "        checkpoint_path = self.checkpoint_dir / f\"checkpoint-{self.global_step}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"üíæ Checkpoint saved: {checkpoint_path}\")\n",
    "        \n",
    "        # Best model\n",
    "        if is_best:\n",
    "            best_path = self.checkpoint_dir / \"best_model.pt\"\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"üèÜ Best model saved: {best_path}\")\n",
    "            \n",
    "        # Son 3 checkpoint'i tut\n",
    "        self._cleanup_old_checkpoints()\n",
    "        \n",
    "    def load_checkpoint(self, checkpoint_path: str, model, optimizer=None, scheduler=None):\n",
    "        \"\"\"Checkpoint y√ºkle\"\"\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            \n",
    "        if scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "            \n",
    "        self.global_step = checkpoint.get('global_step', 0)\n",
    "        self.current_epoch = checkpoint.get('epoch', 0)\n",
    "        self.training_history = checkpoint.get('training_history', [])\n",
    "        \n",
    "        print(f\"‚úÖ Checkpoint loaded from: {checkpoint_path}\")\n",
    "        print(f\"   Resuming from epoch {self.current_epoch}, step {self.global_step}\")\n",
    "        \n",
    "        return checkpoint.get('metrics', {})\n",
    "        \n",
    "    def _cleanup_old_checkpoints(self, keep_last: int = 3):\n",
    "        \"\"\"Eski checkpoint'leri temizle\"\"\"\n",
    "        checkpoints = sorted(self.checkpoint_dir.glob(\"checkpoint-*.pt\"))\n",
    "        \n",
    "        if len(checkpoints) > keep_last:\n",
    "            for checkpoint in checkpoints[:-keep_last]:\n",
    "                checkpoint.unlink()\n",
    "                print(f\"üóëÔ∏è Deleted old checkpoint: {checkpoint}\")\n",
    "                \n",
    "    def handle_cuda_oom(self):\n",
    "        \"\"\"CUDA OOM hatasƒ± y√∂netimi\"\"\"\n",
    "        print(\"\\n‚ö†Ô∏è CUDA OOM Detected! Attempting recovery...\")\n",
    "        \n",
    "        # GPU belleƒüini temizle\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "        # Batch size'ƒ± azalt\n",
    "        self.config.batch_size = max(1, self.config.batch_size // 2)\n",
    "        print(f\"   Reduced batch size to: {self.config.batch_size}\")\n",
    "        \n",
    "        # Gradient accumulation'ƒ± artƒ±r\n",
    "        self.config.gradient_accumulation_steps *= 2\n",
    "        print(f\"   Increased gradient accumulation to: {self.config.gradient_accumulation_steps}\")\n",
    "        \n",
    "        time.sleep(5)  # GPU'nun toparlanmasƒ± i√ßin bekle\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def train_with_recovery(self, \n",
    "                           train_dataloader,\n",
    "                           eval_dataloader,\n",
    "                           model,\n",
    "                           optimizer,\n",
    "                           scheduler,\n",
    "                           trainer):\n",
    "        \"\"\"Hata kurtarma √∂zellikli eƒüitim d√∂ng√ºs√º\"\"\"\n",
    "        \n",
    "        max_retries = 3\n",
    "        retry_count = 0\n",
    "        \n",
    "        while retry_count < max_retries:\n",
    "            try:\n",
    "                # Ana eƒüitim d√∂ng√ºs√º\n",
    "                for epoch in range(self.current_epoch, self.config.num_epochs):\n",
    "                    self.current_epoch = epoch\n",
    "                    print(f\"\\nüìö Epoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "                    \n",
    "                    # Training\n",
    "                    model.train()\n",
    "                    epoch_losses = []\n",
    "                    \n",
    "                    progress_bar = tqdm(train_dataloader, desc=\"Training\")\n",
    "                    for batch_idx, batch in enumerate(progress_bar):\n",
    "                        try:\n",
    "                            # Training step\n",
    "                            loss_dict = trainer.training_step(batch, optimizer)\n",
    "                            \n",
    "                            epoch_losses.append(loss_dict['total_loss'])\n",
    "                            self.global_step += 1\n",
    "                            \n",
    "                            # Update progress\n",
    "                            progress_bar.set_postfix({\n",
    "                                'loss': loss_dict['total_loss'],\n",
    "                                'kd_loss': loss_dict['kd_loss']\n",
    "                            })\n",
    "                            \n",
    "                            # Checkpoint kaydet\n",
    "                            if self.global_step % self.config.save_steps == 0:\n",
    "                                metrics = {\n",
    "                                    'train_loss': np.mean(epoch_losses),\n",
    "                                    'learning_rate': optimizer.param_groups[0]['lr']\n",
    "                                }\n",
    "                                self.save_checkpoint(model, optimizer, scheduler, metrics)\n",
    "                                \n",
    "                            # Evaluation\n",
    "                            if self.global_step % self.config.eval_steps == 0:\n",
    "                                eval_loss = self.evaluate(model, eval_dataloader, trainer)\n",
    "                                \n",
    "                                # Early stopping check\n",
    "                                if self.check_early_stopping(eval_loss):\n",
    "                                    print(\"\\nüõë Early stopping triggered!\")\n",
    "                                    return\n",
    "                                    \n",
    "                        except RuntimeError as e:\n",
    "                            if \"out of memory\" in str(e):\n",
    "                                if self.handle_cuda_oom():\n",
    "                                    continue\n",
    "                                else:\n",
    "                                    raise\n",
    "                            else:\n",
    "                                raise\n",
    "                                \n",
    "                    # Epoch biti≈üi\n",
    "                    avg_epoch_loss = np.mean(epoch_losses)\n",
    "                    print(f\"\\nüìä Epoch {epoch + 1} completed. Avg loss: {avg_epoch_loss:.4f}\")\n",
    "                    \n",
    "                    # Curriculum learning advancement\n",
    "                    if hasattr(train_dataloader.dataset, 'advance_curriculum'):\n",
    "                        train_dataloader.dataset.advance_curriculum()\n",
    "                        \n",
    "                # Eƒüitim ba≈üarƒ±yla tamamlandƒ±\n",
    "                print(\"\\n‚úÖ Training completed successfully!\")\n",
    "                break\n",
    "                \n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                print(f\"\\n‚ùå Training error (attempt {retry_count}/{max_retries}): {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                \n",
    "                if retry_count < max_retries:\n",
    "                    print(\"\\nüîÑ Attempting to recover from last checkpoint...\")\n",
    "                    \n",
    "                    # Son checkpoint'i y√ºkle\n",
    "                    latest_checkpoint = self.get_latest_checkpoint()\n",
    "                    if latest_checkpoint:\n",
    "                        self.load_checkpoint(latest_checkpoint, model, optimizer, scheduler)\n",
    "                        time.sleep(10)  # Sistem toparlanmasƒ± i√ßin bekle\n",
    "                    else:\n",
    "                        print(\"No checkpoint found, starting from scratch\")\n",
    "                        self.global_step = 0\n",
    "                        self.current_epoch = 0\n",
    "                else:\n",
    "                    print(\"\\n‚ùå Maximum retries reached. Training failed.\")\n",
    "                    raise\n",
    "                    \n",
    "    def evaluate(self, model, eval_dataloader, trainer) -> float:\n",
    "        \"\"\"Model deƒüerlendirme\"\"\"\n",
    "        model.eval()\n",
    "        eval_losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "                loss_dict = trainer.training_step(batch, None)  # optimizer=None for eval\n",
    "                eval_losses.append(loss_dict['total_loss'])\n",
    "                \n",
    "        avg_eval_loss = np.mean(eval_losses)\n",
    "        print(f\"\\nüìä Evaluation loss: {avg_eval_loss:.4f}\")\n",
    "        \n",
    "        # Best model check\n",
    "        if avg_eval_loss < self.best_eval_loss:\n",
    "            self.best_eval_loss = avg_eval_loss\n",
    "            metrics = {'eval_loss': avg_eval_loss}\n",
    "            self.save_checkpoint(model, None, None, metrics, is_best=True)\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "            \n",
    "        model.train()\n",
    "        return avg_eval_loss\n",
    "        \n",
    "    def check_early_stopping(self, eval_loss: float) -> bool:\n",
    "        \"\"\"Early stopping kontrol√º\"\"\"\n",
    "        if self.patience_counter >= self.config.early_stopping_patience:\n",
    "            return True\n",
    "            \n",
    "        if abs(self.best_eval_loss - eval_loss) < self.config.early_stopping_threshold:\n",
    "            self.patience_counter += 1\n",
    "        else:\n",
    "            self.patience_counter = 0\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def get_latest_checkpoint(self) -> Optional[str]:\n",
    "        \"\"\"En son checkpoint'i bul\"\"\"\n",
    "        checkpoints = list(self.checkpoint_dir.glob(\"checkpoint-*.pt\"))\n",
    "        if checkpoints:\n",
    "            return str(max(checkpoints, key=lambda x: x.stat().st_mtime))\n",
    "        return None\n",
    "\n",
    "# Test\n",
    "config = TrainingConfig()\n",
    "pipeline = RobustTrainingPipeline(config)\n",
    "print(\"‚úÖ Robust Training Pipeline hazƒ±r\")\n",
    "print(f\"\\nüìÅ Checkpoint directory: {pipeline.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 7. Main Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_training():\n",
    "    \"\"\"Ana eƒüitim fonksiyonu\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"üöÄ TURKISH NLP KNOWLEDGE DISTILLATION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # 1. Configuration\n",
    "    config = TrainingConfig(\n",
    "        batch_size=BATCH_SIZE,  # GPU'ya g√∂re ayarlandƒ±\n",
    "        num_epochs=3,\n",
    "        learning_rate=2e-5,\n",
    "        temperature=selected_teacher.distillation_temperature\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìã Configuration:\")\n",
    "    for key, value in asdict(config).items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "    # 2. Load models\n",
    "    print(\"\\nüìö Loading models...\")\n",
    "    \n",
    "    # Teacher model (4-bit quantization)\n",
    "    teacher_bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.teacher_model_id,\n",
    "        quantization_config=teacher_bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.teacher_model_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Teacher model loaded: {config.teacher_model_id}\")\n",
    "    \n",
    "    # Student model\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.student_model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(config.student_model_id)\n",
    "    \n",
    "    print(f\"‚úÖ Student model loaded: {config.student_model_id}\")\n",
    "    \n",
    "    # 3. Unified tokenizer\n",
    "    unified_tokenizer = UnifiedTokenizerWrapper(teacher_tokenizer, student_tokenizer)\n",
    "    \n",
    "    # 4. Initialize trainer\n",
    "    kd_trainer = KnowledgeDistillationTrainer(\n",
    "        teacher_model=teacher_model,\n",
    "        student_model=student_model,\n",
    "        unified_tokenizer=unified_tokenizer,\n",
    "        temperature=config.temperature,\n",
    "        alpha=config.alpha,\n",
    "        use_gradient_checkpointing=config.use_gradient_checkpointing,\n",
    "        mixed_precision=config.mixed_precision\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Training setup complete!\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Ready for training! Load your data and start the pipeline.\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return config, kd_trainer, unified_tokenizer\n",
    "\n",
    "# Ana pipeline'ƒ± ba≈ülat\n",
    "if __name__ == \"__main__\":\n",
    "    config, trainer, tokenizer = main_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
