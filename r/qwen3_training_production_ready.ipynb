{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 QWEN3-8B Turkish Training - Production Ready Version\n",
    "## 100% Google Colab Compatible with Comprehensive Error Handling\n",
    "\n",
    "### Features:\n",
    "- ✅ Full Google Colab compatibility\n",
    "- ✅ Comprehensive error handling\n",
    "- ✅ Automatic fallbacks for all components\n",
    "- ✅ Memory-efficient training\n",
    "- ✅ Production-grade logging\n",
    "- ✅ Automatic recovery from failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 1. Environment Setup & Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('training.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EnvironmentManager:\n",
    "    \"\"\"Manages environment detection and setup\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_environment() -> Dict[str, Any]:\n",
    "        \"\"\"Detect current environment (Colab, Local, etc.)\"\"\"\n",
    "        env_info = {\n",
    "            'platform': platform.system(),\n",
    "            'python_version': sys.version,\n",
    "            'is_colab': False,\n",
    "            'is_kaggle': False,\n",
    "            'has_gpu': False,\n",
    "            'gpu_info': None\n",
    "        }\n",
    "        \n",
    "        # Check if running in Google Colab\n",
    "        try:\n",
    "            import google.colab\n",
    "            env_info['is_colab'] = True\n",
    "            logger.info(\"✅ Running in Google Colab\")\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        # Check if running in Kaggle\n",
    "        if os.path.exists('/kaggle'):\n",
    "            env_info['is_kaggle'] = True\n",
    "            logger.info(\"✅ Running in Kaggle\")\n",
    "        \n",
    "        # Check GPU availability\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                env_info['has_gpu'] = True\n",
    "                env_info['gpu_info'] = {\n",
    "                    'name': torch.cuda.get_device_name(0),\n",
    "                    'memory': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "                    'capability': torch.cuda.get_device_capability(0)\n",
    "                }\n",
    "                logger.info(f\"✅ GPU detected: {env_info['gpu_info']['name']} ({env_info['gpu_info']['memory']:.1f}GB)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"❌ GPU detection failed: {e}\")\n",
    "        \n",
    "        return env_info\n",
    "\n",
    "# Detect environment\n",
    "ENV_INFO = EnvironmentManager.detect_environment()\n",
    "print(f\"Environment: {ENV_INFO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with error handling\n",
    "def install_package(package: str, upgrade: bool = False) -> bool:\n",
    "    \"\"\"Install a package with error handling\"\"\"\n",
    "    try:\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "        if upgrade:\n",
    "            cmd.append(\"--upgrade\")\n",
    "        cmd.append(package)\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(f\"✅ Successfully installed {package}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"❌ Failed to install {package}: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"❌ Error installing {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Core dependencies\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"transformers==4.44.0\",\n",
    "    \"datasets==2.14.0\",\n",
    "    \"accelerate>=0.25.0\",\n",
    "    \"peft==0.11.1\",\n",
    "    \"bitsandbytes==0.43.1\",\n",
    "    \"sentencepiece>=0.1.99\",\n",
    "    \"tiktoken>=0.5.0\",\n",
    "    \"trl>=0.7.0\",\n",
    "    \"psutil\",\n",
    "    \"py-cpuinfo\",\n",
    "    \"numpy<2.0\",  # Compatibility fix\n",
    "]\n",
    "\n",
    "# Optional packages (install with fallback)\n",
    "OPTIONAL_PACKAGES = [\n",
    "    \"wandb\",  # For experiment tracking\n",
    "    \"tensorboard\",  # For visualization\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "print(\"Installing required packages...\")\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    if not install_package(package):\n",
    "        logger.warning(f\"Retrying installation of {package}...\")\n",
    "        install_package(package, upgrade=True)\n",
    "\n",
    "# Install optional packages (don't fail if they can't be installed)\n",
    "for package in OPTIONAL_PACKAGES:\n",
    "    install_package(package)\n",
    "\n",
    "# Platform-specific packages\n",
    "if ENV_INFO['has_gpu'] and ENV_INFO['platform'] != 'Windows':\n",
    "    # Try to install flash-attn (may fail on some systems)\n",
    "    try:\n",
    "        install_package(\"flash-attn>=2.3.0\")\n",
    "    except:\n",
    "        logger.warning(\"Flash Attention not available, using standard attention\")\n",
    "\n",
    "print(\"✅ Package installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 2. System Configuration & GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class GPUManager:\n",
    "    \"\"\"Comprehensive GPU management with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.has_gpu = torch.cuda.is_available()\n",
    "        self.device = None\n",
    "        self.gpu_info = {}\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize GPU with error handling\"\"\"\n",
    "        try:\n",
    "            if self.has_gpu:\n",
    "                self.device = torch.device(\"cuda\")\n",
    "                self.gpu_info = self._get_gpu_info()\n",
    "                self._optimize_gpu_settings()\n",
    "                logger.info(f\"✅ GPU initialized: {self.gpu_info['name']}\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                logger.warning(\"⚠️ No GPU detected, using CPU\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ GPU initialization failed: {e}\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.has_gpu = False\n",
    "    \n",
    "    def _get_gpu_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive GPU information\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            gpu_id = 0\n",
    "            props = torch.cuda.get_device_properties(gpu_id)\n",
    "            \n",
    "            info = {\n",
    "                'name': props.name,\n",
    "                'memory_total': props.total_memory / 1e9,\n",
    "                'memory_reserved': torch.cuda.memory_reserved(gpu_id) / 1e9,\n",
    "                'memory_allocated': torch.cuda.memory_allocated(gpu_id) / 1e9,\n",
    "                'capability': f\"{props.major}.{props.minor}\",\n",
    "                'multi_processor_count': props.multi_processor_count,\n",
    "                'supports_bf16': props.major >= 8,  # Ampere and newer\n",
    "                'supports_flash_attn': props.major >= 7 and props.minor >= 5,\n",
    "                'gpu_type': self._classify_gpu(props.name)\n",
    "            }\n",
    "            \n",
    "            return info\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get GPU info: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _classify_gpu(self, gpu_name: str) -> str:\n",
    "        \"\"\"Classify GPU type for optimization\"\"\"\n",
    "        gpu_name_lower = gpu_name.lower()\n",
    "        \n",
    "        if 't4' in gpu_name_lower:\n",
    "            return 't4'\n",
    "        elif 'v100' in gpu_name_lower:\n",
    "            return 'v100'\n",
    "        elif 'a100' in gpu_name_lower:\n",
    "            return 'a100'\n",
    "        elif 'a10' in gpu_name_lower:\n",
    "            return 'a10'\n",
    "        elif 'rtx 3090' in gpu_name_lower:\n",
    "            return 'rtx3090'\n",
    "        elif 'rtx 4090' in gpu_name_lower:\n",
    "            return 'rtx4090'\n",
    "        else:\n",
    "            return 'generic'\n",
    "    \n",
    "    def _optimize_gpu_settings(self):\n",
    "        \"\"\"Apply GPU-specific optimizations\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Enable TF32 for better performance on Ampere GPUs\n",
    "            if self.gpu_info.get('supports_bf16', False):\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                logger.info(\"✅ TF32 enabled for Ampere GPU\")\n",
    "            \n",
    "            # Set memory fraction to prevent OOM\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "            \n",
    "            # Enable cudnn benchmarking for better performance\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to apply GPU optimizations: {e}\")\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU memory with error handling\"\"\"\n",
    "        try:\n",
    "            if self.has_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "            logger.info(\"✅ Memory cleared\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to clear memory: {e}\")\n",
    "    \n",
    "    def get_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            return {'ram_used': psutil.virtual_memory().percent}\n",
    "        \n",
    "        try:\n",
    "            return {\n",
    "                'gpu_allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "                'gpu_reserved': torch.cuda.memory_reserved() / 1e9,\n",
    "                'gpu_free': (torch.cuda.get_device_properties(0).total_memory - \n",
    "                           torch.cuda.memory_reserved()) / 1e9,\n",
    "                'ram_used': psutil.virtual_memory().percent\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get memory usage: {e}\")\n",
    "            return {}\n",
    "\n",
    "# Initialize GPU Manager\n",
    "gpu_manager = GPUManager()\n",
    "print(f\"GPU Info: {json.dumps(gpu_manager.gpu_info, indent=2)}\")\n",
    "print(f\"Memory Usage: {json.dumps(gpu_manager.get_memory_usage(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 3. Google Drive Integration (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StorageManager:\n",
    "    \"\"\"Manages storage with Google Drive integration for Colab\"\"\"\n",
    "    \n",
    "    def __init__(self, use_drive: bool = True):\n",
    "        self.use_drive = use_drive and ENV_INFO['is_colab']\n",
    "        self.base_path = Path.cwd()\n",
    "        self.drive_path = None\n",
    "        self._setup_storage()\n",
    "    \n",
    "    def _setup_storage(self):\n",
    "        \"\"\"Setup storage with error handling\"\"\"\n",
    "        if self.use_drive:\n",
    "            try:\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive', force_remount=True)\n",
    "                self.drive_path = Path('/content/drive/MyDrive/teknofest-training')\n",
    "                self.drive_path.mkdir(parents=True, exist_ok=True)\n",
    "                logger.info(f\"✅ Google Drive mounted at {self.drive_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"❌ Failed to mount Google Drive: {e}\")\n",
    "                self.use_drive = False\n",
    "        \n",
    "        # Setup local directories\n",
    "        self.setup_directories()\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        directories = [\n",
    "            'models',\n",
    "            'checkpoints',\n",
    "            'logs',\n",
    "            'data',\n",
    "            'cache'\n",
    "        ]\n",
    "        \n",
    "        for dir_name in directories:\n",
    "            dir_path = self.base_path / dir_name\n",
    "            dir_path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            if self.use_drive:\n",
    "                drive_dir = self.drive_path / dir_name\n",
    "                drive_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        logger.info(\"✅ Directories created\")\n",
    "    \n",
    "    def get_path(self, filename: str, use_drive: bool = True) -> Path:\n",
    "        \"\"\"Get appropriate path based on storage settings\"\"\"\n",
    "        if self.use_drive and use_drive:\n",
    "            return self.drive_path / filename\n",
    "        return self.base_path / filename\n",
    "    \n",
    "    def save_checkpoint(self, state: Dict, filename: str):\n",
    "        \"\"\"Save checkpoint with error handling\"\"\"\n",
    "        try:\n",
    "            # Save locally first\n",
    "            local_path = self.base_path / 'checkpoints' / filename\n",
    "            torch.save(state, local_path)\n",
    "            logger.info(f\"✅ Checkpoint saved locally: {local_path}\")\n",
    "            \n",
    "            # Copy to drive if available\n",
    "            if self.use_drive:\n",
    "                drive_path = self.drive_path / 'checkpoints' / filename\n",
    "                import shutil\n",
    "                shutil.copy2(local_path, drive_path)\n",
    "                logger.info(f\"✅ Checkpoint backed up to Drive: {drive_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to save checkpoint: {e}\")\n",
    "    \n",
    "    def load_checkpoint(self, filename: str) -> Optional[Dict]:\n",
    "        \"\"\"Load checkpoint with fallback\"\"\"\n",
    "        paths_to_try = []\n",
    "        \n",
    "        # Add possible paths\n",
    "        paths_to_try.append(self.base_path / 'checkpoints' / filename)\n",
    "        if self.use_drive:\n",
    "            paths_to_try.append(self.drive_path / 'checkpoints' / filename)\n",
    "        \n",
    "        for path in paths_to_try:\n",
    "            if path.exists():\n",
    "                try:\n",
    "                    state = torch.load(path, map_location=gpu_manager.device)\n",
    "                    logger.info(f\"✅ Checkpoint loaded from: {path}\")\n",
    "                    return state\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load checkpoint from {path}: {e}\")\n",
    "        \n",
    "        logger.warning(f\"❌ No checkpoint found: {filename}\")\n",
    "        return None\n",
    "\n",
    "# Initialize Storage Manager\n",
    "storage_manager = StorageManager(use_drive=ENV_INFO['is_colab'])\n",
    "print(f\"Storage initialized. Base path: {storage_manager.base_path}\")\n",
    "if storage_manager.use_drive:\n",
    "    print(f\"Drive path: {storage_manager.drive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 4. Data Loading with Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import hashlib\n",
    "import pickle\n",
    "from typing import Optional, Dict, Any, List\n",
    "\n",
    "class DataManager:\n",
    "    \"\"\"Manages data loading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[Path] = None):\n",
    "        self.cache_dir = cache_dir or storage_manager.get_path('cache')\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.dataset = None\n",
    "        self.tokenized_dataset = None\n",
    "    \n",
    "    def load_dataset_with_fallback(\n",
    "        self,\n",
    "        dataset_name: str = \"Huseyin/turkish-200k-dataset\",\n",
    "        max_samples: Optional[int] = None\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"Load dataset with multiple fallback options\"\"\"\n",
    "        \n",
    "        # Try loading from cache first\n",
    "        cache_file = self.cache_dir / f\"{dataset_name.replace('/', '_')}.pkl\"\n",
    "        \n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.dataset = pickle.load(f)\n",
    "                logger.info(f\"✅ Dataset loaded from cache: {cache_file}\")\n",
    "                return self.dataset\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Cache load failed: {e}\")\n",
    "        \n",
    "        # Try loading from HuggingFace\n",
    "        try:\n",
    "            logger.info(f\"Loading dataset: {dataset_name}\")\n",
    "            dataset = load_dataset(dataset_name, split=\"train\")\n",
    "            \n",
    "            if max_samples:\n",
    "                dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "            \n",
    "            # Split into train/test\n",
    "            split = dataset.train_test_split(test_size=0.02, seed=42)\n",
    "            self.dataset = DatasetDict({\n",
    "                'train': split['train'],\n",
    "                'test': split['test']\n",
    "            })\n",
    "            \n",
    "            # Cache the dataset\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(self.dataset, f)\n",
    "            \n",
    "            logger.info(f\"✅ Dataset loaded: {len(self.dataset['train'])} train, {len(self.dataset['test'])} test\")\n",
    "            return self.dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to load dataset: {e}\")\n",
    "            logger.info(\"Creating fallback dataset...\")\n",
    "            return self._create_fallback_dataset()\n",
    "    \n",
    "    def _create_fallback_dataset(self) -> DatasetDict:\n",
    "        \"\"\"Create a fallback dataset for testing\"\"\"\n",
    "        try:\n",
    "            # Turkish educational content samples\n",
    "            samples = [\n",
    "                {\"text\": \"Python programlama dili, basit sözdizimi ve güçlü kütüphaneleri ile popüler bir dildir.\"},\n",
    "                {\"text\": \"Makine öğrenmesi, veriden öğrenen ve tahminlerde bulunan algoritmaların geliştirilmesidir.\"},\n",
    "                {\"text\": \"Derin öğrenme, yapay sinir ağlarını kullanarak karmaşık problemleri çözen bir yöntemdir.\"},\n",
    "                {\"text\": \"Veri bilimi, büyük veri setlerinden anlamlı bilgiler çıkarma sürecidir.\"},\n",
    "                {\"text\": \"Türkiye'de teknoloji eğitimi giderek daha önemli hale gelmektedir.\"},\n",
    "                {\"text\": \"Yazılım geliştirme, problem çözme ve yaratıcılık gerektiren bir süreçtir.\"},\n",
    "                {\"text\": \"Bulut bilişim, internet üzerinden bilgi işlem hizmetlerinin sunulmasıdır.\"},\n",
    "                {\"text\": \"Siber güvenlik, dijital sistemleri kötü niyetli saldırılardan koruma bilimidir.\"},\n",
    "                {\"text\": \"Mobil uygulama geliştirme, akıllı telefonlar için yazılım oluşturma sürecidir.\"},\n",
    "                {\"text\": \"Web teknolojileri sürekli gelişmekte ve yeni framework'ler ortaya çıkmaktadır.\"},\n",
    "            ]\n",
    "            \n",
    "            # Expand dataset\n",
    "            expanded_samples = samples * 100  # Create 1000 samples\n",
    "            \n",
    "            # Create dataset\n",
    "            dataset = Dataset.from_list(expanded_samples)\n",
    "            \n",
    "            # Split into train/test\n",
    "            split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "            self.dataset = DatasetDict({\n",
    "                'train': split['train'],\n",
    "                'test': split['test']\n",
    "            })\n",
    "            \n",
    "            logger.info(f\"✅ Fallback dataset created: {len(self.dataset['train'])} train, {len(self.dataset['test'])} test\")\n",
    "            return self.dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Failed to create fallback dataset: {e}\")\n",
    "            raise RuntimeError(\"Cannot create dataset\")\n",
    "    \n",
    "    def prepare_dataset_for_training(\n",
    "        self,\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "        use_cache: bool = True\n",
    "    ) -> DatasetDict:\n",
    "        \"\"\"Tokenize dataset with caching\"\"\"\n",
    "        \n",
    "        cache_key = f\"{tokenizer.__class__.__name__}_{max_length}\"\n",
    "        cache_file = self.cache_dir / f\"tokenized_{cache_key}.pkl\"\n",
    "        \n",
    "        if use_cache and cache_file.exists():\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    self.tokenized_dataset = pickle.load(f)\n",
    "                logger.info(f\"✅ Tokenized dataset loaded from cache\")\n",
    "                return self.tokenized_dataset\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load tokenized cache: {e}\")\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            \"\"\"Tokenize examples with error handling\"\"\"\n",
    "            try:\n",
    "                return tokenizer(\n",
    "                    examples['text'],\n",
    "                    truncation=True,\n",
    "                    padding='max_length',\n",
    "                    max_length=max_length,\n",
    "                    return_tensors=None\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Tokenization error: {e}\")\n",
    "                # Return empty tokens as fallback\n",
    "                return {\n",
    "                    'input_ids': [[0] * max_length] * len(examples['text']),\n",
    "                    'attention_mask': [[0] * max_length] * len(examples['text'])\n",
    "                }\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"Tokenizing dataset...\")\n",
    "            self.tokenized_dataset = self.dataset.map(\n",
    "                tokenize_function,\n",
    "                batched=True,\n",
    "                num_proc=4 if not ENV_INFO['is_colab'] else 2,\n",
    "                remove_columns=self.dataset['train'].column_names,\n",
    "                desc=\"Tokenizing\"\n",
    "            )\n",
    "            \n",
    "            # Cache tokenized dataset\n",
    "            if use_cache:\n",
    "                with open(cache_file, 'wb') as f:\n",
    "                    pickle.dump(self.tokenized_dataset, f)\n",
    "            \n",
    "            logger.info(\"✅ Dataset tokenized successfully\")\n",
    "            return self.tokenized_dataset\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"❌ Tokenization failed: {e}\")\n",
    "            raise\n",
    "\n",
    "# Initialize Data Manager\n",
    "data_manager = DataManager()\n",
    "\n",
    "# Load dataset with automatic fallback\n",
    "dataset = data_manager.load_dataset_with_fallback(\n",
    "    dataset_name=\"Huseyin/turkish-200k-dataset\",\n",
    "    max_samples=10000 if gpu_manager.gpu_info.get('memory_total', 0) < 20 else None\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {dataset}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 5. Model Configuration with Auto-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Production-ready training configuration with auto-tuning\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"Qwen/Qwen2.5-7B\"  # Using stable version\n",
    "    teacher_model_name: Optional[str] = \"TURKCELL/Turkcell-LLM-7b-v1\"\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # Batch settings (will be auto-tuned)\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Optimization settings\n",
    "    use_lora: bool = True\n",
    "    lora_rank: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # Quantization\n",
    "    use_4bit: bool = True\n",
    "    use_8bit: bool = False\n",
    "    bnb_4bit_compute_dtype: str = \"float16\"\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing: bool = True\n",
    "    optim: str = \"paged_adamw_32bit\"\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16: bool = False\n",
    "    bf16: bool = False\n",
    "    tf32: bool = True\n",
    "    \n",
    "    # Knowledge Distillation\n",
    "    use_distillation: bool = False  # Disabled by default for stability\n",
    "    distillation_temperature: float = 4.0\n",
    "    distillation_alpha: float = 0.7\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 100\n",
    "    save_total_limit: int = 2\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"./checkpoints\"\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Auto-tune configuration based on hardware\"\"\"\n",
    "        self._auto_tune_for_hardware()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _auto_tune_for_hardware(self):\n",
    "        \"\"\"Automatically adjust settings based on available hardware\"\"\"\n",
    "        if not gpu_manager.has_gpu:\n",
    "            logger.warning(\"No GPU detected, using CPU settings\")\n",
    "            self.batch_size = 1\n",
    "            self.gradient_accumulation_steps = 16\n",
    "            self.max_length = 128\n",
    "            self.use_4bit = False\n",
    "            self.gradient_checkpointing = False\n",
    "            return\n",
    "        \n",
    "        gpu_memory = gpu_manager.gpu_info.get('memory_total', 16)\n",
    "        gpu_type = gpu_manager.gpu_info.get('gpu_type', 'generic')\n",
    "        \n",
    "        logger.info(f\"Auto-tuning for {gpu_type} GPU with {gpu_memory:.1f}GB memory\")\n",
    "        \n",
    "        # T4 GPU (16GB) - Colab Free\n",
    "        if gpu_type == 't4' or gpu_memory < 20:\n",
    "            self.batch_size = 1\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.max_length = 256\n",
    "            self.lora_rank = 16\n",
    "            self.lora_alpha = 32\n",
    "            self.use_distillation = False  # Disable for memory\n",
    "            self.fp16 = True\n",
    "            self.bf16 = False\n",
    "            logger.info(\"Configured for T4/low-memory GPU\")\n",
    "        \n",
    "        # V100 GPU (16-32GB)\n",
    "        elif gpu_type == 'v100' or gpu_memory < 40:\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.max_length = 384\n",
    "            self.lora_rank = 32\n",
    "            self.lora_alpha = 64\n",
    "            self.fp16 = True\n",
    "            self.bf16 = False\n",
    "            logger.info(\"Configured for V100/mid-range GPU\")\n",
    "        \n",
    "        # A100 GPU (40-80GB) or high-end consumer GPUs\n",
    "        else:\n",
    "            self.batch_size = 4\n",
    "            self.gradient_accumulation_steps = 2\n",
    "            self.max_length = 512\n",
    "            self.lora_rank = 64\n",
    "            self.lora_alpha = 128\n",
    "            self.bf16 = gpu_manager.gpu_info.get('supports_bf16', False)\n",
    "            self.fp16 = not self.bf16\n",
    "            logger.info(\"Configured for A100/high-end GPU\")\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate configuration for consistency\"\"\"\n",
    "        # Ensure only one quantization method is used\n",
    "        if self.use_4bit and self.use_8bit:\n",
    "            logger.warning(\"Both 4-bit and 8-bit quantization enabled, using 4-bit only\")\n",
    "            self.use_8bit = False\n",
    "        \n",
    "        # Ensure only one precision is used\n",
    "        if self.fp16 and self.bf16:\n",
    "            logger.warning(\"Both fp16 and bf16 enabled, using bf16 if supported\")\n",
    "            if gpu_manager.gpu_info.get('supports_bf16', False):\n",
    "                self.fp16 = False\n",
    "            else:\n",
    "                self.bf16 = False\n",
    "        \n",
    "        # Adjust output directory\n",
    "        self.output_dir = str(storage_manager.get_path('checkpoints'))\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    def save(self, filename: str = \"training_config.json\"):\n",
    "        \"\"\"Save configuration to file\"\"\"\n",
    "        config_path = storage_manager.get_path(filename)\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "        logger.info(f\"✅ Config saved to {config_path}\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filename: str = \"training_config.json\"):\n",
    "        \"\"\"Load configuration from file\"\"\"\n",
    "        config_path = storage_manager.get_path(filename)\n",
    "        if config_path.exists():\n",
    "            with open(config_path, 'r') as f:\n",
    "                config_dict = json.load(f)\n",
    "            return cls(**config_dict)\n",
    "        return cls()\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrainingConfig()\n",
    "config.save()\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(config.to_dict(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔤 6. Tokenizer Setup with Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import tiktoken\n",
    "\n",
    "class TokenizerManager:\n",
    "    \"\"\"Manages tokenizer with multiple fallback options\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.tokenizer_type = None\n",
    "        self._initialize_tokenizer()\n",
    "    \n",
    "    def _initialize_tokenizer(self):\n",
    "        \"\"\"Initialize tokenizer with fallback options\"\"\"\n",
    "        \n",
    "        # Try loading model-specific tokenizer\n",
    "        try:\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            self.tokenizer_type = \"model_specific\"\n",
    "            logger.info(f\"✅ Loaded {self.model_name} tokenizer\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load model tokenizer: {e}\")\n",
    "            self._try_fallback_tokenizers()\n",
    "        \n",
    "        # Ensure special tokens are set\n",
    "        self._setup_special_tokens()\n",
    "    \n",
    "    def _try_fallback_tokenizers(self):\n",
    "        \"\"\"Try fallback tokenizer options\"\"\"\n",
    "        \n",
    "        # Try tiktoken as fallback\n",
    "        try:\n",
    "            logger.info(\"Trying tiktoken as fallback...\")\n",
    "            self.tokenizer = self._create_tiktoken_wrapper()\n",
    "            self.tokenizer_type = \"tiktoken\"\n",
    "            logger.info(\"✅ Using tiktoken tokenizer\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Tiktoken fallback failed: {e}\")\n",
    "        \n",
    "        # Try generic GPT2 tokenizer as last resort\n",
    "        try:\n",
    "            logger.info(\"Trying GPT2 tokenizer as final fallback...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "            self.tokenizer_type = \"gpt2\"\n",
    "            logger.info(\"✅ Using GPT2 tokenizer\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"All tokenizer options failed: {e}\")\n",
    "            raise RuntimeError(\"Cannot initialize tokenizer\")\n",
    "    \n",
    "    def _create_tiktoken_wrapper(self):\n",
    "        \"\"\"Create a wrapper for tiktoken to work with transformers\"\"\"\n",
    "        \n",
    "        class TiktokenWrapper:\n",
    "            def __init__(self):\n",
    "                self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "                self.pad_token_id = 100257\n",
    "                self.eos_token_id = 100257\n",
    "                self.bos_token_id = 100258\n",
    "                self.unk_token_id = 100259\n",
    "                self.model_max_length = 8192\n",
    "            \n",
    "            def encode(self, text, **kwargs):\n",
    "                return self.encoder.encode(text)\n",
    "            \n",
    "            def decode(self, ids, **kwargs):\n",
    "                return self.encoder.decode(ids)\n",
    "            \n",
    "            def __call__(self, text, **kwargs):\n",
    "                if isinstance(text, str):\n",
    "                    text = [text]\n",
    "                \n",
    "                max_length = kwargs.get('max_length', 512)\n",
    "                padding = kwargs.get('padding', False)\n",
    "                truncation = kwargs.get('truncation', False)\n",
    "                \n",
    "                encoded = []\n",
    "                attention_masks = []\n",
    "                \n",
    "                for t in text:\n",
    "                    ids = self.encode(t)\n",
    "                    \n",
    "                    if truncation and len(ids) > max_length:\n",
    "                        ids = ids[:max_length]\n",
    "                    \n",
    "                    attention_mask = [1] * len(ids)\n",
    "                    \n",
    "                    if padding == 'max_length':\n",
    "                        pad_length = max_length - len(ids)\n",
    "                        ids = ids + [self.pad_token_id] * pad_length\n",
    "                        attention_mask = attention_mask + [0] * pad_length\n",
    "                    \n",
    "                    encoded.append(ids)\n",
    "                    attention_masks.append(attention_mask)\n",
    "                \n",
    "                return {\n",
    "                    'input_ids': encoded,\n",
    "                    'attention_mask': attention_masks\n",
    "                }\n",
    "        \n",
    "        return TiktokenWrapper()\n",
    "    \n",
    "    def _setup_special_tokens(self):\n",
    "        \"\"\"Setup special tokens for the tokenizer\"\"\"\n",
    "        if self.tokenizer_type == \"tiktoken\":\n",
    "            return  # Already set in wrapper\n",
    "        \n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token or \"<pad>\"\n",
    "        \n",
    "        if not self.tokenizer.eos_token:\n",
    "            self.tokenizer.eos_token = \"</s>\"\n",
    "        \n",
    "        if not self.tokenizer.bos_token:\n",
    "            self.tokenizer.bos_token = \"<s>\"\n",
    "        \n",
    "        logger.info(\"✅ Special tokens configured\")\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        \"\"\"Get the initialized tokenizer\"\"\"\n",
    "        return self.tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer_manager = TokenizerManager(config.model_name)\n",
    "tokenizer = tokenizer_manager.get_tokenizer()\n",
    "\n",
    "print(f\"Tokenizer initialized: {tokenizer_manager.tokenizer_type}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"Merhaba, bu bir test metnidir.\"\n",
    "encoded = tokenizer(test_text, truncation=True, padding='max_length', max_length=32)\n",
    "print(f\"Test encoding: {encoded['input_ids'][:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 7. Model Loading with Comprehensive Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelManager:\n",
    "    \"\"\"Manages model loading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.peft_config = None\n",
    "        self.bnb_config = None\n",
    "        self._setup_quantization()\n",
    "    \n",
    "    def _setup_quantization(self):\n",
    "        \"\"\"Setup quantization configuration\"\"\"\n",
    "        if not self.config.use_4bit and not self.config.use_8bit:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            compute_dtype = getattr(torch, self.config.bnb_4bit_compute_dtype)\n",
    "            \n",
    "            self.bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=self.config.use_4bit,\n",
    "                load_in_8bit=self.config.use_8bit,\n",
    "                bnb_4bit_compute_dtype=compute_dtype,\n",
    "                bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "            )\n",
    "            logger.info(\"✅ Quantization configured\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to setup quantization: {e}\")\n",
    "            self.bnb_config = None\n",
    "    \n",
    "    def load_model(self, model_name: Optional[str] = None) -> nn.Module:\n",
    "        \"\"\"Load model with multiple fallback options\"\"\"\n",
    "        model_name = model_name or self.config.model_name\n",
    "        \n",
    "        # Try loading with quantization\n",
    "        if self.bnb_config:\n",
    "            try:\n",
    "                logger.info(f\"Loading {model_name} with quantization...\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    quantization_config=self.bnb_config,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    torch_dtype=torch.float16,\n",
    "                )\n",
    "                logger.info(\"✅ Model loaded with quantization\")\n",
    "                return self._setup_peft_model()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load with quantization: {e}\")\n",
    "                gpu_manager.clear_memory()\n",
    "        \n",
    "        # Try loading without quantization\n",
    "        try:\n",
    "            logger.info(f\"Loading {model_name} without quantization...\")\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"auto\" if gpu_manager.has_gpu else \"cpu\",\n",
    "                trust_remote_code=True,\n",
    "                torch_dtype=torch.float16 if gpu_manager.has_gpu else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "            logger.info(\"✅ Model loaded without quantization\")\n",
    "            return self._setup_peft_model()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            return self._load_fallback_model()\n",
    "    \n",
    "    def _load_fallback_model(self) -> nn.Module:\n",
    "        \"\"\"Load a smaller fallback model\"\"\"\n",
    "        fallback_models = [\n",
    "            \"microsoft/phi-2\",  # 2.7B parameters\n",
    "            \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",  # 1.1B parameters\n",
    "            \"gpt2\",  # 124M parameters\n",
    "        ]\n",
    "        \n",
    "        for model_name in fallback_models:\n",
    "            try:\n",
    "                logger.info(f\"Trying fallback model: {model_name}\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    device_map=\"auto\" if gpu_manager.has_gpu else \"cpu\",\n",
    "                    torch_dtype=torch.float16 if gpu_manager.has_gpu else torch.float32,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                )\n",
    "                logger.info(f\"✅ Fallback model loaded: {model_name}\")\n",
    "                \n",
    "                # Update config\n",
    "                self.config.model_name = model_name\n",
    "                self.config.use_lora = True  # Force LoRA for small models\n",
    "                \n",
    "                return self._setup_peft_model()\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load {model_name}: {e}\")\n",
    "                gpu_manager.clear_memory()\n",
    "        \n",
    "        raise RuntimeError(\"Cannot load any model\")\n",
    "    \n",
    "    def _setup_peft_model(self) -> nn.Module:\n",
    "        \"\"\"Setup PEFT (LoRA) for the model\"\"\"\n",
    "        if not self.config.use_lora:\n",
    "            return self.model\n",
    "        \n",
    "        try:\n",
    "            # Prepare model for training\n",
    "            if self.config.use_4bit or self.config.use_8bit:\n",
    "                self.model = prepare_model_for_kbit_training(\n",
    "                    self.model,\n",
    "                    use_gradient_checkpointing=self.config.gradient_checkpointing\n",
    "                )\n",
    "            \n",
    "            # Configure LoRA\n",
    "            self.peft_config = LoraConfig(\n",
    "                r=self.config.lora_rank,\n",
    "                lora_alpha=self.config.lora_alpha,\n",
    "                lora_dropout=self.config.lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            )\n",
    "            \n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(self.model, self.peft_config)\n",
    "            self.model.print_trainable_parameters()\n",
    "            \n",
    "            logger.info(\"✅ LoRA configured successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to setup LoRA: {e}\")\n",
    "        \n",
    "        # Enable gradient checkpointing if requested\n",
    "        if self.config.gradient_checkpointing:\n",
    "            try:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                logger.info(\"✅ Gradient checkpointing enabled\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to enable gradient checkpointing: {e}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def get_model(self) -> nn.Module:\n",
    "        \"\"\"Get the loaded model\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        return self.model\n",
    "\n",
    "# Initialize model manager and load model\n",
    "model_manager = ModelManager(config)\n",
    "model = model_manager.load_model()\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 8. Training Setup with Error Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages training with comprehensive error handling and recovery\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        config: TrainingConfig,\n",
    "        dataset: DatasetDict\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.dataset = dataset\n",
    "        self.trainer = None\n",
    "        self.training_args = None\n",
    "    \n",
    "    def setup_training_arguments(self) -> TrainingArguments:\n",
    "        \"\"\"Setup training arguments with error handling\"\"\"\n",
    "        try:\n",
    "            self.training_args = TrainingArguments(\n",
    "                output_dir=self.config.output_dir,\n",
    "                num_train_epochs=self.config.num_epochs,\n",
    "                per_device_train_batch_size=self.config.batch_size,\n",
    "                per_device_eval_batch_size=self.config.batch_size,\n",
    "                gradient_accumulation_steps=self.config.gradient_accumulation_steps,\n",
    "                learning_rate=self.config.learning_rate,\n",
    "                weight_decay=self.config.weight_decay,\n",
    "                warmup_ratio=self.config.warmup_ratio,\n",
    "                \n",
    "                # Optimization\n",
    "                optim=self.config.optim,\n",
    "                gradient_checkpointing=self.config.gradient_checkpointing,\n",
    "                \n",
    "                # Mixed precision\n",
    "                fp16=self.config.fp16,\n",
    "                bf16=self.config.bf16,\n",
    "                tf32=self.config.tf32,\n",
    "                \n",
    "                # Logging\n",
    "                logging_steps=self.config.logging_steps,\n",
    "                save_steps=self.config.save_steps,\n",
    "                eval_steps=self.config.eval_steps,\n",
    "                evaluation_strategy=\"steps\",\n",
    "                save_strategy=\"steps\",\n",
    "                \n",
    "                # Saving\n",
    "                save_total_limit=self.config.save_total_limit,\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_loss\",\n",
    "                greater_is_better=False,\n",
    "                \n",
    "                # Others\n",
    "                report_to=[\"tensorboard\"] if ENV_INFO['is_colab'] else [\"none\"],\n",
    "                push_to_hub=False,\n",
    "                resume_from_checkpoint=self.config.resume_from_checkpoint,\n",
    "                \n",
    "                # Memory optimization\n",
    "                gradient_checkpointing_kwargs={'use_reentrant': False} if self.config.gradient_checkpointing else None,\n",
    "                dataloader_num_workers=2,\n",
    "                remove_unused_columns=False,\n",
    "            )\n",
    "            \n",
    "            logger.info(\"✅ Training arguments configured\")\n",
    "            return self.training_args\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup training arguments: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_trainer(self):\n",
    "        \"\"\"Setup trainer with error handling\"\"\"\n",
    "        try:\n",
    "            # Prepare datasets\n",
    "            train_dataset = self.dataset['train']\n",
    "            eval_dataset = self.dataset['test']\n",
    "            \n",
    "            # Setup data collator\n",
    "            data_collator = DataCollatorForLanguageModeling(\n",
    "                tokenizer=self.tokenizer,\n",
    "                mlm=False,\n",
    "                pad_to_multiple_of=8\n",
    "            )\n",
    "            \n",
    "            # Create trainer\n",
    "            self.trainer = SFTTrainer(\n",
    "                model=self.model,\n",
    "                args=self.training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=eval_dataset,\n",
    "                tokenizer=self.tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                dataset_text_field=\"text\",\n",
    "                max_seq_length=self.config.max_length,\n",
    "                packing=False,\n",
    "            )\n",
    "            \n",
    "            logger.info(\"✅ Trainer configured\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup trainer: {e}\")\n",
    "            # Try simpler Trainer as fallback\n",
    "            self._setup_simple_trainer()\n",
    "    \n",
    "    def _setup_simple_trainer(self):\n",
    "        \"\"\"Setup a simpler trainer as fallback\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Setting up simple trainer as fallback...\")\n",
    "            \n",
    "            # Tokenize datasets if not already done\n",
    "            tokenized_dataset = data_manager.prepare_dataset_for_training(\n",
    "                self.tokenizer,\n",
    "                max_length=self.config.max_length\n",
    "            )\n",
    "            \n",
    "            self.trainer = Trainer(\n",
    "                model=self.model,\n",
    "                args=self.training_args,\n",
    "                train_dataset=tokenized_dataset['train'],\n",
    "                eval_dataset=tokenized_dataset['test'],\n",
    "                tokenizer=self.tokenizer,\n",
    "            )\n",
    "            \n",
    "            logger.info(\"✅ Simple trainer configured\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup simple trainer: {e}\")\n",
    "            raise RuntimeError(\"Cannot setup trainer\")\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Start training with error recovery\"\"\"\n",
    "        if self.trainer is None:\n",
    "            self.setup_training_arguments()\n",
    "            self.setup_trainer()\n",
    "        \n",
    "        try:\n",
    "            logger.info(\"🚀 Starting training...\")\n",
    "            \n",
    "            # Setup callbacks\n",
    "            self._setup_callbacks()\n",
    "            \n",
    "            # Start training\n",
    "            train_result = self.trainer.train(\n",
    "                resume_from_checkpoint=self.config.resume_from_checkpoint\n",
    "            )\n",
    "            \n",
    "            # Save final model\n",
    "            self.save_model(\"final_model\")\n",
    "            \n",
    "            logger.info(\"✅ Training completed successfully\")\n",
    "            return train_result\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Training interrupted by user\")\n",
    "            self.save_model(\"interrupted_checkpoint\")\n",
    "            raise\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Training failed: {e}\")\n",
    "            self.save_model(\"error_checkpoint\")\n",
    "            raise\n",
    "    \n",
    "    def _setup_callbacks(self):\n",
    "        \"\"\"Setup training callbacks\"\"\"\n",
    "        from transformers import TrainerCallback\n",
    "        \n",
    "        class MemoryCallback(TrainerCallback):\n",
    "            \"\"\"Callback to monitor and manage memory\"\"\"\n",
    "            \n",
    "            def on_step_end(self, args, state, control, **kwargs):\n",
    "                if state.global_step % 50 == 0:\n",
    "                    memory_usage = gpu_manager.get_memory_usage()\n",
    "                    logger.info(f\"Step {state.global_step}: Memory usage: {memory_usage}\")\n",
    "                    \n",
    "                    # Clear cache if memory usage is high\n",
    "                    if gpu_manager.has_gpu:\n",
    "                        gpu_free = memory_usage.get('gpu_free', float('inf'))\n",
    "                        if gpu_free < 2.0:  # Less than 2GB free\n",
    "                            gpu_manager.clear_memory()\n",
    "                            logger.warning(\"Cleared GPU cache due to low memory\")\n",
    "        \n",
    "        self.trainer.add_callback(MemoryCallback())\n",
    "    \n",
    "    def save_model(self, name: str = \"checkpoint\"):\n",
    "        \"\"\"Save model with error handling\"\"\"\n",
    "        try:\n",
    "            save_path = storage_manager.get_path(f\"models/{name}\")\n",
    "            self.trainer.save_model(str(save_path))\n",
    "            logger.info(f\"✅ Model saved to {save_path}\")\n",
    "            \n",
    "            # Save to drive if available\n",
    "            if storage_manager.use_drive:\n",
    "                drive_path = storage_manager.drive_path / f\"models/{name}\"\n",
    "                self.trainer.save_model(str(drive_path))\n",
    "                logger.info(f\"✅ Model backed up to Drive\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save model: {e}\")\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Evaluating model...\")\n",
    "            eval_results = self.trainer.evaluate()\n",
    "            logger.info(f\"Evaluation results: {eval_results}\")\n",
    "            return eval_results\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Evaluation failed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Prepare tokenized dataset\n",
    "logger.info(\"Preparing dataset for training...\")\n",
    "tokenized_dataset = data_manager.prepare_dataset_for_training(\n",
    "    tokenizer,\n",
    "    max_length=config.max_length\n",
    ")\n",
    "\n",
    "# Initialize training manager\n",
    "training_manager = TrainingManager(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# Setup training\n",
    "training_manager.setup_training_arguments()\n",
    "training_manager.setup_trainer()\n",
    "\n",
    "print(\"✅ Training setup complete!\")\n",
    "print(f\"Train dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Eval dataset size: {len(tokenized_dataset['test'])}\")\n",
    "print(f\"Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 9. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training with comprehensive error handling\n",
    "try:\n",
    "    logger.info(\"=\"*50)\n",
    "    logger.info(\"🚀 STARTING TRAINING\")\n",
    "    logger.info(\"=\"*50)\n",
    "    \n",
    "    # Display current configuration\n",
    "    print(\"\\nTraining Configuration:\")\n",
    "    print(f\"  Model: {config.model_name}\")\n",
    "    print(f\"  Epochs: {config.num_epochs}\")\n",
    "    print(f\"  Batch Size: {config.batch_size}\")\n",
    "    print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "    print(f\"  Max Length: {config.max_length}\")\n",
    "    print(f\"  LoRA: {'Enabled' if config.use_lora else 'Disabled'}\")\n",
    "    print(f\"  Quantization: {'4-bit' if config.use_4bit else '8-bit' if config.use_8bit else 'None'}\")\n",
    "    print(f\"  Mixed Precision: {'bf16' if config.bf16 else 'fp16' if config.fp16 else 'fp32'}\")\n",
    "    print()\n",
    "    \n",
    "    # Check memory before training\n",
    "    memory_before = gpu_manager.get_memory_usage()\n",
    "    print(f\"Memory usage before training: {memory_before}\")\n",
    "    \n",
    "    # Start training\n",
    "    train_result = training_manager.train()\n",
    "    \n",
    "    # Display training results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"✅ TRAINING COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Training Loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"Training Runtime: {train_result.metrics['train_runtime']:.2f} seconds\")\n",
    "    print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    eval_results = training_manager.evaluate()\n",
    "    if eval_results:\n",
    "        print(f\"\\nEvaluation Loss: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "    \n",
    "    # Save final model\n",
    "    training_manager.save_model(\"final_model\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"\\n⚠️ Training interrupted by user\")\n",
    "    print(\"Saving checkpoint...\")\n",
    "    training_manager.save_model(\"interrupted_checkpoint\")\n",
    "    print(\"Checkpoint saved. You can resume training by setting config.resume_from_checkpoint\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"\\n❌ Training failed with error: {e}\")\n",
    "    print(\"Attempting to save emergency checkpoint...\")\n",
    "    try:\n",
    "        training_manager.save_model(\"emergency_checkpoint\")\n",
    "        print(\"Emergency checkpoint saved\")\n",
    "    except:\n",
    "        print(\"Failed to save emergency checkpoint\")\n",
    "    \n",
    "    # Clear GPU memory\n",
    "    gpu_manager.clear_memory()\n",
    "    \n",
    "    # Print debugging information\n",
    "    print(\"\\nDebugging Information:\")\n",
    "    print(f\"Error Type: {type(e).__name__}\")\n",
    "    print(f\"Error Message: {str(e)}\")\n",
    "    print(f\"Memory Usage: {gpu_manager.get_memory_usage()}\")\n",
    "    \n",
    "finally:\n",
    "    # Cleanup\n",
    "    gpu_manager.clear_memory()\n",
    "    print(\"\\n🔧 Cleanup completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 10. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    try:\n",
    "        # Encode the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Move to device\n",
    "        if gpu_manager.has_gpu:\n",
    "            inputs = {k: v.to(gpu_manager.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generation failed: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"Python programlama dili\",\n",
    "    \"Makine öğrenmesi nedir?\",\n",
    "    \"Türkiye'de teknoloji\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🧪 TESTING TRAINED MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 30)\n",
    "    generated = generate_text(prompt, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 11. Training Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📊 TRAINING SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "summary = {\n",
    "    \"Environment\": {\n",
    "        \"Platform\": ENV_INFO['platform'],\n",
    "        \"Is Colab\": ENV_INFO['is_colab'],\n",
    "        \"GPU Available\": gpu_manager.has_gpu,\n",
    "        \"GPU Name\": gpu_manager.gpu_info.get('name', 'N/A'),\n",
    "        \"GPU Memory\": f\"{gpu_manager.gpu_info.get('memory_total', 0):.1f}GB\",\n",
    "    },\n",
    "    \"Model\": {\n",
    "        \"Name\": config.model_name,\n",
    "        \"Parameters\": f\"{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B\",\n",
    "        \"Trainable Parameters\": f\"{sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\",\n",
    "        \"LoRA Enabled\": config.use_lora,\n",
    "        \"Quantization\": \"4-bit\" if config.use_4bit else \"8-bit\" if config.use_8bit else \"None\",\n",
    "    },\n",
    "    \"Dataset\": {\n",
    "        \"Train Size\": len(tokenized_dataset['train']),\n",
    "        \"Test Size\": len(tokenized_dataset['test']),\n",
    "        \"Max Length\": config.max_length,\n",
    "    },\n",
    "    \"Training\": {\n",
    "        \"Epochs\": config.num_epochs,\n",
    "        \"Batch Size\": config.batch_size,\n",
    "        \"Gradient Accumulation\": config.gradient_accumulation_steps,\n",
    "        \"Effective Batch Size\": config.batch_size * config.gradient_accumulation_steps,\n",
    "        \"Learning Rate\": config.learning_rate,\n",
    "        \"Mixed Precision\": \"bf16\" if config.bf16 else \"fp16\" if config.fp16 else \"fp32\",\n",
    "    },\n",
    "    \"Output\": {\n",
    "        \"Checkpoint Directory\": config.output_dir,\n",
    "        \"Final Model Path\": str(storage_manager.get_path(\"models/final_model\")),\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, items in summary.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for key, value in items.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ NEXT STEPS\")\n",
    "print(\"=\"*50)\n",
    "print(\"\"\"\n",
    "1. 📥 Download the trained model:\n",
    "   - Navigate to the 'models/final_model' directory\n",
    "   - Download all files for local use\n",
    "\n",
    "2. 🔄 Resume training:\n",
    "   - Set config.resume_from_checkpoint = './checkpoints/checkpoint-XXX'\n",
    "   - Re-run the training cell\n",
    "\n",
    "3. 🎯 Fine-tune further:\n",
    "   - Adjust hyperparameters in TrainingConfig\n",
    "   - Load a different dataset\n",
    "   - Try different model architectures\n",
    "\n",
    "4. 🚀 Deploy the model:\n",
    "   - Use the model for inference\n",
    "   - Create an API endpoint\n",
    "   - Share on Hugging Face Hub\n",
    "\n",
    "5. 📊 Analyze results:\n",
    "   - Check tensorboard logs (if in Colab)\n",
    "   - Evaluate on test datasets\n",
    "   - Compare with baseline models\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🎉 Congratulations! Your model has been successfully trained and is ready for use!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
