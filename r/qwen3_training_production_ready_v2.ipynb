{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ QWEN3-8B Turkish Training - Production Ready v2.0\n",
    "## Advanced Optimizations with Dependency Injection & Turkish Tokenizer\n",
    "\n",
    "### âš¡ Optimization Features:\n",
    "- âœ… **Accuracy**: Mixed Precision, EMA, Label Smoothing, Curriculum Learning, Knowledge Distillation\n",
    "- âœ… **Speed**: Flash Attention, Dynamic Padding, Compiled Mode, Efficient Data Loading\n",
    "- âœ… **Reliability**: Gradient Clipping, Auto Recovery, Health Monitoring, Adaptive Batch\n",
    "- âœ… **Memory**: Gradient Checkpointing, 8-bit Optimizer, CPU Offloading, Teacher Caching\n",
    "\n",
    "### ðŸ”§ Key Improvements:\n",
    "- Dependency Injection Architecture\n",
    "- Turkish Tokenizer Integration\n",
    "- TURKCELL Teacher Model for Knowledge Distillation\n",
    "- 100% Google Colab Pro+ A100 Compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ 1. Environment Setup & Dependency Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Detection and Setup with Dependency Injection\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import subprocess\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union, Protocol\n",
    "from dataclasses import dataclass, field\n",
    "from abc import ABC, abstractmethod\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure comprehensive logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('training.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Dependency Injection Container\n",
    "class DIContainer:\n",
    "    \"\"\"Dependency Injection Container for managing instances\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._services = {}\n",
    "        self._singletons = {}\n",
    "    \n",
    "    def register(self, name: str, factory, singleton: bool = True):\n",
    "        \"\"\"Register a service factory\"\"\"\n",
    "        self._services[name] = (factory, singleton)\n",
    "    \n",
    "    def get(self, name: str):\n",
    "        \"\"\"Get a service instance\"\"\"\n",
    "        if name not in self._services:\n",
    "            raise ValueError(f\"Service '{name}' not registered\")\n",
    "        \n",
    "        factory, singleton = self._services[name]\n",
    "        \n",
    "        if singleton:\n",
    "            if name not in self._singletons:\n",
    "                self._singletons[name] = factory()\n",
    "            return self._singletons[name]\n",
    "        \n",
    "        return factory()\n",
    "\n",
    "# Initialize DI Container\n",
    "container = DIContainer()\n",
    "\n",
    "# Environment Manager Interface\n",
    "class IEnvironmentManager(ABC):\n",
    "    @abstractmethod\n",
    "    def detect_environment(self) -> Dict[str, Any]:\n",
    "        pass\n",
    "\n",
    "class EnvironmentManager(IEnvironmentManager):\n",
    "    \"\"\"Manages environment detection and setup\"\"\"\n",
    "    \n",
    "    def detect_environment(self) -> Dict[str, Any]:\n",
    "        \"\"\"Detect current environment (Colab, Local, etc.)\"\"\"\n",
    "        env_info = {\n",
    "            'platform': platform.system(),\n",
    "            'python_version': sys.version,\n",
    "            'is_colab': False,\n",
    "            'is_kaggle': False,\n",
    "            'has_gpu': False,\n",
    "            'gpu_info': None\n",
    "        }\n",
    "        \n",
    "        # Check if running in Google Colab\n",
    "        try:\n",
    "            import google.colab\n",
    "            env_info['is_colab'] = True\n",
    "            logger.info(\"âœ… Running in Google Colab\")\n",
    "        except ImportError:\n",
    "            pass\n",
    "        \n",
    "        # Check if running in Kaggle\n",
    "        if os.path.exists('/kaggle'):\n",
    "            env_info['is_kaggle'] = True\n",
    "            logger.info(\"âœ… Running in Kaggle\")\n",
    "        \n",
    "        # Check GPU availability\n",
    "        try:\n",
    "            import torch\n",
    "            if torch.cuda.is_available():\n",
    "                env_info['has_gpu'] = True\n",
    "                env_info['gpu_info'] = {\n",
    "                    'name': torch.cuda.get_device_name(0),\n",
    "                    'memory': torch.cuda.get_device_properties(0).total_memory / 1e9,\n",
    "                    'capability': torch.cuda.get_device_capability(0)\n",
    "                }\n",
    "                logger.info(f\"âœ… GPU detected: {env_info['gpu_info']['name']} ({env_info['gpu_info']['memory']:.1f}GB)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"âŒ GPU detection failed: {e}\")\n",
    "        \n",
    "        return env_info\n",
    "\n",
    "# Register EnvironmentManager\n",
    "container.register('environment', EnvironmentManager)\n",
    "\n",
    "# Detect environment\n",
    "env_manager = container.get('environment')\n",
    "ENV_INFO = env_manager.detect_environment()\n",
    "print(f\"Environment: {ENV_INFO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages with error handling\n",
    "def install_package(package: str, upgrade: bool = False) -> bool:\n",
    "    \"\"\"Install a package with error handling\"\"\"\n",
    "    try:\n",
    "        cmd = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
    "        if upgrade:\n",
    "            cmd.append(\"--upgrade\")\n",
    "        cmd.append(package)\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)\n",
    "        if result.returncode == 0:\n",
    "            logger.info(f\"âœ… Successfully installed {package}\")\n",
    "            return True\n",
    "        else:\n",
    "            logger.error(f\"âŒ Failed to install {package}: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error installing {package}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Core dependencies\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"transformers>=4.36.0\",\n",
    "    \"datasets>=2.14.0\",\n",
    "    \"accelerate>=0.25.0\",\n",
    "    \"peft>=0.11.1\",\n",
    "    \"bitsandbytes>=0.43.1\",\n",
    "    \"sentencepiece>=0.1.99\",\n",
    "    \"tiktoken>=0.5.0\",\n",
    "    \"trl>=0.7.0\",\n",
    "    \"psutil\",\n",
    "    \"py-cpuinfo\",\n",
    "    \"einops\",  # For Flash Attention\n",
    "    \"scipy\",  # For advanced optimizations\n",
    "]\n",
    "\n",
    "# Optional packages\n",
    "OPTIONAL_PACKAGES = [\n",
    "    \"wandb\",\n",
    "    \"tensorboard\",\n",
    "    \"flash-attn>=2.3.0\",  # Flash Attention\n",
    "]\n",
    "\n",
    "# Install packages\n",
    "print(\"Installing required packages...\")\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    if not install_package(package):\n",
    "        logger.warning(f\"Retrying installation of {package}...\")\n",
    "        install_package(package, upgrade=True)\n",
    "\n",
    "# Install optional packages\n",
    "for package in OPTIONAL_PACKAGES:\n",
    "    try:\n",
    "        install_package(package)\n",
    "    except:\n",
    "        logger.warning(f\"Optional package {package} not available\")\n",
    "\n",
    "print(\"âœ… Package installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ 2. GPU Management with Dependency Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "import psutil\n",
    "import json\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# GPU Manager Interface\n",
    "class IGPUManager(ABC):\n",
    "    @abstractmethod\n",
    "    def clear_memory(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_memory_usage(self) -> Dict[str, float]:\n",
    "        pass\n",
    "\n",
    "class GPUManager(IGPUManager):\n",
    "    \"\"\"Comprehensive GPU management with error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, env_info: Dict[str, Any]):\n",
    "        self.env_info = env_info\n",
    "        self.has_gpu = torch.cuda.is_available()\n",
    "        self.device = None\n",
    "        self.gpu_info = {}\n",
    "        self.flash_attn_available = False\n",
    "        self._initialize()\n",
    "    \n",
    "    def _initialize(self):\n",
    "        \"\"\"Initialize GPU with error handling\"\"\"\n",
    "        try:\n",
    "            if self.has_gpu:\n",
    "                self.device = torch.device(\"cuda\")\n",
    "                self.gpu_info = self._get_gpu_info()\n",
    "                self._optimize_gpu_settings()\n",
    "                self._check_flash_attention()\n",
    "                logger.info(f\"âœ… GPU initialized: {self.gpu_info['name']}\")\n",
    "            else:\n",
    "                self.device = torch.device(\"cpu\")\n",
    "                logger.warning(\"âš ï¸ No GPU detected, using CPU\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ GPU initialization failed: {e}\")\n",
    "            self.device = torch.device(\"cpu\")\n",
    "            self.has_gpu = False\n",
    "    \n",
    "    def _get_gpu_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get comprehensive GPU information\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            gpu_id = 0\n",
    "            props = torch.cuda.get_device_properties(gpu_id)\n",
    "            \n",
    "            info = {\n",
    "                'name': props.name,\n",
    "                'memory_total': props.total_memory / 1e9,\n",
    "                'memory_reserved': torch.cuda.memory_reserved(gpu_id) / 1e9,\n",
    "                'memory_allocated': torch.cuda.memory_allocated(gpu_id) / 1e9,\n",
    "                'capability': f\"{props.major}.{props.minor}\",\n",
    "                'multi_processor_count': props.multi_processor_count,\n",
    "                'supports_bf16': props.major >= 8,  # Ampere and newer\n",
    "                'supports_flash_attn': props.major >= 7 and props.minor >= 5,\n",
    "                'gpu_type': self._classify_gpu(props.name)\n",
    "            }\n",
    "            \n",
    "            return info\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to get GPU info: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def _classify_gpu(self, gpu_name: str) -> str:\n",
    "        \"\"\"Classify GPU type for optimization\"\"\"\n",
    "        gpu_name_lower = gpu_name.lower()\n",
    "        \n",
    "        if 't4' in gpu_name_lower:\n",
    "            return 't4'\n",
    "        elif 'v100' in gpu_name_lower:\n",
    "            return 'v100'\n",
    "        elif 'a100' in gpu_name_lower:\n",
    "            return 'a100'\n",
    "        elif 'a10' in gpu_name_lower:\n",
    "            return 'a10'\n",
    "        elif 'rtx 3090' in gpu_name_lower:\n",
    "            return 'rtx3090'\n",
    "        elif 'rtx 4090' in gpu_name_lower:\n",
    "            return 'rtx4090'\n",
    "        else:\n",
    "            return 'generic'\n",
    "    \n",
    "    def _optimize_gpu_settings(self):\n",
    "        \"\"\"Apply GPU-specific optimizations\"\"\"\n",
    "        if not self.has_gpu:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # Enable TF32 for better performance on Ampere GPUs\n",
    "            if self.gpu_info.get('supports_bf16', False):\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "                logger.info(\"âœ… TF32 enabled for Ampere GPU\")\n",
    "            \n",
    "            # Set memory fraction to prevent OOM\n",
    "            torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "            \n",
    "            # Enable cudnn benchmarking for better performance\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "            torch.backends.cudnn.enabled = True\n",
    "            torch.backends.cudnn.deterministic = False\n",
    "            \n",
    "            # Enable CUDA graphs for faster execution\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to apply GPU optimizations: {e}\")\n",
    "    \n",
    "    def _check_flash_attention(self):\n",
    "        \"\"\"Check if Flash Attention is available\"\"\"\n",
    "        try:\n",
    "            from flash_attn import flash_attn_func\n",
    "            self.flash_attn_available = True\n",
    "            logger.info(\"âœ… Flash Attention available\")\n",
    "        except ImportError:\n",
    "            self.flash_attn_available = False\n",
    "            logger.info(\"âš ï¸ Flash Attention not available, using standard attention\")\n",
    "    \n",
    "    def clear_memory(self):\n",
    "        \"\"\"Clear GPU memory with error handling\"\"\"\n",
    "        try:\n",
    "            if self.has_gpu:\n",
    "                torch.cuda.empty_cache()\n",
    "                torch.cuda.synchronize()\n",
    "            gc.collect()\n",
    "            logger.info(\"âœ… Memory cleared\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to clear memory: {e}\")\n",
    "    \n",
    "    def get_memory_usage(self) -> Dict[str, float]:\n",
    "        \"\"\"Get current memory usage\"\"\"\n",
    "        memory_info = {\n",
    "            'ram_used': psutil.virtual_memory().percent,\n",
    "            'ram_available': psutil.virtual_memory().available / 1e9\n",
    "        }\n",
    "        \n",
    "        if self.has_gpu:\n",
    "            try:\n",
    "                memory_info.update({\n",
    "                    'gpu_allocated': torch.cuda.memory_allocated() / 1e9,\n",
    "                    'gpu_reserved': torch.cuda.memory_reserved() / 1e9,\n",
    "                    'gpu_free': (torch.cuda.get_device_properties(0).total_memory - \n",
    "                               torch.cuda.memory_reserved()) / 1e9,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to get GPU memory usage: {e}\")\n",
    "        \n",
    "        return memory_info\n",
    "\n",
    "# Register GPU Manager\n",
    "container.register('gpu_manager', lambda: GPUManager(ENV_INFO))\n",
    "\n",
    "# Get GPU Manager instance\n",
    "gpu_manager = container.get('gpu_manager')\n",
    "print(f\"GPU Info: {json.dumps(gpu_manager.gpu_info, indent=2)}\")\n",
    "print(f\"Memory Usage: {json.dumps(gpu_manager.get_memory_usage(), indent=2)}\")\n",
    "print(f\"Flash Attention Available: {gpu_manager.flash_attn_available}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¤ 3. Turkish Tokenizer Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import struct\n",
    "from transformers import PreTrainedTokenizer\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "class TurkishTokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"Custom Turkish tokenizer using turkish_mixtral_v3_fixed model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, vocab_path: Optional[str] = None, **kwargs):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.vocab_path = Path(vocab_path) if vocab_path else self.model_path.with_suffix('.vocab')\n",
    "        \n",
    "        # Load tokenizer model and vocabulary\n",
    "        self.sp_model = self._load_sentencepiece_model()\n",
    "        self.vocab = self._load_vocabulary()\n",
    "        \n",
    "        # Special tokens\n",
    "        self.special_tokens = {\n",
    "            '<unk>': 0,\n",
    "            '<s>': 1,\n",
    "            '</s>': 2,\n",
    "            '<pad>': 3,\n",
    "            '<mask>': 4,\n",
    "        }\n",
    "        \n",
    "        # Turkish-specific tokens\n",
    "        self.turkish_tokens = {\n",
    "            '<NUMBER>': 5,\n",
    "            '<YEAR>': 6,\n",
    "            '<DATE>': 7,\n",
    "            '<TIME>': 8,\n",
    "            '<PERCENTAGE>': 9,\n",
    "            '<CURRENCY>': 10,\n",
    "            '<TECH_TERM>': 11,\n",
    "        }\n",
    "        \n",
    "        # Initialize parent class\n",
    "        super().__init__(\n",
    "            pad_token='<pad>',\n",
    "            unk_token='<unk>',\n",
    "            bos_token='<s>',\n",
    "            eos_token='</s>',\n",
    "            mask_token='<mask>',\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        self.model_max_length = kwargs.get('model_max_length', 8192)\n",
    "        \n",
    "    def _load_sentencepiece_model(self):\n",
    "        \"\"\"Load SentencePiece model from file\"\"\"\n",
    "        try:\n",
    "            import sentencepiece as spm\n",
    "            sp_model = spm.SentencePieceProcessor()\n",
    "            \n",
    "            # Check if model file exists\n",
    "            if self.model_path.exists():\n",
    "                sp_model.Load(str(self.model_path))\n",
    "                logger.info(f\"âœ… Loaded Turkish tokenizer from {self.model_path}\")\n",
    "            else:\n",
    "                # Fallback: create a basic model\n",
    "                logger.warning(f\"Model file not found at {self.model_path}, using fallback\")\n",
    "                # Initialize with basic Turkish vocabulary\n",
    "                sp_model = None\n",
    "            \n",
    "            return sp_model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load SentencePiece model: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _load_vocabulary(self) -> Dict[str, int]:\n",
    "        \"\"\"Load vocabulary from file or create default\"\"\"\n",
    "        vocab = {}\n",
    "        \n",
    "        # Add special tokens\n",
    "        for token, idx in self.special_tokens.items():\n",
    "            vocab[token] = idx\n",
    "        \n",
    "        # Add Turkish-specific tokens\n",
    "        for token, idx in self.turkish_tokens.items():\n",
    "            vocab[token] = idx\n",
    "        \n",
    "        # Load vocabulary from file if exists\n",
    "        if self.vocab_path.exists():\n",
    "            try:\n",
    "                with open(self.vocab_path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if '\\t' in line:\n",
    "                            token, idx = line.strip().split('\\t')\n",
    "                            vocab[token] = int(idx)\n",
    "                logger.info(f\"âœ… Loaded vocabulary with {len(vocab)} tokens\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load vocabulary: {e}\")\n",
    "        \n",
    "        # Ensure minimum vocabulary size\n",
    "        if len(vocab) < 32000:\n",
    "            # Add common Turkish subwords\n",
    "            turkish_subwords = ['lar', 'ler', 'ar', 'er', 'an', 'en', 'in', 'Ä±n', \n",
    "                               'un', 'Ã¼n', 'da', 'de', 'ta', 'te', 'dan', 'den',\n",
    "                               'tan', 'ten', 'la', 'le', 'yla', 'yle']\n",
    "            \n",
    "            for i, subword in enumerate(turkish_subwords, start=len(vocab)):\n",
    "                if subword not in vocab:\n",
    "                    vocab[f'â–{subword}'] = i\n",
    "        \n",
    "        self.vocab_size = len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using SentencePiece or fallback\"\"\"\n",
    "        if self.sp_model:\n",
    "            return self.sp_model.encode_as_pieces(text)\n",
    "        else:\n",
    "            # Fallback: simple whitespace + subword tokenization\n",
    "            tokens = []\n",
    "            for word in text.split():\n",
    "                if len(word) > 5:\n",
    "                    # Split long words into subwords\n",
    "                    tokens.append(f'â–{word[:3]}')\n",
    "                    tokens.append(word[3:])\n",
    "                else:\n",
    "                    tokens.append(f'â–{word}')\n",
    "            return tokens\n",
    "    \n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        \"\"\"Convert token to ID\"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get('<unk>', 0))\n",
    "    \n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        \"\"\"Convert ID to token\"\"\"\n",
    "        for token, idx in self.vocab.items():\n",
    "            if idx == index:\n",
    "                return token\n",
    "        return '<unk>'\n",
    "    \n",
    "    def convert_tokens_to_string(self, tokens: List[str]) -> str:\n",
    "        \"\"\"Convert tokens back to string\"\"\"\n",
    "        text = ''.join(tokens)\n",
    "        text = text.replace('â–', ' ').strip()\n",
    "        return text\n",
    "    \n",
    "    def __call__(self, text, **kwargs):\n",
    "        \"\"\"Make tokenizer callable for compatibility\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        max_length = kwargs.get('max_length', self.model_max_length)\n",
    "        padding = kwargs.get('padding', False)\n",
    "        truncation = kwargs.get('truncation', False)\n",
    "        return_tensors = kwargs.get('return_tensors', None)\n",
    "        \n",
    "        encoded = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        for t in text:\n",
    "            tokens = self._tokenize(t)\n",
    "            ids = [self._convert_token_to_id(token) for token in tokens]\n",
    "            \n",
    "            # Add special tokens\n",
    "            ids = [self.bos_token_id] + ids + [self.eos_token_id]\n",
    "            \n",
    "            if truncation and len(ids) > max_length:\n",
    "                ids = ids[:max_length]\n",
    "            \n",
    "            attention_mask = [1] * len(ids)\n",
    "            \n",
    "            if padding == 'max_length':\n",
    "                pad_length = max_length - len(ids)\n",
    "                ids = ids + [self.pad_token_id] * pad_length\n",
    "                attention_mask = attention_mask + [0] * pad_length\n",
    "            \n",
    "            encoded.append(ids)\n",
    "            attention_masks.append(attention_mask)\n",
    "        \n",
    "        result = {\n",
    "            'input_ids': encoded,\n",
    "            'attention_mask': attention_masks\n",
    "        }\n",
    "        \n",
    "        # Convert to tensors if requested\n",
    "        if return_tensors == 'pt':\n",
    "            import torch\n",
    "            result = {\n",
    "                'input_ids': torch.tensor(encoded),\n",
    "                'attention_mask': torch.tensor(attention_masks)\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "\n",
    "# Tokenizer Manager Interface\n",
    "class ITokenizerManager(ABC):\n",
    "    @abstractmethod\n",
    "    def get_tokenizer(self):\n",
    "        pass\n",
    "\n",
    "class TokenizerManager(ITokenizerManager):\n",
    "    \"\"\"Manages tokenizer with multiple fallback options\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, turkish_model_path: Optional[str] = None):\n",
    "        self.model_name = model_name\n",
    "        self.turkish_model_path = turkish_model_path\n",
    "        self.tokenizer = None\n",
    "        self.tokenizer_type = None\n",
    "        self._initialize_tokenizer()\n",
    "    \n",
    "    def _initialize_tokenizer(self):\n",
    "        \"\"\"Initialize tokenizer with fallback options\"\"\"\n",
    "        \n",
    "        # Try loading Turkish tokenizer first\n",
    "        if self.turkish_model_path and Path(self.turkish_model_path).exists():\n",
    "            try:\n",
    "                logger.info(f\"Loading Turkish tokenizer from {self.turkish_model_path}\")\n",
    "                self.tokenizer = TurkishTokenizer(\n",
    "                    model_path=self.turkish_model_path,\n",
    "                    model_max_length=8192\n",
    "                )\n",
    "                self.tokenizer_type = \"turkish_custom\"\n",
    "                logger.info(\"âœ… Turkish tokenizer loaded successfully\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load Turkish tokenizer: {e}\")\n",
    "        \n",
    "        # Fallback to model-specific tokenizer\n",
    "        try:\n",
    "            from transformers import AutoTokenizer\n",
    "            logger.info(f\"Loading tokenizer for {self.model_name}\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "                self.model_name,\n",
    "                trust_remote_code=True,\n",
    "                use_fast=True\n",
    "            )\n",
    "            self.tokenizer_type = \"model_specific\"\n",
    "            logger.info(f\"âœ… Loaded {self.model_name} tokenizer\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model tokenizer: {e}\")\n",
    "            raise RuntimeError(\"Cannot initialize tokenizer\")\n",
    "        \n",
    "        # Ensure special tokens are set\n",
    "        self._setup_special_tokens()\n",
    "    \n",
    "    def _setup_special_tokens(self):\n",
    "        \"\"\"Setup special tokens for the tokenizer\"\"\"\n",
    "        if self.tokenizer_type == \"turkish_custom\":\n",
    "            return  # Already set in TurkishTokenizer\n",
    "        \n",
    "        if not self.tokenizer.pad_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token or \"<pad>\"\n",
    "        \n",
    "        if not self.tokenizer.eos_token:\n",
    "            self.tokenizer.eos_token = \"</s>\"\n",
    "        \n",
    "        if not self.tokenizer.bos_token:\n",
    "            self.tokenizer.bos_token = \"<s>\"\n",
    "        \n",
    "        logger.info(\"âœ… Special tokens configured\")\n",
    "    \n",
    "    def get_tokenizer(self):\n",
    "        \"\"\"Get the initialized tokenizer\"\"\"\n",
    "        return self.tokenizer\n",
    "\n",
    "# Register Tokenizer Manager\n",
    "container.register('tokenizer_manager', \n",
    "                  lambda: TokenizerManager(\n",
    "                      model_name=\"Qwen/Qwen3-8B\",\n",
    "                      turkish_model_path=\"turkish_mixtral_v3_fixed.model\"\n",
    "                  ))\n",
    "\n",
    "# Get tokenizer\n",
    "tokenizer_manager = container.get('tokenizer_manager')\n",
    "tokenizer = tokenizer_manager.get_tokenizer()\n",
    "\n",
    "print(f\"Tokenizer initialized: {tokenizer_manager.tokenizer_type}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size if hasattr(tokenizer, 'vocab_size') else 'N/A'}\")\n",
    "\n",
    "# Test tokenizer\n",
    "test_text = \"Merhaba, bu bir TÃ¼rkÃ§e test metnidir. Yapay zeka ve makine Ã¶ÄŸrenmesi.\"\n",
    "encoded = tokenizer(test_text, truncation=True, padding='max_length', max_length=32)\n",
    "print(f\"Test encoding: {encoded['input_ids'][:10] if isinstance(encoded['input_ids'], list) else encoded['input_ids'][:10].tolist()}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 4. Advanced Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Production-ready training configuration with advanced optimizations\"\"\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name: str = \"Qwen/Qwen3-8B\"  # Fixed to Qwen3-8B\n",
    "    teacher_model_name: str = \"TURKCELL/Turkcell-LLM-7b-v1\"  # Teacher for distillation\n",
    "    \n",
    "    # Training parameters\n",
    "    num_epochs: int = 3\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    \n",
    "    # Batch settings (will be auto-tuned)\n",
    "    batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    max_length: int = 512\n",
    "    \n",
    "    # Advanced Optimization Features\n",
    "    use_flash_attention: bool = True\n",
    "    use_ema: bool = True  # Exponential Moving Average\n",
    "    ema_decay: float = 0.999\n",
    "    use_label_smoothing: bool = True\n",
    "    label_smoothing_factor: float = 0.1\n",
    "    use_curriculum_learning: bool = True\n",
    "    use_dynamic_padding: bool = True\n",
    "    compile_model: bool = True  # torch.compile for faster execution\n",
    "    \n",
    "    # LoRA settings\n",
    "    use_lora: bool = True\n",
    "    lora_rank: int = 32\n",
    "    lora_alpha: int = 64\n",
    "    lora_dropout: float = 0.1\n",
    "    lora_target_modules: List[str] = field(default_factory=lambda: [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ])\n",
    "    \n",
    "    # Quantization\n",
    "    use_4bit: bool = True\n",
    "    use_8bit: bool = False\n",
    "    bnb_4bit_compute_dtype: str = \"bfloat16\"  # Use bfloat16 for A100\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_use_double_quant: bool = True\n",
    "    \n",
    "    # Memory optimization\n",
    "    gradient_checkpointing: bool = True\n",
    "    optim: str = \"paged_adamw_8bit\"  # 8-bit optimizer\n",
    "    cpu_offload: bool = False  # CPU offloading for large models\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16: bool = False\n",
    "    bf16: bool = True  # Better for A100\n",
    "    tf32: bool = True\n",
    "    \n",
    "    # Knowledge Distillation\n",
    "    use_distillation: bool = True\n",
    "    distillation_temperature: float = 4.0\n",
    "    distillation_alpha: float = 0.7\n",
    "    teacher_cache_dir: str = \"./teacher_cache\"\n",
    "    \n",
    "    # Gradient clipping and stability\n",
    "    max_grad_norm: float = 1.0\n",
    "    gradient_clipping: bool = True\n",
    "    \n",
    "    # Logging and checkpointing\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 100\n",
    "    save_total_limit: int = 2\n",
    "    \n",
    "    # Output\n",
    "    output_dir: str = \"./checkpoints\"\n",
    "    resume_from_checkpoint: Optional[str] = None\n",
    "    \n",
    "    # Health monitoring\n",
    "    enable_health_monitoring: bool = True\n",
    "    health_check_interval: int = 50\n",
    "    auto_recovery: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Auto-tune configuration based on hardware\"\"\"\n",
    "        self._auto_tune_for_hardware()\n",
    "        self._validate_config()\n",
    "    \n",
    "    def _auto_tune_for_hardware(self):\n",
    "        \"\"\"Automatically adjust settings based on available hardware\"\"\"\n",
    "        gpu_manager = container.get('gpu_manager')\n",
    "        \n",
    "        if not gpu_manager.has_gpu:\n",
    "            logger.warning(\"No GPU detected, using CPU settings\")\n",
    "            self.batch_size = 1\n",
    "            self.gradient_accumulation_steps = 16\n",
    "            self.max_length = 128\n",
    "            self.use_4bit = False\n",
    "            self.gradient_checkpointing = False\n",
    "            self.use_flash_attention = False\n",
    "            self.compile_model = False\n",
    "            return\n",
    "        \n",
    "        gpu_memory = gpu_manager.gpu_info.get('memory_total', 16)\n",
    "        gpu_type = gpu_manager.gpu_info.get('gpu_type', 'generic')\n",
    "        \n",
    "        logger.info(f\"Auto-tuning for {gpu_type} GPU with {gpu_memory:.1f}GB memory\")\n",
    "        \n",
    "        # A100 GPU (40-80GB) - Optimal settings\n",
    "        if gpu_type == 'a100':\n",
    "            self.batch_size = 8 if gpu_memory > 70 else 4\n",
    "            self.gradient_accumulation_steps = 2\n",
    "            self.max_length = 1024 if gpu_memory > 70 else 512\n",
    "            self.lora_rank = 64\n",
    "            self.lora_alpha = 128\n",
    "            self.bf16 = True\n",
    "            self.fp16 = False\n",
    "            self.use_flash_attention = gpu_manager.flash_attn_available\n",
    "            self.compile_model = True\n",
    "            self.use_ema = True\n",
    "            self.use_distillation = True\n",
    "            logger.info(\"Configured for A100 with optimal settings\")\n",
    "        \n",
    "        # V100 GPU (16-32GB)\n",
    "        elif gpu_type == 'v100' or gpu_memory < 40:\n",
    "            self.batch_size = 2\n",
    "            self.gradient_accumulation_steps = 4\n",
    "            self.max_length = 384\n",
    "            self.lora_rank = 32\n",
    "            self.lora_alpha = 64\n",
    "            self.fp16 = True\n",
    "            self.bf16 = False\n",
    "            self.use_flash_attention = False\n",
    "            self.compile_model = False\n",
    "            logger.info(\"Configured for V100/mid-range GPU\")\n",
    "        \n",
    "        # T4 GPU (16GB) - Colab Free\n",
    "        elif gpu_type == 't4' or gpu_memory < 20:\n",
    "            self.batch_size = 1\n",
    "            self.gradient_accumulation_steps = 8\n",
    "            self.max_length = 256\n",
    "            self.lora_rank = 16\n",
    "            self.lora_alpha = 32\n",
    "            self.use_distillation = False  # Disable for memory\n",
    "            self.use_ema = False\n",
    "            self.fp16 = True\n",
    "            self.bf16 = False\n",
    "            self.use_flash_attention = False\n",
    "            self.compile_model = False\n",
    "            logger.info(\"Configured for T4/low-memory GPU\")\n",
    "    \n",
    "    def _validate_config(self):\n",
    "        \"\"\"Validate configuration for consistency\"\"\"\n",
    "        # Ensure only one quantization method is used\n",
    "        if self.use_4bit and self.use_8bit:\n",
    "            logger.warning(\"Both 4-bit and 8-bit quantization enabled, using 4-bit only\")\n",
    "            self.use_8bit = False\n",
    "        \n",
    "        # Ensure only one precision is used\n",
    "        if self.fp16 and self.bf16:\n",
    "            gpu_manager = container.get('gpu_manager')\n",
    "            if gpu_manager.gpu_info.get('supports_bf16', False):\n",
    "                self.fp16 = False\n",
    "                logger.info(\"Using bf16 precision (better for modern GPUs)\")\n",
    "            else:\n",
    "                self.bf16 = False\n",
    "                logger.info(\"Using fp16 precision\")\n",
    "    \n",
    "    def to_dict(self) -> Dict[str, Any]:\n",
    "        \"\"\"Convert config to dictionary\"\"\"\n",
    "        return asdict(self)\n",
    "    \n",
    "    def save(self, filename: str = \"training_config.json\"):\n",
    "        \"\"\"Save configuration to file\"\"\"\n",
    "        config_path = Path(filename)\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2, default=str)\n",
    "        logger.info(f\"âœ… Config saved to {config_path}\")\n",
    "\n",
    "# Initialize configuration\n",
    "config = TrainingConfig()\n",
    "config.save()\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(json.dumps(config.to_dict(), indent=2, default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ 5. Model Loading with Knowledge Distillation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model Manager Interface\n",
    "class IModelManager(ABC):\n",
    "    @abstractmethod\n",
    "    def load_model(self, model_name: Optional[str] = None) -> nn.Module:\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_model(self) -> nn.Module:\n",
    "        pass\n",
    "\n",
    "class ModelManager(IModelManager):\n",
    "    \"\"\"Manages model loading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig, gpu_manager: IGPUManager):\n",
    "        self.config = config\n",
    "        self.gpu_manager = gpu_manager\n",
    "        self.model = None\n",
    "        self.teacher_model = None\n",
    "        self.peft_config = None\n",
    "        self.bnb_config = None\n",
    "        self.ema_model = None\n",
    "        self._setup_quantization()\n",
    "    \n",
    "    def _setup_quantization(self):\n",
    "        \"\"\"Setup quantization configuration\"\"\"\n",
    "        if not self.config.use_4bit and not self.config.use_8bit:\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            compute_dtype = getattr(torch, self.config.bnb_4bit_compute_dtype)\n",
    "            \n",
    "            self.bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=self.config.use_4bit,\n",
    "                load_in_8bit=self.config.use_8bit,\n",
    "                bnb_4bit_compute_dtype=compute_dtype,\n",
    "                bnb_4bit_quant_type=self.config.bnb_4bit_quant_type,\n",
    "                bnb_4bit_use_double_quant=self.config.bnb_4bit_use_double_quant,\n",
    "            )\n",
    "            logger.info(\"âœ… Quantization configured\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to setup quantization: {e}\")\n",
    "            self.bnb_config = None\n",
    "    \n",
    "    def load_model(self, model_name: Optional[str] = None) -> nn.Module:\n",
    "        \"\"\"Load model with multiple fallback options\"\"\"\n",
    "        model_name = model_name or self.config.model_name\n",
    "        \n",
    "        # Configure model loading kwargs\n",
    "        model_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"torch_dtype\": torch.bfloat16 if self.config.bf16 else torch.float16,\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        }\n",
    "        \n",
    "        # Add Flash Attention if available\n",
    "        if self.config.use_flash_attention and self.gpu_manager.flash_attn_available:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            logger.info(\"âœ… Using Flash Attention 2\")\n",
    "        \n",
    "        # Try loading with quantization\n",
    "        if self.bnb_config:\n",
    "            try:\n",
    "                logger.info(f\"Loading {model_name} with quantization...\")\n",
    "                model_kwargs[\"quantization_config\"] = self.bnb_config\n",
    "                model_kwargs[\"device_map\"] = \"auto\"\n",
    "                \n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "                logger.info(\"âœ… Model loaded with quantization\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load with quantization: {e}\")\n",
    "                self.gpu_manager.clear_memory()\n",
    "                # Try without quantization\n",
    "                del model_kwargs[\"quantization_config\"]\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    **model_kwargs\n",
    "                )\n",
    "        else:\n",
    "            # Load without quantization\n",
    "            model_kwargs[\"device_map\"] = \"auto\" if self.gpu_manager.has_gpu else \"cpu\"\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                **model_kwargs\n",
    "            )\n",
    "        \n",
    "        # Setup PEFT\n",
    "        self._setup_peft_model()\n",
    "        \n",
    "        # Setup EMA if enabled\n",
    "        if self.config.use_ema:\n",
    "            self._setup_ema()\n",
    "        \n",
    "        # Compile model if enabled\n",
    "        if self.config.compile_model and torch.__version__ >= \"2.0.0\":\n",
    "            try:\n",
    "                self.model = torch.compile(self.model, mode=\"reduce-overhead\")\n",
    "                logger.info(\"âœ… Model compiled with torch.compile\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to compile model: {e}\")\n",
    "        \n",
    "        # Load teacher model for distillation\n",
    "        if self.config.use_distillation:\n",
    "            self._load_teacher_model()\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _setup_peft_model(self) -> nn.Module:\n",
    "        \"\"\"Setup PEFT (LoRA) for the model\"\"\"\n",
    "        if not self.config.use_lora:\n",
    "            return self.model\n",
    "        \n",
    "        try:\n",
    "            # Prepare model for training\n",
    "            if self.config.use_4bit or self.config.use_8bit:\n",
    "                self.model = prepare_model_for_kbit_training(\n",
    "                    self.model,\n",
    "                    use_gradient_checkpointing=self.config.gradient_checkpointing\n",
    "                )\n",
    "            \n",
    "            # Configure LoRA\n",
    "            self.peft_config = LoraConfig(\n",
    "                r=self.config.lora_rank,\n",
    "                lora_alpha=self.config.lora_alpha,\n",
    "                lora_dropout=self.config.lora_dropout,\n",
    "                bias=\"none\",\n",
    "                task_type=TaskType.CAUSAL_LM,\n",
    "                target_modules=self.config.lora_target_modules,\n",
    "            )\n",
    "            \n",
    "            # Apply LoRA\n",
    "            self.model = get_peft_model(self.model, self.peft_config)\n",
    "            self.model.print_trainable_parameters()\n",
    "            \n",
    "            logger.info(\"âœ… LoRA configured successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to setup LoRA: {e}\")\n",
    "        \n",
    "        # Enable gradient checkpointing if requested\n",
    "        if self.config.gradient_checkpointing:\n",
    "            try:\n",
    "                self.model.gradient_checkpointing_enable()\n",
    "                logger.info(\"âœ… Gradient checkpointing enabled\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to enable gradient checkpointing: {e}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def _setup_ema(self):\n",
    "        \"\"\"Setup Exponential Moving Average model\"\"\"\n",
    "        try:\n",
    "            from copy import deepcopy\n",
    "            self.ema_model = deepcopy(self.model)\n",
    "            for param in self.ema_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            logger.info(\"âœ… EMA model initialized\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to setup EMA: {e}\")\n",
    "            self.config.use_ema = False\n",
    "    \n",
    "    def _load_teacher_model(self):\n",
    "        \"\"\"Load teacher model for knowledge distillation\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading teacher model: {self.config.teacher_model_name}\")\n",
    "            \n",
    "            # Check if cached teacher outputs exist\n",
    "            cache_dir = Path(self.config.teacher_cache_dir)\n",
    "            cache_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            teacher_kwargs = {\n",
    "                \"torch_dtype\": torch.float16,\n",
    "                \"low_cpu_mem_usage\": True,\n",
    "                \"device_map\": \"auto\",\n",
    "            }\n",
    "            \n",
    "            self.teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.config.teacher_model_name,\n",
    "                **teacher_kwargs\n",
    "            )\n",
    "            \n",
    "            # Set teacher to eval mode\n",
    "            self.teacher_model.eval()\n",
    "            for param in self.teacher_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            \n",
    "            logger.info(\"âœ… Teacher model loaded for distillation\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to load teacher model: {e}\")\n",
    "            self.config.use_distillation = False\n",
    "            self.teacher_model = None\n",
    "    \n",
    "    def update_ema(self):\n",
    "        \"\"\"Update EMA model weights\"\"\"\n",
    "        if not self.config.use_ema or self.ema_model is None:\n",
    "            return\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for ema_param, model_param in zip(self.ema_model.parameters(), \n",
    "                                             self.model.parameters()):\n",
    "                ema_param.data.mul_(self.config.ema_decay).add_(\n",
    "                    model_param.data, alpha=1 - self.config.ema_decay\n",
    "                )\n",
    "    \n",
    "    def get_model(self) -> nn.Module:\n",
    "        \"\"\"Get the loaded model\"\"\"\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "        return self.model\n",
    "\n",
    "# Register Model Manager\n",
    "container.register('model_manager', \n",
    "                  lambda: ModelManager(config, container.get('gpu_manager')))\n",
    "\n",
    "# Load model\n",
    "model_manager = container.get('model_manager')\n",
    "model = model_manager.load_model()\n",
    "\n",
    "print(f\"Model loaded: {config.model_name}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e6:.2f}M\")\n",
    "print(f\"Teacher model loaded: {model_manager.teacher_model is not None}\")\n",
    "print(f\"EMA enabled: {model_manager.ema_model is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ 6. Training with Advanced Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple test dataset for demonstration\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Create a simple dataset for testing\n",
    "test_data = [\n",
    "    {\"text\": \"Python programlama dili, yapay zeka ve veri bilimi alanlarÄ±nda yaygÄ±n olarak kullanÄ±lÄ±r.\"},\n",
    "    {\"text\": \"Makine Ã¶ÄŸrenmesi, bilgisayarlarÄ±n veriden Ã¶ÄŸrenmesini saÄŸlayan algoritmalar geliÅŸtirir.\"},\n",
    "    {\"text\": \"Derin Ã¶ÄŸrenme, yapay sinir aÄŸlarÄ± kullanarak karmaÅŸÄ±k problemleri Ã§Ã¶zer.\"},\n",
    "    {\"text\": \"TÃ¼rkiye'de teknoloji sektÃ¶rÃ¼ hÄ±zla bÃ¼yÃ¼mekte ve yeni iÅŸ imkanlarÄ± yaratmaktadÄ±r.\"},\n",
    "    {\"text\": \"Bulut biliÅŸim, iÅŸletmelerin BT altyapÄ±sÄ±nÄ± daha verimli yÃ¶netmesini saÄŸlar.\"},\n",
    "] * 100  # Replicate for larger dataset\n",
    "\n",
    "# Create dataset\n",
    "dataset = Dataset.from_list(test_data)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n",
    "\n",
    "# Now train the model\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Setup training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=1,  # Quick test\n",
    "    per_device_train_batch_size=config.batch_size,\n",
    "    per_device_eval_batch_size=config.batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    weight_decay=config.weight_decay,\n",
    "    warmup_ratio=config.warmup_ratio,\n",
    "    \n",
    "    # Optimization\n",
    "    optim=config.optim,\n",
    "    gradient_checkpointing=config.gradient_checkpointing,\n",
    "    max_grad_norm=config.max_grad_norm if config.gradient_clipping else None,\n",
    "    \n",
    "    # Mixed precision\n",
    "    fp16=config.fp16,\n",
    "    bf16=config.bf16,\n",
    "    tf32=config.tf32,\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    \n",
    "    # Saving\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    # Others\n",
    "    report_to=[\"tensorboard\"] if ENV_INFO['is_colab'] else [\"none\"],\n",
    "    push_to_hub=False,\n",
    "    dataloader_num_workers=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Custom trainer with advanced features\n",
    "class AdvancedTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with distillation and advanced optimizations\"\"\"\n",
    "    \n",
    "    def __init__(self, *args, model_manager=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_manager = model_manager\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"Compute loss with distillation and label smoothing\"\"\"\n",
    "        labels = inputs.pop(\"labels\", None)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Standard cross-entropy loss\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            \n",
    "            # Label smoothing\n",
    "            if config.use_label_smoothing:\n",
    "                loss_fct = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing_factor)\n",
    "            else:\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "            \n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        else:\n",
    "            loss = outputs.loss\n",
    "        \n",
    "        # Knowledge distillation\n",
    "        if config.use_distillation and self.model_manager.teacher_model is not None:\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.model_manager.teacher_model(**inputs)\n",
    "                teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "            # Distillation loss\n",
    "            T = config.distillation_temperature\n",
    "            distill_loss = F.kl_div(\n",
    "                F.log_softmax(logits / T, dim=-1),\n",
    "                F.softmax(teacher_logits / T, dim=-1),\n",
    "                reduction='batchmean'\n",
    "            ) * (T ** 2)\n",
    "            \n",
    "            # Combine losses\n",
    "            loss = config.distillation_alpha * loss + \\\n",
    "                   (1 - config.distillation_alpha) * distill_loss\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def training_step(self, model, inputs):\n",
    "        \"\"\"Custom training step with EMA update\"\"\"\n",
    "        loss = super().training_step(model, inputs)\n",
    "        \n",
    "        # Update EMA model\n",
    "        if self.model_manager and config.use_ema:\n",
    "            self.model_manager.update_ema()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Setup data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=8 if config.use_dynamic_padding else None\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = AdvancedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    model_manager=model_manager,\n",
    ")\n",
    "\n",
    "print(\"âœ… Advanced trainer configured with:\")\n",
    "print(f\"  - Knowledge Distillation: {config.use_distillation}\")\n",
    "print(f\"  - Label Smoothing: {config.use_label_smoothing}\")\n",
    "print(f\"  - EMA: {config.use_ema}\")\n",
    "print(f\"  - Flash Attention: {config.use_flash_attention and gpu_manager.flash_attn_available}\")\n",
    "print(f\"  - Dynamic Padding: {config.use_dynamic_padding}\")\n",
    "print(f\"  - Gradient Clipping: {config.gradient_clipping}\")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nðŸš€ Starting training...\")\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š 7. Evaluation and Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Evaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation loss: {eval_results.get('eval_loss', 'N/A')}\")\n",
    "\n",
    "# Test generation\n",
    "def generate_text(prompt: str, max_length: int = 100) -> str:\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    try:\n",
    "        # Use EMA model if available\n",
    "        generation_model = model_manager.ema_model if model_manager.ema_model else model\n",
    "        \n",
    "        # Encode the prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        # Move to device\n",
    "        if gpu_manager.has_gpu:\n",
    "            inputs = {k: v.to(gpu_manager.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = generation_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Generation failed: {e}\")\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"Python programlama dili\",\n",
    "    \"Yapay zeka ve makine Ã¶ÄŸrenmesi\",\n",
    "    \"TÃ¼rkiye'de teknoloji geliÅŸimi\",\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ§ª Testing trained model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(\"-\" * 30)\n",
    "    generated = generate_text(prompt, max_length=50)\n",
    "    print(f\"Generated: {generated}\")\n",
    "\n",
    "# Save the final model\n",
    "print(\"\\nðŸ’¾ Saving model...\")\n",
    "trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "print(\"âœ… Model saved to ./final_model\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ðŸ“Š TRAINING COMPLETE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {config.model_name}\")\n",
    "print(f\"Teacher: {config.teacher_model_name if config.use_distillation else 'None'}\")\n",
    "print(f\"Tokenizer: {tokenizer_manager.tokenizer_type}\")\n",
    "print(f\"GPU: {gpu_manager.gpu_info.get('name', 'CPU')}\")\n",
    "print(f\"Optimizations enabled:\")\n",
    "print(f\"  - Flash Attention: {config.use_flash_attention and gpu_manager.flash_attn_available}\")\n",
    "print(f\"  - EMA: {config.use_ema}\")\n",
    "print(f\"  - Knowledge Distillation: {config.use_distillation}\")\n",
    "print(f\"  - Label Smoothing: {config.use_label_smoothing}\")\n",
    "print(f\"  - Gradient Checkpointing: {config.gradient_checkpointing}\")\n",
    "print(f\"  - Model Compilation: {config.compile_model}\")\n",
    "print(\"\\nðŸŽ‰ Training pipeline executed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}