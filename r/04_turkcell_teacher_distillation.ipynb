{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Turkcell-LLM-7B Knowledge Distillation Pipeline\n",
    "## Türkçe Eğitim Asistanı için Optimize Edilmiş Bilgi Damıtma\n",
    "\n",
    "Bu notebook, **Turkcell-LLM-7b-v1** teacher model kullanarak Türkçe için optimize edilmiş knowledge distillation pipeline'ı içerir.\n",
    "\n",
    "### ✅ Özellikler:\n",
    "- Turkcell-LLM-7b-v1 teacher model\n",
    "- A100 GPU için optimize edilmiş\n",
    "- Mixed precision training (bf16)\n",
    "- Gradient checkpointing\n",
    "- Curriculum learning\n",
    "- Türkçe'ye özel metrikler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistem kontrolü ve setup\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"🎯 TURKCELL-LLM-7B TEACHER MODEL SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# GPU kontrolü\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"✅ GPU: {gpu_name}\")\n",
    "    print(f\"💾 GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"🔧 CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"⚡ Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "    \n",
    "    # A100 optimizasyonları\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\n🚀 A100 GPU tespit edildi - Optimizasyonlar aktif:\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"  ✓ TF32 precision aktif\")\n",
    "        print(\"  ✓ Flash Attention 2 kullanılabilir\")\n",
    "        OPTIMAL_BATCH_SIZE = 16\n",
    "    else:\n",
    "        OPTIMAL_BATCH_SIZE = 8\n",
    "else:\n",
    "    print(\"⚠️ GPU bulunamadı\")\n",
    "    OPTIMAL_BATCH_SIZE = 4\n",
    "\n",
    "print(f\"\\n📊 Önerilen batch size: {OPTIMAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli kütüphaneleri yükle\n",
    "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 bitsandbytes>=0.41.3 peft>=0.7.1\n",
    "!pip install -q datasets evaluate nltk rouge-score sacrebleu bert-score\n",
    "!pip install -q sentencepiece protobuf ftfy langdetect\n",
    "!pip install -q wandb  # Opsiyonel: Experiment tracking\n",
    "\n",
    "print(\"✅ Tüm kütüphaneler yüklendi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 1. Turkcell Teacher Model Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import gc\n",
    "\n",
    "@dataclass\n",
    "class TurkcellTeacherConfig:\n",
    "    \"\"\"Turkcell Teacher Model Konfigürasyonu\"\"\"\n",
    "    model_id: str = \"TURKCELL/Turkcell-LLM-7b-v1\"\n",
    "    quantization: str = \"8bit\"  # 4bit, 8bit, none\n",
    "    use_flash_attention: bool = True\n",
    "    max_length: int = 2048\n",
    "    temperature: float = 3.0  # Distillation temperature\n",
    "    batch_size: int = 16\n",
    "    \n",
    "def load_turkcell_teacher(config: TurkcellTeacherConfig):\n",
    "    \"\"\"Turkcell-LLM-7b-v1 modelini optimize şekilde yükle\"\"\"\n",
    "    \n",
    "    print(f\"\\n📥 Turkcell Teacher Model yükleniyor...\")\n",
    "    print(f\"   Model: {config.model_id}\")\n",
    "    print(f\"   Quantization: {config.quantization}\")\n",
    "    \n",
    "    # Quantization config\n",
    "    if config.quantization == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"   ✓ 4-bit quantization aktif (En düşük bellek)\")\n",
    "    elif config.quantization == \"8bit\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"   ✓ 8-bit quantization aktif (Dengeli kalite-bellek)\")\n",
    "    else:\n",
    "        bnb_config = None\n",
    "        print(\"   ✓ Full precision (Maksimum kalite)\")\n",
    "    \n",
    "    # Model yükleme argümanları\n",
    "    model_kwargs = {\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    }\n",
    "    \n",
    "    if bnb_config:\n",
    "        model_kwargs[\"quantization_config\"] = bnb_config\n",
    "        \n",
    "    # Flash Attention 2 (A100 için)\n",
    "    if config.use_flash_attention and torch.cuda.is_available():\n",
    "        try:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"   ✓ Flash Attention 2 aktif\")\n",
    "        except:\n",
    "            print(\"   ⚠️ Flash Attention 2 kullanılamıyor\")\n",
    "    \n",
    "    # Model yükle\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_id,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        # Tokenizer yükle\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_id,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True  # Fast tokenizer\n",
    "        )\n",
    "        \n",
    "        # Padding token ayarla\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        # Model bilgileri\n",
    "        total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9\n",
    "        \n",
    "        print(f\"\\n✅ Turkcell Teacher Model başarıyla yüklendi!\")\n",
    "        print(f\"   Total parameters: {total_params:.2f}B\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:.2f}B\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   GPU Memory used: {memory_used:.2f}GB\")\n",
    "        \n",
    "        # Model'i eval moduna al\n",
    "        model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(\"   ✓ Model evaluation modunda ve donduruldu\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Model yükleme hatası: {e}\")\n",
    "        print(\"\\n💡 Çözüm önerileri:\")\n",
    "        print(\"   1. Quantization seviyesini artırın (8bit -> 4bit)\")\n",
    "        print(\"   2. Batch size'ı azaltın\")\n",
    "        print(\"   3. Gradient checkpointing kullanın\")\n",
    "        return None, None\n",
    "\n",
    "# Teacher model'i yükle\n",
    "teacher_config = TurkcellTeacherConfig(\n",
    "    quantization=\"8bit\",  # A100 40GB için 8-bit yeterli\n",
    "    use_flash_attention=True,\n",
    "    batch_size=OPTIMAL_BATCH_SIZE\n",
    ")\n",
    "\n",
    "teacher_model, teacher_tokenizer = load_turkcell_teacher(teacher_config)\n",
    "\n",
    "# Bellek temizleme\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 2. Student Model Seçimi ve Yükleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "@dataclass\n",
    "class StudentModelConfig:\n",
    "    \"\"\"Student Model Konfigürasyonu\"\"\"\n",
    "    model_id: str = \"ytu-ce-cosmos/turkish-gpt2-large\"  # Hafif Türkçe model\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list = None\n",
    "    \n",
    "def load_student_model(config: StudentModelConfig):\n",
    "    \"\"\"Student model'i LoRA ile yükle\"\"\"\n",
    "    \n",
    "    print(f\"\\n📥 Student Model yükleniyor...\")\n",
    "    print(f\"   Model: {config.model_id}\")\n",
    "    \n",
    "    # Student model - daha küçük\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Padding token\n",
    "    if student_tokenizer.pad_token is None:\n",
    "        student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "        student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
    "    \n",
    "    # LoRA configuration\n",
    "    if config.use_lora:\n",
    "        print(\"\\n🔧 LoRA konfigürasyonu uygulanıyor...\")\n",
    "        \n",
    "        # Model'i LoRA için hazırla\n",
    "        student_model = prepare_model_for_kbit_training(student_model)\n",
    "        \n",
    "        # LoRA config\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            target_modules=config.target_modules or [\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # LoRA uygula\n",
    "        student_model = get_peft_model(student_model, lora_config)\n",
    "        \n",
    "        # LoRA istatistikleri\n",
    "        trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in student_model.parameters())\n",
    "        \n",
    "        print(f\"\\n✅ Student Model (LoRA) hazır!\")\n",
    "        print(f\"   Total parameters: {all_params/1e6:.2f}M\")\n",
    "        print(f\"   Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "        print(f\"   Trainable %: {100 * trainable_params / all_params:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✅ Student Model (Full Fine-tuning) hazır!\")\n",
    "        \n",
    "    # Gradient checkpointing\n",
    "    if hasattr(student_model, 'gradient_checkpointing_enable'):\n",
    "        student_model.gradient_checkpointing_enable()\n",
    "        print(\"   ✓ Gradient checkpointing aktif\")\n",
    "        \n",
    "    return student_model, student_tokenizer\n",
    "\n",
    "# Student model alternatifler\n",
    "STUDENT_MODEL_OPTIONS = {\n",
    "    \"turkish-gpt2\": \"ytu-ce-cosmos/turkish-gpt2-large\",\n",
    "    \"turkish-bert-gpt2\": \"redrussianarmy/turkish-bert-gpt2\",\n",
    "    \"gpt2-small-turkish\": \"ytu-ce-cosmos/turkish-gpt2-small\",\n",
    "    \"distilgpt2-turkish\": \"dbmdz/distilbert-base-turkish-cased\"  # Encoder model\n",
    "}\n",
    "\n",
    "print(\"📋 Mevcut Student Model Seçenekleri:\")\n",
    "for key, value in STUDENT_MODEL_OPTIONS.items():\n",
    "    print(f\"   • {key}: {value}\")\n",
    "\n",
    "# Student model'i yükle\n",
    "student_config = StudentModelConfig(\n",
    "    model_id=STUDENT_MODEL_OPTIONS[\"turkish-gpt2\"],\n",
    "    use_lora=True,\n",
    "    lora_r=64,\n",
    "    lora_alpha=128\n",
    ")\n",
    "\n",
    "student_model, student_tokenizer = load_student_model(student_config)\n",
    "\n",
    "# Bellek durumu\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"\\n💾 GPU Bellek Durumu:\")\n",
    "    print(f\"   Kullanılan: {memory_used:.2f}GB\")\n",
    "    print(f\"   Rezerve: {memory_reserved:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 3. Knowledge Distillation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class TurkcellDistillationTrainer:\n",
    "    \"\"\"Turkcell Teacher Model için Knowledge Distillation Trainer\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 teacher_model,\n",
    "                 student_model,\n",
    "                 teacher_tokenizer,\n",
    "                 student_tokenizer,\n",
    "                 temperature: float = 3.0,\n",
    "                 alpha: float = 0.7,\n",
    "                 max_length: int = 512):\n",
    "        \n",
    "        self.teacher_model = teacher_model\n",
    "        self.student_model = student_model\n",
    "        self.teacher_tokenizer = teacher_tokenizer\n",
    "        self.student_tokenizer = student_tokenizer\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha  # KD loss weight\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Device\n",
    "        self.device = next(student_model.parameters()).device\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        self.use_amp = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        if self.use_amp:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"✅ Mixed precision (BF16) training aktif\")\n",
    "            \n",
    "        # Metrics tracking\n",
    "        self.training_history = []\n",
    "        \n",
    "    def compute_distillation_loss(self, \n",
    "                                 student_logits: torch.Tensor,\n",
    "                                 teacher_logits: torch.Tensor,\n",
    "                                 labels: torch.Tensor,\n",
    "                                 attention_mask: torch.Tensor = None):\n",
    "        \"\"\"Distillation loss hesaplama\"\"\"\n",
    "        \n",
    "        # Reshape logits\n",
    "        vocab_size = student_logits.size(-1)\n",
    "        student_logits_view = student_logits.view(-1, vocab_size)\n",
    "        teacher_logits_view = teacher_logits.view(-1, teacher_logits.size(-1))\n",
    "        labels_view = labels.view(-1)\n",
    "        \n",
    "        # Mask invalid positions\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.view(-1) == 1\n",
    "            student_logits_view = student_logits_view[mask]\n",
    "            teacher_logits_view = teacher_logits_view[mask]\n",
    "            labels_view = labels_view[mask]\n",
    "        \n",
    "        # KL Divergence loss (soft targets)\n",
    "        kd_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits_view / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_logits_view / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        # Cross entropy loss (hard targets)\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_logits_view,\n",
    "            labels_view,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'kd_loss': kd_loss.item(),\n",
    "            'ce_loss': ce_loss.item()\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_teacher_outputs(self, input_ids, attention_mask):\n",
    "        \"\"\"Teacher model çıktılarını al\"\"\"\n",
    "        outputs = self.teacher_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs.logits\n",
    "    \n",
    "    def train_step(self, batch, optimizer, scheduler=None):\n",
    "        \"\"\"Tek training adımı\"\"\"\n",
    "        \n",
    "        # Prepare inputs\n",
    "        texts = batch['text'] if isinstance(batch, dict) else batch\n",
    "        \n",
    "        # Teacher tokenization\n",
    "        teacher_inputs = self.teacher_tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Student tokenization\n",
    "        student_inputs = self.student_tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get teacher logits (no grad)\n",
    "        teacher_logits = self.get_teacher_outputs(\n",
    "            teacher_inputs['input_ids'],\n",
    "            teacher_inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if self.use_amp:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                student_outputs = self.student_model(\n",
    "                    input_ids=student_inputs['input_ids'],\n",
    "                    attention_mask=student_inputs['attention_mask'],\n",
    "                    labels=student_inputs['input_ids']\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss_dict = self.compute_distillation_loss(\n",
    "                    student_outputs.logits,\n",
    "                    teacher_logits,\n",
    "                    student_inputs['input_ids'],\n",
    "                    student_inputs['attention_mask']\n",
    "                )\n",
    "        else:\n",
    "            student_outputs = self.student_model(\n",
    "                input_ids=student_inputs['input_ids'],\n",
    "                attention_mask=student_inputs['attention_mask'],\n",
    "                labels=student_inputs['input_ids']\n",
    "            )\n",
    "            \n",
    "            loss_dict = self.compute_distillation_loss(\n",
    "                student_outputs.logits,\n",
    "                teacher_logits,\n",
    "                student_inputs['input_ids'],\n",
    "                student_inputs['attention_mask']\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return loss_dict\n",
    "    \n",
    "    def train_epoch(self, dataloader, optimizer, scheduler=None, epoch=0):\n",
    "        \"\"\"Bir epoch training\"\"\"\n",
    "        \n",
    "        self.student_model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                loss_dict = self.train_step(batch, optimizer, scheduler)\n",
    "                epoch_losses.append(loss_dict)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss_dict['loss']:.4f}\",\n",
    "                    'kd': f\"{loss_dict['kd_loss']:.4f}\",\n",
    "                    'ce': f\"{loss_dict['ce_loss']:.4f}\"\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"\\n⚠️ CUDA OOM! Batch'i atlayıp devam ediliyor...\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = np.mean([d['loss'] for d in epoch_losses])\n",
    "        avg_kd_loss = np.mean([d['kd_loss'] for d in epoch_losses])\n",
    "        avg_ce_loss = np.mean([d['ce_loss'] for d in epoch_losses])\n",
    "        \n",
    "        print(f\"\\n📊 Epoch {epoch+1} Özeti:\")\n",
    "        print(f\"   Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"   KD Loss: {avg_kd_loss:.4f}\")\n",
    "        print(f\"   CE Loss: {avg_ce_loss:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_loss,\n",
    "            'avg_kd_loss': avg_kd_loss,\n",
    "            'avg_ce_loss': avg_ce_loss\n",
    "        }\n",
    "\n",
    "# Trainer'ı oluştur\n",
    "trainer = TurkcellDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    temperature=teacher_config.temperature,\n",
    "    alpha=0.7,  # %70 KD, %30 CE\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Turkcell Distillation Trainer hazır!\")\n",
    "print(f\"   Temperature: {trainer.temperature}\")\n",
    "print(f\"   Alpha (KD weight): {trainer.alpha}\")\n",
    "print(f\"   Max length: {trainer.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 4. Türkçe Veri Yükleme ve Hazırlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class TurkishEducationDataset(Dataset):\n",
    "    \"\"\"Türkçe eğitim veri seti\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer=None, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        if isinstance(item, dict):\n",
    "            text = item.get('text', '') or item.get('content', '')\n",
    "        else:\n",
    "            text = str(item)\n",
    "            \n",
    "        return {'text': text}\n",
    "\n",
    "def load_turkish_data(source=\"demo\", max_samples=1000):\n",
    "    \"\"\"Türkçe veri yükle\"\"\"\n",
    "    \n",
    "    print(f\"\\n📥 Türkçe veri yükleniyor: {source}\")\n",
    "    \n",
    "    if source == \"demo\":\n",
    "        # Demo veri oluştur\n",
    "        demo_texts = [\n",
    "            \"Yapay zeka, insan zekasını taklit eden bilgisayar sistemleridir.\",\n",
    "            \"Python programlama dili, veri bilimi için popüler bir araçtır.\",\n",
    "            \"Makine öğrenmesi, verilerden örüntü çıkarma sürecidir.\",\n",
    "            \"Derin öğrenme, yapay sinir ağlarını kullanan bir makine öğrenmesi yöntemidir.\",\n",
    "            \"Türkiye'de teknoloji eğitimi hızla gelişmektedir.\",\n",
    "            \"Matematik, bilimin temelidir ve problem çözme becerilerini geliştirir.\",\n",
    "            \"Fizik, doğadaki olayları açıklayan temel bilimlerden biridir.\",\n",
    "            \"Kimya, maddenin yapısını ve özelliklerini inceler.\",\n",
    "            \"Biyoloji, canlıların yapısını ve yaşam süreçlerini araştırır.\",\n",
    "            \"Tarih, geçmişi anlamamıza ve geleceği şekillendirmemize yardımcı olur.\"\n",
    "        ]\n",
    "        \n",
    "        # Veriyi çoğalt\n",
    "        data = demo_texts * (max_samples // len(demo_texts) + 1)\n",
    "        data = data[:max_samples]\n",
    "        \n",
    "        print(f\"✅ {len(data)} demo örnek oluşturuldu\")\n",
    "        \n",
    "    elif source == \"huggingface\":\n",
    "        # Hugging Face'den Türkçe veri yükle\n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"ytu-ce-cosmos/turkish-qa\",  # Türkçe Q&A dataset\n",
    "                split=f\"train[:{max_samples}]\"\n",
    "            )\n",
    "            data = [item['text'] for item in dataset]\n",
    "            print(f\"✅ {len(data)} örnek Hugging Face'den yüklendi\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Hugging Face yükleme hatası: {e}\")\n",
    "            print(\"Demo veri kullanılıyor...\")\n",
    "            return load_turkish_data(\"demo\", max_samples)\n",
    "            \n",
    "    else:\n",
    "        # CSV'den yükle\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "            data = df['text'].tolist()[:max_samples]\n",
    "            print(f\"✅ {len(data)} örnek CSV'den yüklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ CSV yükleme hatası: {e}\")\n",
    "            return load_turkish_data(\"demo\", max_samples)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Veri yükle\n",
    "train_data = load_turkish_data(\"demo\", max_samples=100)\n",
    "val_data = load_turkish_data(\"demo\", max_samples=20)\n",
    "\n",
    "# Dataset oluştur\n",
    "train_dataset = TurkishEducationDataset(train_data)\n",
    "val_dataset = TurkishEducationDataset(val_data)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=teacher_config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=teacher_config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\n📊 Veri Özeti:\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Val samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {teacher_config.batch_size}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer ve Scheduler setup\n",
    "optimizer = AdamW(\n",
    "    student_model.parameters(),\n",
    "    lr=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # %10 warmup\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 Training Konfigürasyonu:\")\n",
    "print(f\"   Learning rate: {2e-5}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   Warmup steps: {warmup_steps}\")\n",
    "print(f\"   Epochs: 3\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🚀 TRAINING BAŞLIYOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_epochs = 3\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n📚 Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = trainer.train_epoch(\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    print(\"\\n🔍 Validation...\")\n",
    "    student_model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text']\n",
    "            \n",
    "            # Student forward\n",
    "            student_inputs = student_tokenizer(\n",
    "                texts,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(trainer.device)\n",
    "            \n",
    "            outputs = student_model(**student_inputs, labels=student_inputs['input_ids'])\n",
    "            val_losses.append(outputs.loss.item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f\"   Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Best model saving\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(f\"   🏆 Yeni en iyi model! (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        # Model'i kaydet\n",
    "        save_path = f\"./turkcell_distilled_epoch{epoch+1}\"\n",
    "        student_model.save_pretrained(save_path)\n",
    "        student_tokenizer.save_pretrained(save_path)\n",
    "        print(f\"   💾 Model kaydedildi: {save_path}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ TRAINING TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n📊 Final Sonuçlar:\")\n",
    "print(f\"   Best validation loss: {best_loss:.4f}\")\n",
    "print(f\"   Model saved to: ./turkcell_distilled_epoch{epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 6. Model Test ve Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8):\n",
    "    \"\"\"Model ile metin üret\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Yapay zeka nedir?\",\n",
    "    \"Python programlama dilinin avantajları\",\n",
    "    \"Matematik öğrenmenin önemi\",\n",
    "    \"Türkiye'de teknoloji eğitimi\"\n",
    "]\n",
    "\n",
    "print(\"\\n🧪 Model Test Sonuçları\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n📝 Test {i}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Teacher model çıktısı\n",
    "    print(\"👨‍🏫 Teacher (Turkcell):\")\n",
    "    teacher_output = generate_text(teacher_model, teacher_tokenizer, prompt, max_length=50)\n",
    "    print(teacher_output)\n",
    "    \n",
    "    # Student model çıktısı\n",
    "    print(\"\\n👨‍🎓 Student (Distilled):\")\n",
    "    student_output = generate_text(student_model, student_tokenizer, prompt, max_length=50)\n",
    "    print(student_output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"✅ Test tamamlandı!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 7. Model Export ve Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model'i Hugging Face formatında kaydet\n",
    "final_save_path = \"./turkcell_distilled_final\"\n",
    "\n",
    "print(\"\\n💾 Final model kaydediliyor...\")\n",
    "\n",
    "# Student model'i kaydet\n",
    "student_model.save_pretrained(final_save_path)\n",
    "student_tokenizer.save_pretrained(final_save_path)\n",
    "\n",
    "# Training config'i kaydet\n",
    "import json\n",
    "\n",
    "training_config = {\n",
    "    \"teacher_model\": teacher_config.model_id,\n",
    "    \"student_model\": student_config.model_id,\n",
    "    \"temperature\": teacher_config.temperature,\n",
    "    \"alpha\": 0.7,\n",
    "    \"batch_size\": teacher_config.batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"best_loss\": float(best_loss),\n",
    "    \"quantization\": teacher_config.quantization\n",
    "}\n",
    "\n",
    "with open(f\"{final_save_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"✅ Model kaydedildi: {final_save_path}\")\n",
    "print(\"\\n📦 Model içeriği:\")\n",
    "print(f\"   - Model weights\")\n",
    "print(f\"   - Tokenizer files\")\n",
    "print(f\"   - Training config\")\n",
    "\n",
    "# Model boyutu\n",
    "import os\n",
    "model_size = sum(\n",
    "    os.path.getsize(os.path.join(final_save_path, f))\n",
    "    for f in os.listdir(final_save_path)\n",
    "    if os.path.isfile(os.path.join(final_save_path, f))\n",
    ") / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"\\n📊 Model boyutu: {model_size:.2f} MB\")\n",
    "\n",
    "# Deployment önerileri\n",
    "print(\"\\n🚀 Deployment Önerileri:\")\n",
    "print(\"   1. Hugging Face Hub'a yükleyin:\")\n",
    "print(\"      ```python\")\n",
    "print(\"      from huggingface_hub import HfApi\")\n",
    "print(\"      api = HfApi()\")\n",
    "print(\"      api.upload_folder(\")\n",
    "print(f\"          folder_path='{final_save_path}',\")\n",
    "print(\"          repo_id='username/turkcell-distilled',\")\n",
    "print(\"          repo_type='model'\")\n",
    "print(\"      )\")\n",
    "print(\"      ```\")\n",
    "print(\"   2. ONNX formatına dönüştürün (hızlı inference)\")\n",
    "print(\"   3. TorchScript ile optimize edin\")\n",
    "print(\"   4. API endpoint olarak servis edin\")\n",
    "\n",
    "print(\"\\n✅ Pipeline tamamlandı!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}