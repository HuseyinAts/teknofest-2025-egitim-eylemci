{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Turkcell-LLM-7B Knowledge Distillation Pipeline\n",
    "## T√ºrk√ße Eƒüitim Asistanƒ± i√ßin Optimize Edilmi≈ü Bilgi Damƒ±tma\n",
    "\n",
    "Bu notebook, **Turkcell-LLM-7b-v1** teacher model kullanarak T√ºrk√ße i√ßin optimize edilmi≈ü knowledge distillation pipeline'ƒ± i√ßerir.\n",
    "\n",
    "### ‚úÖ √ñzellikler:\n",
    "- Turkcell-LLM-7b-v1 teacher model\n",
    "- A100 GPU i√ßin optimize edilmi≈ü\n",
    "- Mixed precision training (bf16)\n",
    "- Gradient checkpointing\n",
    "- Curriculum learning\n",
    "- T√ºrk√ße'ye √∂zel metrikler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sistem kontrol√º ve setup\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üéØ TURKCELL-LLM-7B TEACHER MODEL SETUP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# GPU kontrol√º\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"‚úÖ GPU: {gpu_name}\")\n",
    "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚ö° Compute Capability: {torch.cuda.get_device_properties(0).major}.{torch.cuda.get_device_properties(0).minor}\")\n",
    "    \n",
    "    # A100 optimizasyonlarƒ±\n",
    "    if 'A100' in gpu_name:\n",
    "        print(\"\\nüöÄ A100 GPU tespit edildi - Optimizasyonlar aktif:\")\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"  ‚úì TF32 precision aktif\")\n",
    "        print(\"  ‚úì Flash Attention 2 kullanƒ±labilir\")\n",
    "        OPTIMAL_BATCH_SIZE = 16\n",
    "    else:\n",
    "        OPTIMAL_BATCH_SIZE = 8\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU bulunamadƒ±\")\n",
    "    OPTIMAL_BATCH_SIZE = 4\n",
    "\n",
    "print(f\"\\nüìä √ñnerilen batch size: {OPTIMAL_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerekli k√ºt√ºphaneleri y√ºkle\n",
    "!pip install -q transformers>=4.36.0 accelerate>=0.25.0 bitsandbytes>=0.41.3 peft>=0.7.1\n",
    "!pip install -q datasets evaluate nltk rouge-score sacrebleu bert-score\n",
    "!pip install -q sentencepiece protobuf ftfy langdetect\n",
    "!pip install -q wandb  # Opsiyonel: Experiment tracking\n",
    "\n",
    "print(\"‚úÖ T√ºm k√ºt√ºphaneler y√ºklendi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö 1. Turkcell Teacher Model Y√ºkleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from dataclasses import dataclass\n",
    "import gc\n",
    "\n",
    "@dataclass\n",
    "class TurkcellTeacherConfig:\n",
    "    \"\"\"Turkcell Teacher Model Konfig√ºrasyonu\"\"\"\n",
    "    model_id: str = \"TURKCELL/Turkcell-LLM-7b-v1\"\n",
    "    quantization: str = \"8bit\"  # 4bit, 8bit, none\n",
    "    use_flash_attention: bool = True\n",
    "    max_length: int = 2048\n",
    "    temperature: float = 3.0  # Distillation temperature\n",
    "    batch_size: int = 16\n",
    "    \n",
    "def load_turkcell_teacher(config: TurkcellTeacherConfig):\n",
    "    \"\"\"Turkcell-LLM-7b-v1 modelini optimize ≈üekilde y√ºkle\"\"\"\n",
    "    \n",
    "    print(f\"\\nüì• Turkcell Teacher Model y√ºkleniyor...\")\n",
    "    print(f\"   Model: {config.model_id}\")\n",
    "    print(f\"   Quantization: {config.quantization}\")\n",
    "    \n",
    "    # Quantization config\n",
    "    if config.quantization == \"4bit\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"   ‚úì 4-bit quantization aktif (En d√º≈ü√ºk bellek)\")\n",
    "    elif config.quantization == \"8bit\":\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "            bnb_8bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        print(\"   ‚úì 8-bit quantization aktif (Dengeli kalite-bellek)\")\n",
    "    else:\n",
    "        bnb_config = None\n",
    "        print(\"   ‚úì Full precision (Maksimum kalite)\")\n",
    "    \n",
    "    # Model y√ºkleme arg√ºmanlarƒ±\n",
    "    model_kwargs = {\n",
    "        \"device_map\": \"auto\",\n",
    "        \"trust_remote_code\": True,\n",
    "        \"torch_dtype\": torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    }\n",
    "    \n",
    "    if bnb_config:\n",
    "        model_kwargs[\"quantization_config\"] = bnb_config\n",
    "        \n",
    "    # Flash Attention 2 (A100 i√ßin)\n",
    "    if config.use_flash_attention and torch.cuda.is_available():\n",
    "        try:\n",
    "            model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "            print(\"   ‚úì Flash Attention 2 aktif\")\n",
    "        except:\n",
    "            print(\"   ‚ö†Ô∏è Flash Attention 2 kullanƒ±lamƒ±yor\")\n",
    "    \n",
    "    # Model y√ºkle\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            config.model_id,\n",
    "            **model_kwargs\n",
    "        )\n",
    "        \n",
    "        # Tokenizer y√ºkle\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            config.model_id,\n",
    "            trust_remote_code=True,\n",
    "            use_fast=True  # Fast tokenizer\n",
    "        )\n",
    "        \n",
    "        # Padding token ayarla\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "        # Model bilgileri\n",
    "        total_params = sum(p.numel() for p in model.parameters()) / 1e9\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad) / 1e9\n",
    "        \n",
    "        print(f\"\\n‚úÖ Turkcell Teacher Model ba≈üarƒ±yla y√ºklendi!\")\n",
    "        print(f\"   Total parameters: {total_params:.2f}B\")\n",
    "        print(f\"   Trainable parameters: {trainable_params:.2f}B\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   GPU Memory used: {memory_used:.2f}GB\")\n",
    "        \n",
    "        # Model'i eval moduna al\n",
    "        model.eval()\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        print(\"   ‚úì Model evaluation modunda ve donduruldu\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Model y√ºkleme hatasƒ±: {e}\")\n",
    "        print(\"\\nüí° √á√∂z√ºm √∂nerileri:\")\n",
    "        print(\"   1. Quantization seviyesini artƒ±rƒ±n (8bit -> 4bit)\")\n",
    "        print(\"   2. Batch size'ƒ± azaltƒ±n\")\n",
    "        print(\"   3. Gradient checkpointing kullanƒ±n\")\n",
    "        return None, None\n",
    "\n",
    "# Teacher model'i y√ºkle\n",
    "teacher_config = TurkcellTeacherConfig(\n",
    "    quantization=\"8bit\",  # A100 40GB i√ßin 8-bit yeterli\n",
    "    use_flash_attention=True,\n",
    "    batch_size=OPTIMAL_BATCH_SIZE\n",
    ")\n",
    "\n",
    "teacher_model, teacher_tokenizer = load_turkcell_teacher(teacher_config)\n",
    "\n",
    "# Bellek temizleme\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì 2. Student Model Se√ßimi ve Y√ºkleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "@dataclass\n",
    "class StudentModelConfig:\n",
    "    \"\"\"Student Model Konfig√ºrasyonu\"\"\"\n",
    "    model_id: str = \"ytu-ce-cosmos/turkish-gpt2-large\"  # Hafif T√ºrk√ße model\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 64\n",
    "    lora_alpha: int = 128\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: list = None\n",
    "    \n",
    "def load_student_model(config: StudentModelConfig):\n",
    "    \"\"\"Student model'i LoRA ile y√ºkle\"\"\"\n",
    "    \n",
    "    print(f\"\\nüì• Student Model y√ºkleniyor...\")\n",
    "    print(f\"   Model: {config.model_id}\")\n",
    "    \n",
    "    # Student model - daha k√º√ß√ºk\n",
    "    student_model = AutoModelForCausalLM.from_pretrained(\n",
    "        config.model_id,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    student_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        config.model_id,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Padding token\n",
    "    if student_tokenizer.pad_token is None:\n",
    "        student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "        student_tokenizer.pad_token_id = student_tokenizer.eos_token_id\n",
    "    \n",
    "    # LoRA configuration\n",
    "    if config.use_lora:\n",
    "        print(\"\\nüîß LoRA konfig√ºrasyonu uygulanƒ±yor...\")\n",
    "        \n",
    "        # Model'i LoRA i√ßin hazƒ±rla\n",
    "        student_model = prepare_model_for_kbit_training(student_model)\n",
    "        \n",
    "        # LoRA config\n",
    "        lora_config = LoraConfig(\n",
    "            r=config.lora_r,\n",
    "            lora_alpha=config.lora_alpha,\n",
    "            target_modules=config.target_modules or [\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=config.lora_dropout,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # LoRA uygula\n",
    "        student_model = get_peft_model(student_model, lora_config)\n",
    "        \n",
    "        # LoRA istatistikleri\n",
    "        trainable_params = sum(p.numel() for p in student_model.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in student_model.parameters())\n",
    "        \n",
    "        print(f\"\\n‚úÖ Student Model (LoRA) hazƒ±r!\")\n",
    "        print(f\"   Total parameters: {all_params/1e6:.2f}M\")\n",
    "        print(f\"   Trainable parameters: {trainable_params/1e6:.2f}M\")\n",
    "        print(f\"   Trainable %: {100 * trainable_params / all_params:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Student Model (Full Fine-tuning) hazƒ±r!\")\n",
    "        \n",
    "    # Gradient checkpointing\n",
    "    if hasattr(student_model, 'gradient_checkpointing_enable'):\n",
    "        student_model.gradient_checkpointing_enable()\n",
    "        print(\"   ‚úì Gradient checkpointing aktif\")\n",
    "        \n",
    "    return student_model, student_tokenizer\n",
    "\n",
    "# Student model alternatifler\n",
    "STUDENT_MODEL_OPTIONS = {\n",
    "    \"turkish-gpt2\": \"ytu-ce-cosmos/turkish-gpt2-large\",\n",
    "    \"turkish-bert-gpt2\": \"redrussianarmy/turkish-bert-gpt2\",\n",
    "    \"gpt2-small-turkish\": \"ytu-ce-cosmos/turkish-gpt2-small\",\n",
    "    \"distilgpt2-turkish\": \"dbmdz/distilbert-base-turkish-cased\"  # Encoder model\n",
    "}\n",
    "\n",
    "print(\"üìã Mevcut Student Model Se√ßenekleri:\")\n",
    "for key, value in STUDENT_MODEL_OPTIONS.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# Student model'i y√ºkle\n",
    "student_config = StudentModelConfig(\n",
    "    model_id=STUDENT_MODEL_OPTIONS[\"turkish-gpt2\"],\n",
    "    use_lora=True,\n",
    "    lora_r=64,\n",
    "    lora_alpha=128\n",
    ")\n",
    "\n",
    "student_model, student_tokenizer = load_student_model(student_config)\n",
    "\n",
    "# Bellek durumu\n",
    "if torch.cuda.is_available():\n",
    "    memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "    memory_reserved = torch.cuda.memory_reserved() / 1e9\n",
    "    print(f\"\\nüíæ GPU Bellek Durumu:\")\n",
    "    print(f\"   Kullanƒ±lan: {memory_used:.2f}GB\")\n",
    "    print(f\"   Rezerve: {memory_reserved:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 3. Knowledge Distillation Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class TurkcellDistillationTrainer:\n",
    "    \"\"\"Turkcell Teacher Model i√ßin Knowledge Distillation Trainer\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 teacher_model,\n",
    "                 student_model,\n",
    "                 teacher_tokenizer,\n",
    "                 student_tokenizer,\n",
    "                 temperature: float = 3.0,\n",
    "                 alpha: float = 0.7,\n",
    "                 max_length: int = 512):\n",
    "        \n",
    "        self.teacher_model = teacher_model\n",
    "        self.student_model = student_model\n",
    "        self.teacher_tokenizer = teacher_tokenizer\n",
    "        self.student_tokenizer = student_tokenizer\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha  # KD loss weight\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Device\n",
    "        self.device = next(student_model.parameters()).device\n",
    "        \n",
    "        # Mixed precision scaler\n",
    "        self.use_amp = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "        if self.use_amp:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"‚úÖ Mixed precision (BF16) training aktif\")\n",
    "            \n",
    "        # Metrics tracking\n",
    "        self.training_history = []\n",
    "        \n",
    "    def compute_distillation_loss(self, \n",
    "                                 student_logits: torch.Tensor,\n",
    "                                 teacher_logits: torch.Tensor,\n",
    "                                 labels: torch.Tensor,\n",
    "                                 attention_mask: torch.Tensor = None):\n",
    "        \"\"\"Distillation loss hesaplama\"\"\"\n",
    "        \n",
    "        # Reshape logits\n",
    "        vocab_size = student_logits.size(-1)\n",
    "        student_logits_view = student_logits.view(-1, vocab_size)\n",
    "        teacher_logits_view = teacher_logits.view(-1, teacher_logits.size(-1))\n",
    "        labels_view = labels.view(-1)\n",
    "        \n",
    "        # Mask invalid positions\n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.view(-1) == 1\n",
    "            student_logits_view = student_logits_view[mask]\n",
    "            teacher_logits_view = teacher_logits_view[mask]\n",
    "            labels_view = labels_view[mask]\n",
    "        \n",
    "        # KL Divergence loss (soft targets)\n",
    "        kd_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits_view / self.temperature, dim=-1),\n",
    "            F.softmax(teacher_logits_view / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        # Cross entropy loss (hard targets)\n",
    "        ce_loss = F.cross_entropy(\n",
    "            student_logits_view,\n",
    "            labels_view,\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Combined loss\n",
    "        total_loss = self.alpha * kd_loss + (1 - self.alpha) * ce_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss,\n",
    "            'kd_loss': kd_loss.item(),\n",
    "            'ce_loss': ce_loss.item()\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_teacher_outputs(self, input_ids, attention_mask):\n",
    "        \"\"\"Teacher model √ßƒ±ktƒ±larƒ±nƒ± al\"\"\"\n",
    "        outputs = self.teacher_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        return outputs.logits\n",
    "    \n",
    "    def train_step(self, batch, optimizer, scheduler=None):\n",
    "        \"\"\"Tek training adƒ±mƒ±\"\"\"\n",
    "        \n",
    "        # Prepare inputs\n",
    "        texts = batch['text'] if isinstance(batch, dict) else batch\n",
    "        \n",
    "        # Teacher tokenization\n",
    "        teacher_inputs = self.teacher_tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Student tokenization\n",
    "        student_inputs = self.student_tokenizer(\n",
    "            texts,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Get teacher logits (no grad)\n",
    "        teacher_logits = self.get_teacher_outputs(\n",
    "            teacher_inputs['input_ids'],\n",
    "            teacher_inputs['attention_mask']\n",
    "        )\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if self.use_amp:\n",
    "            with torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "                student_outputs = self.student_model(\n",
    "                    input_ids=student_inputs['input_ids'],\n",
    "                    attention_mask=student_inputs['attention_mask'],\n",
    "                    labels=student_inputs['input_ids']\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss_dict = self.compute_distillation_loss(\n",
    "                    student_outputs.logits,\n",
    "                    teacher_logits,\n",
    "                    student_inputs['input_ids'],\n",
    "                    student_inputs['attention_mask']\n",
    "                )\n",
    "        else:\n",
    "            student_outputs = self.student_model(\n",
    "                input_ids=student_inputs['input_ids'],\n",
    "                attention_mask=student_inputs['attention_mask'],\n",
    "                labels=student_inputs['input_ids']\n",
    "            )\n",
    "            \n",
    "            loss_dict = self.compute_distillation_loss(\n",
    "                student_outputs.logits,\n",
    "                teacher_logits,\n",
    "                student_inputs['input_ids'],\n",
    "                student_inputs['attention_mask']\n",
    "            )\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = loss_dict['loss']\n",
    "        \n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), max_norm=1.0)\n",
    "            self.scaler.step(optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Clear cache periodically\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return loss_dict\n",
    "    \n",
    "    def train_epoch(self, dataloader, optimizer, scheduler=None, epoch=0):\n",
    "        \"\"\"Bir epoch training\"\"\"\n",
    "        \n",
    "        self.student_model.train()\n",
    "        epoch_losses = []\n",
    "        \n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(progress_bar):\n",
    "            try:\n",
    "                loss_dict = self.train_step(batch, optimizer, scheduler)\n",
    "                epoch_losses.append(loss_dict)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss_dict['loss']:.4f}\",\n",
    "                    'kd': f\"{loss_dict['kd_loss']:.4f}\",\n",
    "                    'ce': f\"{loss_dict['ce_loss']:.4f}\"\n",
    "                })\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e):\n",
    "                    print(\"\\n‚ö†Ô∏è CUDA OOM! Batch'i atlayƒ±p devam ediliyor...\")\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                    continue\n",
    "                else:\n",
    "                    raise e\n",
    "        \n",
    "        # Epoch statistics\n",
    "        avg_loss = np.mean([d['loss'] for d in epoch_losses])\n",
    "        avg_kd_loss = np.mean([d['kd_loss'] for d in epoch_losses])\n",
    "        avg_ce_loss = np.mean([d['ce_loss'] for d in epoch_losses])\n",
    "        \n",
    "        print(f\"\\nüìä Epoch {epoch+1} √ñzeti:\")\n",
    "        print(f\"   Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"   KD Loss: {avg_kd_loss:.4f}\")\n",
    "        print(f\"   CE Loss: {avg_ce_loss:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'epoch': epoch + 1,\n",
    "            'avg_loss': avg_loss,\n",
    "            'avg_kd_loss': avg_kd_loss,\n",
    "            'avg_ce_loss': avg_ce_loss\n",
    "        }\n",
    "\n",
    "# Trainer'ƒ± olu≈ütur\n",
    "trainer = TurkcellDistillationTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    student_model=student_model,\n",
    "    teacher_tokenizer=teacher_tokenizer,\n",
    "    student_tokenizer=student_tokenizer,\n",
    "    temperature=teacher_config.temperature,\n",
    "    alpha=0.7,  # %70 KD, %30 CE\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Turkcell Distillation Trainer hazƒ±r!\")\n",
    "print(f\"   Temperature: {trainer.temperature}\")\n",
    "print(f\"   Alpha (KD weight): {trainer.alpha}\")\n",
    "print(f\"   Max length: {trainer.max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä 4. T√ºrk√ße Veri Y√ºkleme ve Hazƒ±rlama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class TurkishEducationDataset(Dataset):\n",
    "    \"\"\"T√ºrk√ße eƒüitim veri seti\"\"\"\n",
    "    \n",
    "    def __init__(self, data, tokenizer=None, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        if isinstance(item, dict):\n",
    "            text = item.get('text', '') or item.get('content', '')\n",
    "        else:\n",
    "            text = str(item)\n",
    "            \n",
    "        return {'text': text}\n",
    "\n",
    "def load_turkish_data(source=\"demo\", max_samples=1000):\n",
    "    \"\"\"T√ºrk√ße veri y√ºkle\"\"\"\n",
    "    \n",
    "    print(f\"\\nüì• T√ºrk√ße veri y√ºkleniyor: {source}\")\n",
    "    \n",
    "    if source == \"demo\":\n",
    "        # Demo veri olu≈ütur\n",
    "        demo_texts = [\n",
    "            \"Yapay zeka, insan zekasƒ±nƒ± taklit eden bilgisayar sistemleridir.\",\n",
    "            \"Python programlama dili, veri bilimi i√ßin pop√ºler bir ara√ßtƒ±r.\",\n",
    "            \"Makine √∂ƒürenmesi, verilerden √∂r√ºnt√º √ßƒ±karma s√ºrecidir.\",\n",
    "            \"Derin √∂ƒürenme, yapay sinir aƒülarƒ±nƒ± kullanan bir makine √∂ƒürenmesi y√∂ntemidir.\",\n",
    "            \"T√ºrkiye'de teknoloji eƒüitimi hƒ±zla geli≈ümektedir.\",\n",
    "            \"Matematik, bilimin temelidir ve problem √ß√∂zme becerilerini geli≈ütirir.\",\n",
    "            \"Fizik, doƒüadaki olaylarƒ± a√ßƒ±klayan temel bilimlerden biridir.\",\n",
    "            \"Kimya, maddenin yapƒ±sƒ±nƒ± ve √∂zelliklerini inceler.\",\n",
    "            \"Biyoloji, canlƒ±larƒ±n yapƒ±sƒ±nƒ± ve ya≈üam s√ºre√ßlerini ara≈ütƒ±rƒ±r.\",\n",
    "            \"Tarih, ge√ßmi≈üi anlamamƒ±za ve geleceƒüi ≈üekillendirmemize yardƒ±mcƒ± olur.\"\n",
    "        ]\n",
    "        \n",
    "        # Veriyi √ßoƒüalt\n",
    "        data = demo_texts * (max_samples // len(demo_texts) + 1)\n",
    "        data = data[:max_samples]\n",
    "        \n",
    "        print(f\"‚úÖ {len(data)} demo √∂rnek olu≈üturuldu\")\n",
    "        \n",
    "    elif source == \"huggingface\":\n",
    "        # Hugging Face'den T√ºrk√ße veri y√ºkle\n",
    "        try:\n",
    "            dataset = load_dataset(\n",
    "                \"ytu-ce-cosmos/turkish-qa\",  # T√ºrk√ße Q&A dataset\n",
    "                split=f\"train[:{max_samples}]\"\n",
    "            )\n",
    "            data = [item['text'] for item in dataset]\n",
    "            print(f\"‚úÖ {len(data)} √∂rnek Hugging Face'den y√ºklendi\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Hugging Face y√ºkleme hatasƒ±: {e}\")\n",
    "            print(\"Demo veri kullanƒ±lƒ±yor...\")\n",
    "            return load_turkish_data(\"demo\", max_samples)\n",
    "            \n",
    "    else:\n",
    "        # CSV'den y√ºkle\n",
    "        try:\n",
    "            df = pd.read_csv(source)\n",
    "            data = df['text'].tolist()[:max_samples]\n",
    "            print(f\"‚úÖ {len(data)} √∂rnek CSV'den y√ºklendi\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è CSV y√ºkleme hatasƒ±: {e}\")\n",
    "            return load_turkish_data(\"demo\", max_samples)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Veri y√ºkle\n",
    "train_data = load_turkish_data(\"demo\", max_samples=100)\n",
    "val_data = load_turkish_data(\"demo\", max_samples=20)\n",
    "\n",
    "# Dataset olu≈ütur\n",
    "train_dataset = TurkishEducationDataset(train_data)\n",
    "val_dataset = TurkishEducationDataset(val_data)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=teacher_config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=teacher_config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Veri √ñzeti:\")\n",
    "print(f\"   Train samples: {len(train_dataset)}\")\n",
    "print(f\"   Val samples: {len(val_dataset)}\")\n",
    "print(f\"   Batch size: {teacher_config.batch_size}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ 5. Training Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer ve Scheduler setup\n",
    "optimizer = AdamW(\n",
    "    student_model.parameters(),\n",
    "    lr=2e-5,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * 3  # 3 epochs\n",
    "warmup_steps = int(0.1 * total_steps)  # %10 warmup\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Training Konfig√ºrasyonu:\")\n",
    "print(f\"   Learning rate: {2e-5}\")\n",
    "print(f\"   Total steps: {total_steps}\")\n",
    "print(f\"   Warmup steps: {warmup_steps}\")\n",
    "print(f\"   Epochs: 3\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ TRAINING BA≈ûLIYOR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "num_epochs = 3\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nüìö Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_metrics = trainer.train_epoch(\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        epoch\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    print(\"\\nüîç Validation...\")\n",
    "    student_model.eval()\n",
    "    val_losses = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch['text']\n",
    "            \n",
    "            # Student forward\n",
    "            student_inputs = student_tokenizer(\n",
    "                texts,\n",
    "                max_length=512,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            ).to(trainer.device)\n",
    "            \n",
    "            outputs = student_model(**student_inputs, labels=student_inputs['input_ids'])\n",
    "            val_losses.append(outputs.loss.item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f\"   Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    # Best model saving\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        print(f\"   üèÜ Yeni en iyi model! (loss: {best_loss:.4f})\")\n",
    "        \n",
    "        # Model'i kaydet\n",
    "        save_path = f\"./turkcell_distilled_epoch{epoch+1}\"\n",
    "        student_model.save_pretrained(save_path)\n",
    "        student_tokenizer.save_pretrained(save_path)\n",
    "        print(f\"   üíæ Model kaydedildi: {save_path}\")\n",
    "    \n",
    "    # Memory cleanup\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ TRAINING TAMAMLANDI!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Final Sonu√ßlar:\")\n",
    "print(f\"   Best validation loss: {best_loss:.4f}\")\n",
    "print(f\"   Model saved to: ./turkcell_distilled_epoch{epoch+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ 6. Model Test ve Deƒüerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_length=100, temperature=0.8):\n",
    "    \"\"\"Model ile metin √ºret\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.95,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Yapay zeka nedir?\",\n",
    "    \"Python programlama dilinin avantajlarƒ±\",\n",
    "    \"Matematik √∂ƒürenmenin √∂nemi\",\n",
    "    \"T√ºrkiye'de teknoloji eƒüitimi\"\n",
    "]\n",
    "\n",
    "print(\"\\nüß™ Model Test Sonu√ßlarƒ±\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}: {prompt}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Teacher model √ßƒ±ktƒ±sƒ±\n",
    "    print(\"üë®‚Äçüè´ Teacher (Turkcell):\")\n",
    "    teacher_output = generate_text(teacher_model, teacher_tokenizer, prompt, max_length=50)\n",
    "    print(teacher_output)\n",
    "    \n",
    "    # Student model √ßƒ±ktƒ±sƒ±\n",
    "    print(\"\\nüë®‚Äçüéì Student (Distilled):\")\n",
    "    student_output = generate_text(student_model, student_tokenizer, prompt, max_length=50)\n",
    "    print(student_output)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ Test tamamlandƒ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ 7. Model Export ve Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model'i Hugging Face formatƒ±nda kaydet\n",
    "final_save_path = \"./turkcell_distilled_final\"\n",
    "\n",
    "print(\"\\nüíæ Final model kaydediliyor...\")\n",
    "\n",
    "# Student model'i kaydet\n",
    "student_model.save_pretrained(final_save_path)\n",
    "student_tokenizer.save_pretrained(final_save_path)\n",
    "\n",
    "# Training config'i kaydet\n",
    "import json\n",
    "\n",
    "training_config = {\n",
    "    \"teacher_model\": teacher_config.model_id,\n",
    "    \"student_model\": student_config.model_id,\n",
    "    \"temperature\": teacher_config.temperature,\n",
    "    \"alpha\": 0.7,\n",
    "    \"batch_size\": teacher_config.batch_size,\n",
    "    \"epochs\": num_epochs,\n",
    "    \"best_loss\": float(best_loss),\n",
    "    \"quantization\": teacher_config.quantization\n",
    "}\n",
    "\n",
    "with open(f\"{final_save_path}/training_config.json\", \"w\") as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Model kaydedildi: {final_save_path}\")\n",
    "print(\"\\nüì¶ Model i√ßeriƒüi:\")\n",
    "print(f\"   - Model weights\")\n",
    "print(f\"   - Tokenizer files\")\n",
    "print(f\"   - Training config\")\n",
    "\n",
    "# Model boyutu\n",
    "import os\n",
    "model_size = sum(\n",
    "    os.path.getsize(os.path.join(final_save_path, f))\n",
    "    for f in os.listdir(final_save_path)\n",
    "    if os.path.isfile(os.path.join(final_save_path, f))\n",
    ") / (1024 * 1024)  # MB\n",
    "\n",
    "print(f\"\\nüìä Model boyutu: {model_size:.2f} MB\")\n",
    "\n",
    "# Deployment √∂nerileri\n",
    "print(\"\\nüöÄ Deployment √ñnerileri:\")\n",
    "print(\"   1. Hugging Face Hub'a y√ºkleyin:\")\n",
    "print(\"      ```python\")\n",
    "print(\"      from huggingface_hub import HfApi\")\n",
    "print(\"      api = HfApi()\")\n",
    "print(\"      api.upload_folder(\")\n",
    "print(f\"          folder_path='{final_save_path}',\")\n",
    "print(\"          repo_id='username/turkcell-distilled',\")\n",
    "print(\"          repo_type='model'\")\n",
    "print(\"      )\")\n",
    "print(\"      ```\")\n",
    "print(\"   2. ONNX formatƒ±na d√∂n√º≈üt√ºr√ºn (hƒ±zlƒ± inference)\")\n",
    "print(\"   3. TorchScript ile optimize edin\")\n",
    "print(\"   4. API endpoint olarak servis edin\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline tamamlandƒ±!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}