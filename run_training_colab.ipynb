{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QWEN3 Turkish Training - Quick Start\\n",
        "\\nBu notebook QWEN3 modelini TÃ¼rkÃ§e veri seti ile eÄŸitir."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab kontrolÃ¼ ve Drive mount\\n",
        "import sys\\n",
        "IS_COLAB = 'google.colab' in sys.modules\\n",
        "\\n",
        "if IS_COLAB:\\n",
        "    from google.colab import drive\\n",
        "    drive.mount('/content/drive')\\n",
        "    print('âœ… Google Drive mounted')\\n",
        "else:\\n",
        "    print('ðŸ’» Running locally')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gerekli paketleri yÃ¼kle\\n",
        "if IS_COLAB:\\n",
        "    !pip install -q torch transformers datasets accelerate bitsandbytes\\n",
        "    !pip install -q peft trl wandb sentencepiece protobuf\\n",
        "    print('âœ… Packages installed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training script'ini yÃ¼kle ve Ã§alÄ±ÅŸtÄ±r\\n",
        "import os\\n",
        "\\n",
        "# Script'i GitHub'dan indir\\n",
        "if IS_COLAB:\\n",
        "    !wget -q https://raw.githubusercontent.com/HuseyinAts/teknofest-2025-egitim-eylemci/main/notebooks/qwen3_training_production_v4_fixed.py -O /content/training_script.py\\n",
        "    print('âœ… Training script downloaded')\\n",
        "    \\n",
        "    # Script'i Ã§alÄ±ÅŸtÄ±r\\n",
        "    exec(open('/content/training_script.py').read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternatif: Manuel olarak training baÅŸlat\\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\\n",
        "from datasets import load_dataset\\n",
        "import torch\\n",
        "\\n",
        "# Model ve tokenizer yÃ¼kle\\n",
        "model_name = 'Qwen/Qwen2.5-0.5B'  # KÃ¼Ã§Ã¼k model test iÃ§in\\n",
        "\\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\\n",
        "model = AutoModelForCausalLM.from_pretrained(\\n",
        "    model_name,\\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\\n",
        "    device_map='auto'\\n",
        ")\\n",
        "\\n",
        "print(f'âœ… Model loaded: {model_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Veri setini yÃ¼kle\\n",
        "dataset = load_dataset('Huseyin/turkish-200k-dataset', split='train[:1000]')  # Ä°lk 1000 Ã¶rnek\\n",
        "\\n",
        "# Tokenize fonksiyonu\\n",
        "def tokenize_function(examples):\\n",
        "    return tokenizer(\\n",
        "        examples['text'],\\n",
        "        padding='max_length',\\n",
        "        truncation=True,\\n",
        "        max_length=512\\n",
        "    )\\n",
        "\\n",
        "# Veriyi tokenize et\\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\\n",
        "\\n",
        "# Train/test split\\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.1)\\n",
        "train_dataset = split_dataset['train']\\n",
        "eval_dataset = split_dataset['test']\\n",
        "\\n",
        "print(f'âœ… Dataset ready: {len(train_dataset)} train, {len(eval_dataset)} eval')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training ayarlarÄ±\\n",
        "training_args = TrainingArguments(\\n",
        "    output_dir='/content/drive/MyDrive/qwen_checkpoints',\\n",
        "    num_train_epochs=1,\\n",
        "    per_device_train_batch_size=4,\\n",
        "    per_device_eval_batch_size=4,\\n",
        "    warmup_steps=100,\\n",
        "    weight_decay=0.01,\\n",
        "    logging_dir='/content/logs',\\n",
        "    logging_steps=10,\\n",
        "    save_steps=500,\\n",
        "    eval_steps=100,\\n",
        "    evaluation_strategy='steps',\\n",
        "    save_strategy='steps',\\n",
        "    load_best_model_at_end=True,\\n",
        "    fp16=torch.cuda.is_available(),\\n",
        ")\\n",
        "\\n",
        "# Trainer oluÅŸtur\\n",
        "trainer = Trainer(\\n",
        "    model=model,\\n",
        "    args=training_args,\\n",
        "    train_dataset=train_dataset,\\n",
        "    eval_dataset=eval_dataset,\\n",
        "    tokenizer=tokenizer,\\n",
        ")\\n",
        "\\n",
        "print('âœ… Trainer ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training baÅŸlat\\n",
        "print('ðŸš€ Starting training...')\\n",
        "trainer.train()\\n",
        "\\n",
        "# Modeli kaydet\\n",
        "trainer.save_model('/content/drive/MyDrive/qwen_final_model')\\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/qwen_final_model')\\n",
        "\\n",
        "print('âœ… Training complete! Model saved to Drive.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test et\\n",
        "def generate_text(prompt):\\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\\n",
        "    outputs = model.generate(\\n",
        "        **inputs,\\n",
        "        max_length=100,\\n",
        "        temperature=0.7,\\n",
        "        do_sample=True,\\n",
        "        top_p=0.95\\n",
        "    )\\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\n",
        "\\n",
        "# Test Ã¶rnekleri\\n",
        "test_prompts = [\\n",
        "    'TÃ¼rkiye\\'nin baÅŸkenti',\\n",
        "    'Yapay zeka nedir?',\\n",
        "    'Matematik Ã§alÄ±ÅŸmak iÃ§in'\\n",
        "]\\n",
        "\\n",
        "for prompt in test_prompts:\\n",
        "    print(f'\\\\nPrompt: {prompt}')\\n",
        "    print(f'Response: {generate_text(prompt)}')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}