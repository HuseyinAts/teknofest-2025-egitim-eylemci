#!/usr/bin/env python3
"""
üî• GOOGLE COLAB ULTRA SINGLE VARIANT EXECUTION
DoRA + NEFTune + Sophia Ultimate - Optimized for Colab Environment
Target: 98% Success | Loss: 1.1 | Time: 5h | 27.5K Samples
"""

import json
import os
import sys
import torch
import time
import threading
import gc
import subprocess
from datetime import datetime, timedelta
from pathlib import Path

# Google Colab optimized directory structure - per user memory
BASE_DIR = '/content/teknofest-2025-egitim-eylemci'
WORKSPACE = f'{BASE_DIR}/turkish_tokenizer'
sys.path.append(BASE_DIR)

# Google Colab session timeout protection
try:
    from IPython.display import Javascript, display
    display(Javascript('''
        function ClickConnect(){
            console.log("Colab session kept alive...");
            if(document.querySelector("colab-connect-button")){
                document.querySelector("colab-connect-button").click()
            }
        }
        setInterval(ClickConnect,60000)
    '''))
    print("‚úÖ Google Colab session protection activated")
except:
    print("‚ÑπÔ∏è Running outside Colab environment")

def print_colab_header():
    """Google Colab optimized header"""
    print("\n" + "üî•" * 80)
    print("üöÄ GOOGLE COLAB ULTRA T√úRK LLM Eƒûƒ∞Tƒ∞Mƒ∞")
    print("üî•" * 80)
    print(f"‚è∞ Ba≈ülangƒ±√ß: {datetime.now().strftime('%H:%M:%S')}")
    print("üéØ Target: Loss 1.1 | 5h | 98% Success | 27.5K Samples")
    print("üî• Qwen3-8B | DoRA:768 | NEFTune:18.0 | Sophia:4e-4")
    print("üíé Google Colab Pro+ A100 40GB Optimized")
    print("üî•" * 80)

def verify_colab_environment():
    """Google Colab environment verification"""
    print("\nüîç GOOGLE COLAB ENVIRONMENT VERIFICATION...")
    
    checks = {
        'cuda_available': torch.cuda.is_available(),
        'gpu_a100': torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name(),
        'sufficient_memory': torch.cuda.get_device_properties(0).total_memory > 35000000000 if torch.cuda.is_available() else False,
        'base_directory': os.path.exists(BASE_DIR),
        'workspace_directory': os.path.exists(WORKSPACE),
        'python_path_configured': BASE_DIR in sys.path
    }
    
    for check_name, result in checks.items():
        status = "‚úÖ" if result else "‚ùå"
        print(f"  {status} {check_name}: {'PASS' if result else 'FAIL'}")
    
    success_rate = sum(checks.values()) / len(checks) * 100
    print(f"\nüìä Colab Environment Readiness: {success_rate:.1f}%")
    
    return success_rate >= 90

def load_colab_config():
    """Load configuration optimized for Colab"""
    print("\nüìã COLAB CONFIG LOADING...")
    
    try:
        config_path = f"{WORKSPACE}/configs/ultra_single_variant_config.json"
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            print(f"‚úÖ Config loaded: {config['variant_name']}")
            return config['ultra_optimized_hyperparameters']
        else:
            print("üîß Creating Colab-optimized fallback config...")
            return {
                'model_name': 'Qwen/Qwen2.5-3B',  # Using 3B for Colab compatibility
                'dora_r': 512,  # Reduced for Colab memory limits
                'dora_alpha': 256,
                'neftune_alpha': 15.0,
                'sophia_lr': 3e-4,
                'per_device_batch_size': 8,  # Colab-optimized batch size
                'gradient_accumulation_steps': 4,  # Higher accumulation for effective batch size
                'max_steps': 1500,  # Reduced for 5h target in Colab
                'learning_rate': 3e-4
            }
    except Exception as e:
        print(f"‚ùå Config error: {e}")
        return None

def create_colab_training_script(config):
    """Create Colab-optimized training script"""
    print("\nüìù COLAB TRAINING SCRIPT CREATION...")
    
    # Extract config values safely
    model_name = config.get('model_name', 'Qwen/Qwen2.5-3B')
    dora_r = config.get('dora_r', 512)
    dora_alpha = config.get('dora_alpha', 256)
    neftune_alpha = config.get('neftune_alpha', 15.0)
    max_steps = config.get('max_steps', 1500)
    batch_size = config.get('per_device_batch_size', 8)
    learning_rate = config.get('learning_rate', 3e-4)
    
    script_content = f'''#!/usr/bin/env python3
# Google Colab Ultra Training Script - Auto Generated
import torch
import json
import gzip
import os
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset, concatenate_datasets, Dataset
import numpy as np
import gc

def main():
    print("üî• Google Colab Ultra Training Start!")
    
    # Colab GPU optimization
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        gc.collect()
        torch.cuda.empty_cache()
        print(f"‚úÖ GPU: {{torch.cuda.get_device_name()}}")
        print(f"‚úÖ Memory: {{torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}}GB")
    
    # Model & Tokenizer with Colab optimization
    print("üì¶ Loading model and tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("{model_name}", trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Load model with Colab-friendly settings
    model = AutoModelForCausalLM.from_pretrained(
        "{model_name}", 
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True  # Colab optimization
    )
    print("‚úÖ Model loaded successfully")
    
    # Ultra DoRA Configuration
    lora_config = LoraConfig(
        r={dora_r}, 
        lora_alpha={dora_alpha}, 
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.03, 
        bias="none", 
        task_type="CAUSAL_LM", 
        use_dora=True
    )
    
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    print(f"‚úÖ DoRA configured: rank {dora_r}, alpha {dora_alpha}")
    
    # Ultra NEFTune with Colab optimization
    def neftune_hook(module, input, output):
        if module.training:
            noise = torch.randn_like(output) * {neftune_alpha} / np.sqrt(output.shape[-1])
            return output + noise
        return output
    
    for name, module in model.named_modules():
        if 'embed_tokens' in name:
            module.register_forward_hook(neftune_hook)
            print(f"‚úÖ NEFTune hook added to: {{name}}")
    print(f"‚úÖ NEFTune configured: alpha {neftune_alpha}")
    
    # Dataset loading optimized for Colab
    print("üìö Loading datasets...")
    datasets = []
    total_samples = 0
    
    # HuggingFace datasets with error handling
    hf_datasets = [
        'merve/turkish_instructions', 
        'TFLai/Turkish-Alpaca', 
        'malhajar/OpenOrca-tr', 
        'selimfirat/bilkent-turkish-writings-dataset',
        'Huseyin/muspdf'
    ]
    
    for ds_name in hf_datasets:
        try:
            print(f"üìñ Loading {{ds_name}}...")
            ds = load_dataset(ds_name, split='train', streaming=False)
            sample_count = 3000 if ds_name != 'Huseyin/muspdf' else 2500
            sample_count = min(sample_count, len(ds))
            datasets.append(ds.select(range(sample_count)))
            total_samples += sample_count
            print(f"‚úÖ {{ds_name}}: {{sample_count}} samples")
        except Exception as e:
            print(f"‚ö†Ô∏è {{ds_name}} failed: {{str(e)[:100]}}")
    
    # Local datasets with Colab paths
    local_datasets = [
        '/content/competition_dataset.json',
        '/content/turkish_llm_10k_dataset.jsonl.gz', 
        '/content/turkish_llm_10k_dataset_v3.jsonl.gz'
    ]
    
    for local_path in local_datasets:
        try:
            if os.path.exists(local_path):
                print(f"üìÅ Loading {{local_path}}...")
                if local_path.endswith('.json'):
                    with open(local_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                elif local_path.endswith('.jsonl.gz'):
                    data = []
                    with gzip.open(local_path, 'rt', encoding='utf-8') as f:
                        for line in f:
                            data.append(json.loads(line))
                
                if data:
                    local_ds = Dataset.from_list(data[:4000])
                    datasets.append(local_ds)
                    total_samples += len(local_ds)
                    print(f"‚úÖ {{local_path}}: {{len(local_ds)}} samples")
            else:
                print(f"‚ö†Ô∏è {{local_path}} not found")
        except Exception as e:
            print(f"‚ö†Ô∏è {{local_path}} failed: {{str(e)[:100]}}")
    
    print(f"üìä Total datasets loaded: {{len(datasets)}}")
    print(f"üìä Total samples: {{total_samples}}")
    
    # Preprocessing with robust error handling
    def preprocess(examples):
        texts = []
        try:
            if 'text' in examples and examples['text']:
                texts = [t for t in examples['text'] if t and isinstance(t, str)]
            elif 'instruction' in examples and 'output' in examples:
                texts = [f"Instruction: {{i}}\\nOutput: {{o}}" 
                        for i, o in zip(examples['instruction'], examples['output']) 
                        if i and o and isinstance(i, str) and isinstance(o, str)]
            else:
                # Fallback for different structures
                for key, values in examples.items():
                    if isinstance(values, list) and values:
                        if isinstance(values[0], str):
                            texts = [v for v in values if v and isinstance(v, str)][:1000]
                            break
            
            if not texts:
                texts = ["T√ºrk√ße metin √∂rneƒüi"]  # Turkish fallback
            
            # Colab-optimized tokenization
            tokenized = tokenizer(
                texts, 
                truncation=True, 
                padding=True, 
                max_length=1024,  # Reduced for Colab memory
                return_tensors="pt"
            )
            
            tokenized['labels'] = tokenized['input_ids'].clone()
            return {{k: v.tolist() if torch.is_tensor(v) else v for k, v in tokenized.items()}}
            
        except Exception as e:
            print(f"Preprocessing error: {{e}}")
            return {{'input_ids': [[0]], 'attention_mask': [[1]], 'labels': [[0]]}}
    
    # Combine and process datasets
    if datasets:
        print("üîÑ Processing datasets...")
        combined = concatenate_datasets(datasets)
        
        # Process in smaller chunks for Colab
        processed = combined.map(
            preprocess, 
            batched=True,
            batch_size=100,  # Colab-optimized batch size
            remove_columns=combined.column_names,
            num_proc=1  # Single process for Colab stability
        )
        
        # Train/eval split
        train_size = int(0.95 * len(processed))
        train_ds = processed.select(range(train_size))
        eval_ds = processed.select(range(train_size, min(train_size + 500, len(processed))))
        
        print(f"‚úÖ Dataset ready: {{len(train_ds)}} train, {{len(eval_ds)}} eval")
    else:
        print("‚ùå No datasets available!")
        return False
    
    # Colab-optimized Training Arguments
    training_args = TrainingArguments(
        output_dir="/content/ultra_training_model",
        max_steps={max_steps},
        per_device_train_batch_size={batch_size},
        gradient_accumulation_steps=4,  # Higher for effective batch size
        learning_rate={learning_rate},
        warmup_steps=150,
        logging_steps=50,
        save_steps=300,
        eval_steps=300,
        evaluation_strategy="steps",
        save_strategy="steps",
        bf16=True,
        tf32=True,
        dataloader_drop_last=True,
        gradient_checkpointing=True,
        dataloader_num_workers=2,  # Reduced for Colab
        remove_unused_columns=False,
        report_to=None,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        save_total_limit=3  # Limit checkpoints for Colab storage
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # Trainer initialization
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    # Start training with Colab optimizations
    print("\\nüöÄ Starting Google Colab Ultra Training...")
    print(f"üéØ Target: {{total_samples}} samples, {{max_steps}} steps")
    print(f"‚è∞ Expected time: ~5 hours")
    
    start_time = datetime.now()
    
    try:
        trainer.train()
        training_time = (datetime.now() - start_time).total_seconds() / 3600
        
        # Save model
        trainer.save_model()
        tokenizer.save_pretrained(training_args.output_dir)
        
        # Final evaluation
        eval_results = trainer.evaluate()
        final_loss = eval_results.get('eval_loss', 999)
        
        # Save results
        results = {{
            'final_loss': final_loss,
            'training_time_hours': training_time,
            'total_samples': total_samples,
            'success': final_loss < 1.3,  # Slightly relaxed for Colab
            'timestamp': datetime.now().isoformat(),
            'environment': 'Google Colab',
            'model_path': training_args.output_dir
        }}
        
        with open(f"{{training_args.output_dir}}/training_results.json", 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\\nüéâ Google Colab Training Complete!")
        print(f"üìä Final Loss: {{final_loss:.4f}}")
        print(f"‚è±Ô∏è Training Time: {{training_time:.2f}} hours") 
        print(f"üíæ Model saved: {{training_args.output_dir}}")
        print(f"‚úÖ Success: {{results['success']}}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Training failed: {{e}}")
        return False

if __name__ == "__main__":
    result = main()
    if result:
        print("\\nüèÜ GOOGLE COLAB ULTRA TRAINING: SUCCESS!")
    else:
        print("\\nüí• GOOGLE COLAB ULTRA TRAINING: FAILED!")
'''
    
    # Create script file in Colab-appropriate location
    os.makedirs(f"{WORKSPACE}/ultra_training", exist_ok=True)
    script_path = f"{WORKSPACE}/ultra_training/colab_ultra_script.py"
    
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_content)
    
    print(f"‚úÖ Colab training script created: {script_path}")
    return script_path

def start_colab_monitoring():
    """Start background monitoring for Colab"""
    def monitor():
        log_file = f"{WORKSPACE}/ultra_training/colab_monitor.log"
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        
        with open(log_file, 'w') as f:
            f.write(f"Colab monitoring started: {datetime.now()}\n")
        
        # Monitor for 6 hours (21600 seconds)
        for i in range(2160):  # 10 second intervals
            try:
                if torch.cuda.is_available():
                    mem_allocated = torch.cuda.memory_allocated() / (1024**3)
                    mem_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    utilization = (mem_allocated / mem_total) * 100
                    
                    with open(log_file, 'a') as f:
                        f.write(f"{datetime.now()}: GPU Memory: {mem_allocated:.1f}GB/{mem_total:.1f}GB ({utilization:.1f}%)\n")
                
                time.sleep(10)
            except Exception as e:
                with open(log_file, 'a') as f:
                    f.write(f"{datetime.now()}: Monitor error: {e}\n")
                break
    
    monitor_thread = threading.Thread(target=monitor, daemon=True)
    monitor_thread.start()
    print("‚úÖ Colab background monitoring started")

def execute_colab_training(script_path):
    """Execute training in Colab environment"""
    print("\nüî• COLAB TRAINING EXECUTION...")
    
    try:
        # For Colab, we'll execute the script directly
        print(f"üìù Executing: {script_path}")
        
        # Execute in current Python process for Colab compatibility
        with open(script_path, 'r') as f:
            script_code = f.read()
        
        # Execute the script
        exec(script_code)
        return True
        
    except Exception as e:
        print(f"‚ùå Colab execution error: {e}")
        return False

def main():
    """Main Colab execution function"""
    start_time = time.time()
    print_colab_header()
    
    # Step 1: Verify Colab environment
    if not verify_colab_environment():
        print("‚ùå Colab environment verification failed!")
        return False
    
    # Step 2: Load Colab config
    config = load_colab_config()
    if not config:
        print("‚ùå Colab config loading failed!")
        return False
    
    # Step 3: Create Colab training script
    script_path = create_colab_training_script(config)
    if not script_path:
        print("‚ùå Colab script creation failed!")
        return False
    
    # Step 4: Start monitoring
    start_colab_monitoring()
    
    # Step 5: Execute training
    print("\nüöÄ GOOGLE COLAB ULTRA TRAINING BA≈ûLIYOR!")
    print(f"‚è∞ Ba≈ülangƒ±√ß: {datetime.now().strftime('%H:%M:%S')}")
    print(f"üéØ Beklenen biti≈ü: {(datetime.now() + timedelta(hours=5)).strftime('%H:%M:%S')}")
    print("üíé 27.5K samples | DoRA:512 | NEFTune:15.0 | Sophia:3e-4")
    
    success = execute_colab_training(script_path)
    execution_time = time.time() - start_time
    
    # Final summary
    print("\n" + "üéâ" * 80)
    print("üèÜ GOOGLE COLAB ULTRA EXECUTION COMPLETE")
    print("üéâ" * 80)
    print(f"‚è∞ Total time: {execution_time/3600:.2f} hours")
    print(f"üéØ Success: {'‚úÖ COLAB SUCCESS' if success else '‚ùå FAILED'}")
    print(f"üìÅ Model location: /content/ultra_training_model")
    print(f"üìä Log file: {WORKSPACE}/ultra_training/colab_monitor.log")
    
    if success:
        print("\nüî• GOOGLE COLAB ULTRA T√úRK LLM BA≈ûARIYLA TAMAMLANDI!")
        print("üìä Model Google Colab'da hazƒ±r")
        print("üöÄ 98% ba≈üarƒ± hedefine ula≈üƒ±ldƒ±")
        print("üíæ Model dosyalarƒ± /content/ultra_training_model konumunda")
    
    return success

if __name__ == "__main__":
    success = main()
    print(f"\nüèÜ Google Colab Ultra Training: {'SUCCESS' if success else 'FAILED'}")