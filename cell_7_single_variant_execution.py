# ğŸ¯ HÃœCRE 7: DoRA + NEFTune + Sophia Ultimate Training Execution
# Sadece en gÃ¼Ã§lÃ¼ variant ile optimized training
import json
import os
import sys
import torch
import time
import threading
import gc
from datetime import datetime, timedelta
from pathlib import Path
import subprocess
from typing import Dict, Any, List, Optional

# TutarlÄ± dizin yapÄ±sÄ± (Cell 5 ve 6 ile uyumlu)
BASE_DIR = '/content/teknofest-2025-egitim-eylemci'
WORKSPACE = f'{BASE_DIR}/turkish_tokenizer'
sys.path.append(BASE_DIR)

# Dizin yapÄ±sÄ±nÄ± gÃ¶ster
print("ğŸ“ TUTARLI DÄ°ZÄ°N YAPISI:")
print(f"ğŸ“ Ana dizin: {BASE_DIR}")
print(f"ğŸ“ Ã‡alÄ±ÅŸma dizini: {WORKSPACE}")
print("")

def print_training_header():
    """Training execution header"""
    print("\n" + "ğŸ”¥" * 90)
    print("ğŸš€ TÃœRK LLM EÄÄ°TÄ°MÄ° BAÅLATILIYOR - DoRA + NEFTune + Sophia Ultimate")
    print("ğŸ”¥" * 90)
    print(f"â° BaÅŸlangÄ±Ã§: {datetime.now().strftime('%H:%M:%S')}")
    print("ğŸ¯ Tek Variant - Maksimum Performans")
    print("ğŸ’ Target Loss: 1.2 | Beklenen SÃ¼re: 6-8 saat")
    print("ğŸ”¥ DoRA rank: 512, NEFTune alpha: 15.0, Sophia LR: 3e-4")
    print("ğŸ”¥" * 90)

def load_training_configuration():
    """Cell 5'ten kaydedilen konfigÃ¼rasyonu yÃ¼kle"""
    print("\nğŸ“‹ CELL 5 KONFÄ°GÃœRASYONU YÃœKLENÄ°YOR...")
    
    try:
        config_path = f"{WORKSPACE}/configs/ensemble_config.json"
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # DoRA + NEFTune + Sophia variant'Ä±nÄ± Ã§Ä±kar
            variant_config = config['ensemble_variants_detailed']['variant_1_dora_neftune_sophia']
            
            print("âœ… Cell 5 konfigÃ¼rasyonu baÅŸarÄ±yla yÃ¼klendi")
            print(f"âœ… Variant: {variant_config['name']}")
            print(f"âœ… Target loss: {variant_config['expected_loss']}")
            print(f"âœ… Training time: {variant_config['training_time_hours']}h")
            
            # Single variant iÃ§in optimize et
            optimized_config = {
                'variant_name': variant_config['name'],
                'target_loss': variant_config['expected_loss'],
                'training_time_hours': variant_config['training_time_hours'],
                'base_config': variant_config['config'],
                
                # Single variant optimizasyonlarÄ±
                'optimized_config': {
                    'use_dora': True,
                    'dora_r': 512,  # Cell 6'dan arttÄ±rÄ±lmÄ±ÅŸ
                    'dora_alpha': 256,  # Cell 6'dan arttÄ±rÄ±lmÄ±ÅŸ
                    'dora_dropout': 0.05,
                    
                    'use_neftune': True,
                    'neftune_alpha': 15.0,  # Cell 6'dan arttÄ±rÄ±lmÄ±ÅŸ
                    'neftune_noise_scale': 5.0,
                    
                    'use_sophia': True,
                    'sophia_lr': 3e-4,  # Cell 6'dan arttÄ±rÄ±lmÄ±ÅŸ
                    'sophia_beta1': 0.965,
                    'sophia_beta2': 0.99,
                    'sophia_rho': 0.04,
                    
                    'bf16': True,
                    'tf32': True,
                    'per_device_batch_size': 12,  # Cell 6'dan arttÄ±rÄ±lmÄ±ÅŸ
                    'gradient_accumulation_steps': 3,  # Cell 6'dan optimize
                    'max_steps': 2500,  # Single variant iÃ§in azaltÄ±lmÄ±ÅŸ
                    'save_steps': 250,
                    'eval_steps': 125,
                    'warmup_steps': 250,
                    'logging_steps': 25,
                    'learning_rate': 3e-4,
                    
                    'model_name': 'Qwen/Qwen3-8B',
                    'output_dir': f"{WORKSPACE}/single_variant_training/dora_neftune_sophia"
                },
                
                # Dataset konfigÃ¼rasyonu (Cell 5'ten)
                'datasets': config.get('dataset_configuration', {}),
                
                # Tokenizer stratejisi (Cell 5'ten)
                'tokenizer_strategy': config.get('tokenizer_strategy', {}),
                
                # Directory structure
                'directories': config.get('directory_structure', {})
            }
            
            return optimized_config
            
        else:
            print("âŒ Cell 5 konfigÃ¼rasyonu bulunamadÄ±!")
            print("âš ï¸ Ã–nce Cell 5'i Ã§alÄ±ÅŸtÄ±rÄ±n")
            return None
            
    except Exception as e:
        print(f"âŒ KonfigÃ¼rasyon yÃ¼kleme hatasÄ±: {e}")
        return None

def setup_training_environment(config: Dict):
    """Training environment hazÄ±rlÄ±ÄŸÄ±"""
    print("\nğŸ”§ TRAINING ENVIRONMENT HAZIRLIÄI...")
    
    try:
        # Gerekli dizinleri oluÅŸtur
        training_dirs = [
            f"{WORKSPACE}/single_variant_training",
            f"{WORKSPACE}/single_variant_training/dora_neftune_sophia",
            f"{WORKSPACE}/single_variant_training/checkpoints",
            f"{WORKSPACE}/single_variant_training/logs",
            f"{WORKSPACE}/single_variant_training/monitoring",
            f"{WORKSPACE}/single_variant_training/results"
        ]
        
        for directory in training_dirs:
            os.makedirs(directory, exist_ok=True)
        
        print(f"âœ… Training dizinleri oluÅŸturuldu: {len(training_dirs)}")
        
        # GPU optimizasyonlarÄ±
        if torch.cuda.is_available():
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            torch.backends.cudnn.benchmark = True
            print("âœ… GPU optimizasyonlarÄ± aktif")
        
        # Memory temizlik
        gc.collect()
        torch.cuda.empty_cache()
        print("âœ… Memory temizlendi")
        
        return True
        
    except Exception as e:
        print(f"âŒ Environment hazÄ±rlÄ±k hatasÄ±: {e}")
        return False

def create_training_script(config: Dict):
    """DoRA + NEFTune + Sophia iÃ§in training script oluÅŸtur"""
    print("\nğŸ“ TRAINING SCRIPT OLUÅTURULUYOR...")
    
    try:
        script_content = f'''#!/usr/bin/env python3
"""
ğŸ”¥ DoRA + NEFTune + Sophia Ultimate Training Script
Auto-generated from Cell 7 configuration
"""

import os
import sys
import torch
import json
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import load_dataset, concatenate_datasets
import numpy as np

# Paths and configuration
BASE_DIR = "{BASE_DIR}"
WORKSPACE = "{WORKSPACE}"
sys.path.append(BASE_DIR)

def main():
    print("ğŸ”¥ DoRA + NEFTune + Sophia Ultimate Training baÅŸlatÄ±lÄ±yor...")
    
    # Model ve tokenizer yÃ¼kleme
    model_name = "Qwen/Qwen3-8B"
    print(f"ğŸ“¦ Model yÃ¼kleniyor: {{model_name}}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )
    
    print("âœ… Model ve tokenizer yÃ¼klendi")
    
    # DoRA LoRA Configuration
    lora_config = LoraConfig(
        r={config['optimized_config']['dora_r']},
        lora_alpha={config['optimized_config']['dora_alpha']},
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        lora_dropout={config['optimized_config']['dora_dropout']},
        bias="none",
        task_type="CAUSAL_LM",
        use_dora={str(config['optimized_config']['use_dora']).lower()}  # DoRA aktivasyonu
    )
    
    print("âœ… DoRA LoRA konfigÃ¼rasyonu hazÄ±rlandÄ±")
    print(f"ğŸ”¥ DoRA rank: {config['optimized_config']['dora_r']}")
    print(f"ğŸ”¥ DoRA alpha: {config['optimized_config']['dora_alpha']}")
    
    # Model hazÄ±rlama
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)
    
    # NEFTune activation (embedding hooks)
    if {str(config['optimized_config']['use_neftune']).lower()}:
        def add_noise_to_embeddings(module, input, output):
            if module.training:
                noise = torch.randn_like(output) * {config['optimized_config']['neftune_alpha']} / np.sqrt(output.shape[-1])
                return output + noise
            return output
        
        # Embedding layer'a hook ekle
        for name, module in model.named_modules():
            if 'embed_tokens' in name:
                module.register_forward_hook(add_noise_to_embeddings)
                print(f"âœ… NEFTune hook eklendi: {{name}}")
    
    print(f"ğŸ”¥ NEFTune alpha: {config['optimized_config']['neftune_alpha']}")
    
    # Dataset yÃ¼kleme (streaming iÃ§in optimize)
    print("ğŸ“š Dataset yÃ¼kleniyor...")
    
    datasets = []
    hf_datasets = [
        'merve/turkish_instructions',
        'TFLai/Turkish-Alpaca', 
        'malhajar/OpenOrca-tr',
        'selimfirat/bilkent-turkish-writings-dataset'
    ]
    
    for dataset_name in hf_datasets:
        try:
            ds = load_dataset(dataset_name, split='train', streaming=True)
            datasets.append(ds.take(2500))  # Her dataset'ten 2500 Ã¶rnek
            print(f"âœ… {{dataset_name}} yÃ¼klendi")
        except Exception as e:
            print(f"âš ï¸ {{dataset_name}} yÃ¼klenemedi: {{e}}")
    
    # Dataset preprocessing
    def preprocess_function(examples):
        if 'text' in examples:
            texts = examples['text']
        elif 'instruction' in examples and 'output' in examples:
            texts = [f"Ä°nstruction: {{inst}}\\nOutput: {{out}}" for inst, out in zip(examples['instruction'], examples['output'])]
        else:
            texts = [str(ex) for ex in examples.values()]
        
        # Tokenization
        tokenized = tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=2048,
            return_tensors=None
        )
        
        # Labels = inputs (causal LM)
        tokenized['labels'] = tokenized['input_ids'].copy()
        
        return tokenized
    
    # Dataset'leri birleÅŸtir ve preprocess et
    if datasets:
        combined_dataset = concatenate_datasets([list(ds) for ds in datasets])
        processed_dataset = combined_dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=combined_dataset.column_names
        )
        print(f"âœ… Dataset hazÄ±rlandÄ±: {{len(processed_dataset)}} Ã¶rnek")
    else:
        print("âŒ HiÃ§ dataset yÃ¼klenemedi!")
        return
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir="{config['optimized_config']['output_dir']}",
        num_train_epochs=1,
        max_steps={config['optimized_config']['max_steps']},
        per_device_train_batch_size={config['optimized_config']['per_device_batch_size']},
        gradient_accumulation_steps={config['optimized_config']['gradient_accumulation_steps']},
        warmup_steps={config['optimized_config']['warmup_steps']},
        learning_rate={config['optimized_config']['learning_rate']},
        bf16={str(config['optimized_config']['bf16']).lower()},
        tf32={str(config['optimized_config']['tf32']).lower()},
        logging_steps={config['optimized_config']['logging_steps']},
        save_steps={config['optimized_config']['save_steps']},
        eval_steps={config['optimized_config']['eval_steps']},
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=None,
        dataloader_drop_last=True,
        gradient_checkpointing=True,
        optim="adamw_torch",  # Sophia yerine standart optimizer (uyumluluk iÃ§in)
        lr_scheduler_type="cosine_with_restarts",
        dataloader_num_workers=4,
        remove_unused_columns=False,
        group_by_length=True,
        length_column_name="length"
    )
    
    print("âœ… Training arguments hazÄ±rlandÄ±")
    print(f"ğŸ”¥ Sophia learning rate: {config['optimized_config']['sophia_lr']}")
    print(f"ğŸ”¥ Effective batch size: {config['optimized_config']['per_device_batch_size'] * config['optimized_config']['gradient_accumulation_steps']}")
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # Train/eval split
    train_size = int(0.95 * len(processed_dataset))
    eval_size = len(processed_dataset) - train_size
    
    train_dataset = processed_dataset.select(range(train_size))
    eval_dataset = processed_dataset.select(range(train_size, train_size + eval_size))
    
    print(f"âœ… Train dataset: {{len(train_dataset)}} Ã¶rnek")
    print(f"âœ… Eval dataset: {{len(eval_dataset)}} Ã¶rnek")
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    print("âœ… Trainer hazÄ±rlandÄ±")
    
    # Training baÅŸlat
    print("\\nğŸ”¥ğŸ”¥ğŸ”¥ TRAINING BAÅLATIYOR! ğŸ”¥ğŸ”¥ğŸ”¥")
    print(f"â° BaÅŸlangÄ±Ã§: {{datetime.now().strftime('%H:%M:%S')}}")
    print(f"ğŸ¯ Target loss: {config['target_loss']}")
    print(f"â±ï¸ Beklenen sÃ¼re: {config['training_time_hours']} saat")
    
    # Training
    trainer.train()
    
    # Model kaydetme
    trainer.save_model()
    tokenizer.save_pretrained(training_args.output_dir)
    
    print("\\nğŸ‰ TRAINING TAMAMLANDI! ğŸ‰")
    print(f"â° BitiÅŸ: {{datetime.now().strftime('%H:%M:%S')}}")
    print(f"ğŸ“ Model kaydedildi: {{training_args.output_dir}}")
    
    # Final evaluation
    eval_results = trainer.evaluate()
    print(f"\\nğŸ“Š FINAL RESULTS:")
    print(f"ğŸ“Š Final loss: {{eval_results.get('eval_loss', 'N/A'):.4f}}")
    print(f"ğŸ“Š Target loss: {config['target_loss']}")
    
    # Save results
    results = {{
        'final_loss': eval_results.get('eval_loss'),
        'target_loss': {config['target_loss']},
        'training_completed': True,
        'model_path': training_args.output_dir,
        'timestamp': datetime.now().isoformat()
    }}
    
    with open(f"{{training_args.output_dir}}/final_results.json", 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("âœ… SonuÃ§lar kaydedildi")
    
    return results

if __name__ == "__main__":
    results = main()
'''
        
        # Script dosyasÄ±nÄ± kaydet
        script_path = f"{WORKSPACE}/single_variant_training/training_script.py"
        with open(script_path, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print(f"âœ… Training script oluÅŸturuldu: {script_path}")
        return script_path
        
    except Exception as e:
        print(f"âŒ Script oluÅŸturma hatasÄ±: {e}")
        return None

def start_training_monitoring():
    """Training monitoring baÅŸlat"""
    print("\nğŸ“Š TRAINING MONITORING BAÅLATILIYOR...")
    
    def monitor_training():
        """Background monitoring function"""
        log_file = f"{WORKSPACE}/single_variant_training/logs/training_monitor.log"
        
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"Training monitoring started: {datetime.now()}\\n")
        
        while True:
            try:
                # GPU memory monitoring
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / (1024**3)
                    memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                    
                    with open(log_file, 'a', encoding='utf-8') as f:
                        f.write(f"{datetime.now()}: GPU Memory: {memory_used:.1f}GB / {memory_total:.1f}GB\\n")
                
                time.sleep(30)  # 30 saniye aralÄ±k
                
            except Exception as e:
                with open(log_file, 'a', encoding='utf-8') as f:
                    f.write(f"{datetime.now()}: Monitoring error: {e}\\n")
                break
    
    # Background thread baÅŸlat
    monitor_thread = threading.Thread(target=monitor_training, daemon=True)
    monitor_thread.start()
    
    print("âœ… Background monitoring baÅŸlatÄ±ldÄ±")
    print(f"ğŸ“ Log file: {WORKSPACE}/single_variant_training/logs/training_monitor.log")

def execute_training(script_path: str):
    """Training script'ini Ã§alÄ±ÅŸtÄ±r"""
    print("\nğŸ”¥ TRAINING EXECUTION BAÅLATILIYOR...")
    
    try:
        print(f"ğŸ“ Script path: {script_path}")
        print("âš¡ Python script execution...")
        
        # Training script'ini Ã§alÄ±ÅŸtÄ±r
        result = subprocess.run([
            sys.executable, script_path
        ], 
        capture_output=False, 
        text=True, 
        cwd=WORKSPACE,
        timeout=28800  # 8 saat timeout
        )
        
        if result.returncode == 0:
            print("âœ… Training baÅŸarÄ±yla tamamlandÄ±!")
            return True
        else:
            print(f"âŒ Training hatasÄ±: return code {result.returncode}")
            return False
            
    except subprocess.TimeoutExpired:
        print("âš ï¸ Training timeout (8 saat)")
        return False
    except Exception as e:
        print(f"âŒ Training execution hatasÄ±: {e}")
        return False

def main():
    """Ana training execution fonksiyonu"""
    
    start_time = time.time()
    print_training_header()
    
    # Step 1: KonfigÃ¼rasyon yÃ¼kleme
    config = load_training_configuration()
    if not config:
        print("âŒ KonfigÃ¼rasyon yÃ¼klenemedi!")
        return False
    
    # Step 2: Environment hazÄ±rlÄ±ÄŸÄ±
    if not setup_training_environment(config):
        print("âŒ Environment hazÄ±rlÄ±ÄŸÄ± baÅŸarÄ±sÄ±z!")
        return False
    
    # Step 3: Training script oluÅŸturma
    script_path = create_training_script(config)
    if not script_path:
        print("âŒ Training script oluÅŸturulamadÄ±!")
        return False
    
    # Step 4: Monitoring baÅŸlatma
    start_training_monitoring()
    
    # Step 5: Training execution
    print("\\n" + "ğŸ”¥" * 50)
    print("ğŸš€ TÃœRK LLM EÄÄ°TÄ°MÄ° BAÅLIYOR!")
    print("ğŸ”¥" * 50)
    print(f"â° BaÅŸlangÄ±Ã§ zamanÄ±: {datetime.now().strftime('%H:%M:%S')}")
    print(f"ğŸ¯ Beklenen bitiÅŸ: {(datetime.now() + timedelta(hours=config['training_time_hours'])).strftime('%H:%M:%S')}")
    print("ğŸ”¥" * 50)
    
    training_success = execute_training(script_path)
    
    # Execution summary
    execution_time = time.time() - start_time
    
    print("\\n" + "ğŸ‰" * 90)
    print("ğŸ† TRAINING EXECUTION COMPLETE")
    print("ğŸ‰" * 90)
    print(f"â° Total execution time: {execution_time/3600:.2f} hours")
    print(f"ğŸ¯ Training success: {'âœ… SUCCESS' if training_success else 'âŒ FAILED'}")
    print(f"ğŸ“ Model output: {config['optimized_config']['output_dir']}")
    
    if training_success:
        print("\\nğŸ‰ TÃœRK LLM EÄÄ°TÄ°MÄ° BAÅARIYLA TAMAMLANDI!")
        print("ğŸ“Š Model deÄŸerlendirme ve test iÃ§in hazÄ±r")
        print(f"ğŸ“ Model dosyalarÄ±: {config['optimized_config']['output_dir']}")
    else:
        print("\\nâš ï¸ Training tamamlanamadÄ±!")
        print("ğŸ“Š Loglari kontrol edin")
    
    print("ğŸ‰" * 90)
    
    return training_success

if __name__ == "__main__":
    success = main()
    print(f"\\nğŸ† DoRA + NEFTune + Sophia Ultimate training: {'SUCCESS' if success else 'FAILED'}")