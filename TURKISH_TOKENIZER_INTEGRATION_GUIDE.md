# Turkish Mixtral Tokenizer - Qwen3-8B Entegrasyon KÄ±lavuzu

## ğŸ¯ Ã–zet
Bu kÄ±lavuz, Turkish Mixtral tokenizer'Ä± Qwen3-8B modeli ile nasÄ±l kullanacaÄŸÄ±nÄ±zÄ± aÃ§Ä±klar.

## ğŸ“‹ Ä°Ã§indekiler
1. [Neden Bu Entegrasyon?](#neden)
2. [Kurulum](#kurulum)
3. [Temel KullanÄ±m](#temel-kullanÄ±m)
4. [Model EÄŸitimi](#model-eÄŸitimi)
5. [Ãœretim OrtamÄ±](#Ã¼retim-ortamÄ±)

## ğŸ¤” Neden Bu Entegrasyon? <a name="neden"></a>

### Problem:
- Qwen3-8B'nin orijinal tokenizer'Ä± Ã‡ince aÄŸÄ±rlÄ±klÄ± (100K vocab, %40 Ã‡ince)
- TÃ¼rkÃ§e metinlerde verimsiz tokenizasyon
- YÃ¼ksek API maliyetleri

### Ã‡Ã¶zÃ¼m:
- Turkish Mixtral tokenizer (32K vocab, %62.5 TÃ¼rkÃ§e)
- %40 daha az token kullanÄ±mÄ±
- Ã–zel eÄŸitim domain tokenlarÄ±

## ğŸ”§ Kurulum <a name="kurulum"></a>

### 1. Gerekli KÃ¼tÃ¼phaneler

```bash
# Temel kÃ¼tÃ¼phaneler
pip install transformers==4.44.2
pip install torch torchvision torchaudio
pip install sentencepiece
pip install peft accelerate bitsandbytes

# Opsiyonel (4-bit training iÃ§in)
pip install auto-gptq
```

### 2. Dosya YapÄ±sÄ±

```
teknofest-2025-egitim-eylemci/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ turkish_mixtral_v3_fixed.model  # Turkish tokenizer model
â”‚   â””â”€â”€ turkish_mixtral_v3_fixed.vocab  # Turkish vocabulary
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ turkish_mixtral_tokenizer.py    # Turkish tokenizer implementasyonu
â”‚   â””â”€â”€ qwen_turkish_tokenizer_adapter.py # Adapter sÄ±nÄ±fÄ±
â””â”€â”€ train_qwen_with_turkish_tokenizer.py # EÄŸitim scripti
```

## ğŸš€ Temel KullanÄ±m <a name="temel-kullanÄ±m"></a>

### Basit Tokenizasyon

```python
from src.turkish_mixtral_tokenizer import TurkishMixtralTokenizer

# Turkish tokenizer'Ä± yÃ¼kle
tokenizer = TurkishMixtralTokenizer(
    model_path="notebooks/turkish_mixtral_v3_fixed.model"
)

# Tokenize et
text = "Yapay zeka ve makine Ã¶ÄŸrenmesi hakkÄ±nda bilgi"
tokens = tokenizer.encode(text)
print(f"Token sayÄ±sÄ±: {len(tokens)}")  # Qwen: 15, Turkish: 8
```

### Adapter KullanÄ±mÄ±

```python
from src.qwen_turkish_tokenizer_adapter import QwenTurkishTokenizerAdapter

# Adapter'Ä± oluÅŸtur
adapter = QwenTurkishTokenizerAdapter(
    qwen_model_path="Qwen/Qwen2.5-7B-Instruct",
    turkish_tokenizer_path="notebooks/turkish_mixtral_v3_fixed.model"
)

# Text'i model iÃ§in hazÄ±rla
text = "Ã–ÄŸrencilerin %85'i online eÄŸitim platformlarÄ±nÄ± kullanÄ±yor."
model_inputs = adapter.prepare_input_for_model(text, max_length=512)

# Model'e gÃ¶nder
# outputs = model(**model_inputs)
```

## ğŸ“š Model EÄŸitimi <a name="model-eÄŸitimi"></a>

### HÄ±zlÄ± BaÅŸlangÄ±Ã§

```python
from train_qwen_with_turkish_tokenizer import TurkishTokenizerTrainer, ModelConfig

# KonfigÃ¼rasyon
config = ModelConfig(
    model_name="Qwen/Qwen2.5-7B-Instruct",
    turkish_tokenizer_path="notebooks/turkish_mixtral_v3_fixed.model",
    use_4bit=True,  # Bellek tasarrufu iÃ§in
    use_lora=True,  # Efficient fine-tuning
    batch_size=4,
    num_epochs=3,
    output_dir="./qwen-turkish-finetuned"
)

# Trainer oluÅŸtur
trainer = TurkishTokenizerTrainer(config)

# EÄŸit
trainer.train("data/training_data.jsonl")
```

### Veri FormatÄ±

Training verisi JSONL formatÄ±nda olmalÄ±:

```json
{"instruction": "Python'da liste oluÅŸturma", "response": "Python'da liste oluÅŸturmak iÃ§in..."}
{"instruction": "Yapay zeka nedir?", "response": "Yapay zeka, makinelerin..."}
```

### LoRA Parametreleri

```python
lora_config = LoraConfig(
    r=64,              # LoRA rank (8-128 arasÄ±)
    lora_alpha=128,    # LoRA scaling (r*2 Ã¶nerilen)
    lora_dropout=0.05, # Dropout
    target_modules=[   # Qwen iÃ§in Ã¶nerilen modÃ¼ller
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)
```

## ğŸ­ Ãœretim OrtamÄ± <a name="Ã¼retim-ortamÄ±"></a>

### Optimizasyonlar

#### 1. Batch Processing

```python
def batch_tokenize(texts, adapter, batch_size=32):
    """Toplu tokenizasyon iÃ§in optimize edilmiÅŸ fonksiyon"""
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_inputs = [adapter.prepare_input_for_model(t) for t in batch]
        results.extend(batch_inputs)
    return results
```

#### 2. Caching

```python
from functools import lru_cache

@lru_cache(maxsize=10000)
def cached_tokenize(text, max_length=512):
    """SÄ±k kullanÄ±lan metinler iÃ§in cache"""
    return adapter.prepare_input_for_model(text, max_length)
```

#### 3. Model Inference Optimizasyonu

```python
# Model'i optimize et
from torch.quantization import quantize_dynamic

# Dynamic quantization (CPU iÃ§in)
quantized_model = quantize_dynamic(
    model, 
    {torch.nn.Linear}, 
    dtype=torch.qint8
)

# GPU iÃ§in mixed precision
from torch.cuda.amp import autocast

with autocast():
    outputs = model(**inputs)
```

### Deployment SeÃ§enekleri

#### 1. FastAPI Servisi

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class TextRequest(BaseModel):
    text: str
    max_length: int = 512

@app.post("/tokenize")
async def tokenize(request: TextRequest):
    tokens = adapter.prepare_input_for_model(
        request.text, 
        request.max_length
    )
    return {"token_count": len(tokens["input_ids"][0])}

@app.post("/generate")
async def generate(request: TextRequest):
    # Model generation logic
    pass
```

#### 2. Docker Container

```dockerfile
FROM python:3.9-slim

WORKDIR /app

# Dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt

# Model files
COPY notebooks/turkish_mixtral_v3_fixed.* ./notebooks/
COPY src/ ./src/
COPY train_qwen_with_turkish_tokenizer.py .

# Run service
CMD ["python", "api_server.py"]
```

## ğŸ“Š Performans Metrikleri

### Token VerimliliÄŸi

| Metin Tipi | Qwen Tokenizer | Turkish Tokenizer | Ä°yileÅŸme |
|------------|----------------|-------------------|----------|
| KÄ±sa TÃ¼rkÃ§e | 10 token | 7 token | %30 |
| Orta TÃ¼rkÃ§e | 42 token | 17 token | %59.5 |
| Uzun TÃ¼rkÃ§e | 149 token | 69 token | %53.7 |
| **Ortalama** | **45.5 token** | **27.1 token** | **%40.4** |

### Maliyet Tasarrufu

- **AylÄ±k (1M request):** $59 tasarruf
- **YÄ±llÄ±k:** $708 tasarruf
- **Context verimliliÄŸi:** %40 daha fazla iÃ§erik

## ğŸ› Sorun Giderme

### SÄ±k KarÅŸÄ±laÅŸÄ±lan Hatalar

#### 1. SentencePiece HatasÄ±
```bash
# Ã‡Ã¶zÃ¼m
pip install sentencepiece
```

#### 2. CUDA Out of Memory
```python
# Ã‡Ã¶zÃ¼m: Batch size'Ä± dÃ¼ÅŸÃ¼r
config.batch_size = 2
# veya 4-bit quantization kullan
config.use_4bit = True
```

#### 3. Token Mapping HatasÄ±
```python
# Ã‡Ã¶zÃ¼m: Fallback mekanizmasÄ± ekle
try:
    qwen_ids = adapter.adapt_tokens(turkish_ids)
except:
    # Fallback to original tokenizer
    qwen_ids = qwen_tokenizer.encode(text)
```

## ğŸ“ Ä°leri Seviye KullanÄ±m

### Custom Entity Tokens

```python
# Ã–zel entity token'larÄ± ekle
custom_entities = {
    "<STUDENT_LEVEL>": ["<BEGINNER>", "<INTERMEDIATE>", "<ADVANCED>"],
    "<ASSESSMENT>": ["<QUIZ>", "<EXAM>", "<PROJECT>"]
}

# Preprocessing fonksiyonu
def preprocess_with_entities(text):
    text = text.replace("baÅŸlangÄ±Ã§ seviyesi", "<BEGINNER>")
    text = text.replace("ileri seviye", "<ADVANCED>")
    return text
```

### Multi-Modal Entegrasyon

```python
# Vision-Language model iÃ§in
class MultiModalTurkishAdapter:
    def __init__(self, text_adapter, vision_encoder):
        self.text_adapter = text_adapter
        self.vision_encoder = vision_encoder
        
    def process(self, text, image):
        text_features = self.text_adapter.prepare_input_for_model(text)
        image_features = self.vision_encoder(image)
        return combine_features(text_features, image_features)
```

## ğŸ“š Kaynaklar

- [SentencePiece Documentation](https://github.com/google/sentencepiece)
- [Qwen Model Card](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [Turkish NLP Resources](https://github.com/topics/turkish-nlp)

## ğŸ’¡ Ä°puÃ§larÄ±

1. **Ä°lk denemede kÃ¼Ã§Ã¼k model kullanÄ±n** (Qwen2.5-1.5B)
2. **Batch size'Ä± GPU belleÄŸine gÃ¶re ayarlayÄ±n**
3. **Validation set kullanarak overfitting'i kontrol edin**
4. **Production'da model versiyonlama yapÄ±n**
5. **A/B testing ile performansÄ± doÄŸrulayÄ±n**

## âœ… Kontrol Listesi

- [ ] SentencePiece yÃ¼klendi
- [ ] Turkish tokenizer dosyalarÄ± mevcut
- [ ] Adapter sÄ±nÄ±fÄ± test edildi
- [ ] Training verisi hazÄ±r
- [ ] GPU/CPU kaynaklarÄ± yeterli
- [ ] Model checkpoint'leri iÃ§in yeterli disk alanÄ±
- [ ] Monitoring ve logging yapÄ±landÄ±rÄ±ldÄ±

---

**Not:** Bu entegrasyon, TÃ¼rkÃ§e NLP projelerinde %40+ token tasarrufu ve maliyet optimizasyonu saÄŸlar. SorularÄ±nÄ±z iÃ§in issue aÃ§abilirsiniz.